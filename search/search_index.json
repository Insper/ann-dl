{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Ementa", "text": ""}, {"location": "#ementa", "title": "Ementa", "text": "<p>Este curso oferece uma introdu\u00e7\u00e3o abrangente \u00e0 Redes Neurais e Deep Learning, um subcampo do aprendizado de m\u00e1quina que se concentra em algoritmos chamados de redes neurais artificiais. Abordaremos os princ\u00edpios fundamentais, os alicerces matem\u00e1ticos, as principais arquiteturas e as t\u00e9cnicas de implementa\u00e7\u00e3o pr\u00e1tica. O curso se inspira em cursos acad\u00eamicos de ponta e utiliza frameworks modernos de aprendizado profundo, principalmente o TensorFlow e PyTorch. Os t\u00f3picos incluem conceitos fundamentais (gradiente descendente, retropropaga\u00e7\u00e3o), principais arquiteturas de rede (MLPs, CNNs, LSTMs, Transformers), modelos generativos (GANs) e aprendizado autossupervisionado. A \u00eanfase ser\u00e1 dada tanto \u00e0 compreens\u00e3o te\u00f3rica quanto \u00e0 aplica\u00e7\u00e3o pr\u00e1tica em problemas do mundo real em \u00e1reas como vis\u00e3o computacional e processamento de linguagem natural.</p>"}, {"location": "#objetivos", "title": "Objetivos", "text": "<p>Ao final da disciplina o aluno ser\u00e1 capaz de:</p> <ol> <li>Compreender Conceitos Fundamentais: explicar os princ\u00edpios matem\u00e1ticos e algor\u00edtmicos fundamentais sobre redes neurais artificiais, incluindo gradiente descendente, retropropaga\u00e7\u00e3o, fun\u00e7\u00f5es de ativa\u00e7\u00e3o, fun\u00e7\u00f5es de perda e regulariza\u00e7\u00e3o.</li> <li>Dominar Arquiteturas-Chave: descrever a arquitetura e a racionalidade por tr\u00e1s dos principais modelos de aprendizado profundo, incluindo Perceptrons Multicamadas (MLPs), Redes Neurais Convolucionais (CNNs), Redes Neurais Recorrentes (LSTMs, GRUs) e Transformers.</li> <li>Implementar Modelos de Aprendizado Profundo: implementar, treinar e depurar modelos de aprendizado profundo usando frameworks, tais como TensorFlow e PyTorch, para tarefas como classifica\u00e7\u00e3o de imagens, modelagem de sequ\u00eancias e gera\u00e7\u00e3o.</li> <li>Analisar o Desempenho do Modelo: avaliar o desempenho do modelo usando m\u00e9tricas apropriadas, compreender armadilhas comuns (por exemplo, sobreajuste, gradientes que desaparecem/explodem) e aplicar t\u00e9cnicas para mitig\u00e1-las.</li> <li>Explorar T\u00f3picos Avan\u00e7ados: familiarizar-se com conceitos avan\u00e7ados, como modelos generativos (GANs), modelos baseados em energia, aprendizado autossupervisionado e mecanismos de aten\u00e7\u00e3o.</li> <li>Avaliar Pesquisas Criticamente: ler e avaliar criticamente artigos de pesquisa atuais no campo do aprendizado profundo.</li> <li>Aplicar: formular problemas do mundo real em termos adequados para solu\u00e7\u00f5es de aprendizado profundo e projetar modelos apropriados.</li> </ol>"}, {"location": "#conteudo-programatico", "title": "Conte\u00fado Program\u00e1tico", "text": "<ol> <li>Introdu\u00e7\u00e3o e Fundamentos de ML</li> <li>Redes Neurais e Retropropaga\u00e7\u00e3o  </li> <li>Treinamento de Redes Neurais e Regulariza\u00e7\u00e3o</li> <li>Redes Neurais Convolucionais (CNNs)</li> <li>Aplica\u00e7\u00f5es em Vis\u00e3o Computacional</li> <li>Arquiteturas Modernas de CNNs</li> <li>Aprendizado por Transfer\u00eancia</li> <li>Modelos Recorrentes (LSTMs e GRUs)  </li> <li>Mecanismos de Aten\u00e7\u00e3o</li> <li>Transformers</li> <li>Modelos Generativos (GANs)</li> <li>Aprendizado Autossupervisionado</li> <li>Projetos Pr\u00e1ticos e Estudos de Caso</li> </ol>"}, {"location": "#bibliografia-basica", "title": "Bibliografia B\u00e1sica", "text": "<p>Livros:</p> <ol> <li>Fleuret, F. (2023). The little book of deep learning, (Continuously updated online version).</li> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning. MIT Press.</li> </ol>"}, {"location": "#bibliografia-complementar", "title": "Bibliografia Complementar", "text": "<p>Livros:</p> <ol> <li>Nielsen, M. A. (2019). Neural networks and deep learning. Determination Press.</li> <li>Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2024). Dive into deep learning. (Continuously updated online version).</li> </ol>"}, {"location": "references/", "title": "References", "text": ""}, {"location": "references/#artificial-neural-networks-anns-and-deep-learning", "title": "Artificial Neural Networks (ANNs) and Deep Learning", "text": "<p>NYU Center for Data Science:</p> Course Material Deep Learning - DS-GA 1008 Spring 2020 | Summary"}, {"location": "references/#natural-language-processing", "title": "Natural Language Processing", "text": "<p>Stanford University</p> Course Material CS224N: Natural Language Processing with Deep Learning Speech and Language Processing GloVe: Global Vectors for Word Representation Learning Visual N-Grams from Web Data <p>Stanford University School of Engineering</p> Lecture Subject 2 Word Vector Representations: word2vec 3 GloVe: Global Vectors for Word Representation 4 Word Window Classification and Neural Networks 5 Backpropagation and Project Advice 6 Dependency Parsing 7 Introduction to TensorFlow 8 Recurrent Neural Networks and Language Models 9 Machine Translation and Advanced Recurrent LSTMs and GRUs Review Session Midterm Review 10 Neural Machine Translation and Models with Attention 11 Gated Recurrent Units and Further Topics in NMT 12 End-to-End Models for Speech Processing 13 Convolutional Neural Networks 14 Tree Recursive Neural Networks and Constituency Parsing 15 Coreference Resolution 16 Dynamic Neural Networks for Question Answering 17 Issues in NLP and Possible Architectures for NLP 18 Tackling the Limits of Deep Learning for NLP <p>https://github.com/jtrecenti/2025-verao-torch</p> <p>Latent Space Visualisation: PCA, t-SNE, UMAP UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction Visualizing Data using t-SNE How to Use t-SNE Effectively An Introduction to Variational Autoencoders Autoencoders, Unsupervised Learning, and Deep Architectures Autoencoders Generative Adversarial Networks (GANs) Generative Adversarial Networks (GANs) - Deep Learning Specialization Generative Adversarial Networks (GANs) - Paper GANs in Action: Deep learning with Generative Adversarial Networks</p> <p>https://mathigon.org/timeline/shannon</p>"}, {"location": "references/#dsa-data-structures-and-algorithms", "title": "DSA - Data Structures and Algorithms", "text": "<p>TutorialsPoint - Data Structures and Algorithms</p> <p>GeeksforGeeks - Data Structures and Algorithms</p> <p>David Galles - Data Structure Visualizations</p> <p>Antti Laaksonen - Competitive Programmer\u2019s Handbook</p> <p>Gayle Laakmann McDowell, Mike Mroczka, Aline Lerner - Beyond Cracking the Coding Interview</p>"}, {"location": "classes/ann/", "title": "4. Neural Networks", "text": "<p>Artificial Neural Networks (ANNs), or simply neural networks, are computational models inspired by the structure and function of biological neural networks. They consist of interconnected nodes (neurons) that process information in a manner similar to the way neurons in the human brain operate. ANNs are capable of learning from data, making them powerful tools for various tasks such as image recognition, natural language processing, and decision-making.</p> <p>Neural networks are the backbone of many modern AI applications, enabling machines to learn from experience and improve their performance over time. They are particularly effective in handling complex patterns and large datasets, making them suitable for a wide range of applications, from computer vision to speech recognition.</p>"}, {"location": "classes/ann/#milestones", "title": "Milestones", "text": "<p>Foundations of Neural Networks</p> 1943<p> Laid the theoretical groundwork for ANNs, inspiring future computational models of the brain. Warren McCulloch and Walter Pitts publish a paper introducing the first mathematical model of a neural network, describing neurons as logical decision-making units. McCulloch, &amp; W. S., Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. </p> <p>Turing Test Proposed</p> 1950<p> Established a benchmark for assessing AI capabilities, influencing the philosophical and practical development of AI. Alan Turing publishes \"Computing Machinery and Intelligence,\" proposing the Turing Test to evaluate a machine's ability to exhibit intelligent behavior indistinguishable from that of a human. Turing, A. M. (1950). Computing Machinery and Intelligence. </p> <p>Birth of AI as a Discipline</p> 1956<p> Marked the formal establishment of AI as a field of study, fostering research into machine intelligence. The Dartmouth Conference, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, coins the term artificial intelligence. McCarthy, J., Minsky, M., Rochester, N., Shannon, C. (1955). Dartmouth Conference Proposal. </p> <p>Perceptron Introduced</p> 1958<p> Pioneered the concept of a simple neural network, laying the foundation for future developments in machine learning and neural networks. Frank Rosenblatt develops the Perceptron, an early artificial neural network capable of learning to classify patterns. It consists of a single layer of output nodes connected to input features, using a step function to produce binary outputs. Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </p> <p>Limitations of Perceptrons \u2192 AI Winter</p> 1969<p> Highlighted the limitations of early neural networks, leading to a temporary decline in interest in neural networks and AI. Marvin Minsky and Seymour Papert publish \"Perceptrons,\" critiquing the limitations of single-layer perceptrons, particularly their inability to solve non-linearly separable problems like the XOR problem. This work leads to a decline in neural network research for over a decade. Led to the first \"AI winter,\" a period of reduced funding and interest in neural networks, shifting focus to symbolic AI. Minsky, M., Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry. </p> <p>Backpropagation Rediscovered</p> 1986<p> Revived interest in ANNs by overcoming limitations of single-layer perceptrons, paving the way for deep learning. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams publish a paper on backpropagation, enabling training of multi-layer neural networks. Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. </p> <p>Universal Approximation Theorem</p> 1989<p> Established the theoretical foundation for neural networks' ability to approximate any continuous function, leading to the development of deep learning. George Cybenko proves that a feedforward neural network with a single hidden layer can approximate any continuous function on compact subsets of \\(R^n\\) under mild conditions on the activation function. Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. </p> <p>Deep Blue Defeats Chess Champion</p> 1997<p> Showcased the potential of AI in strategic games, leading to advancements in game-playing AI and deep learning. IBM's Deep Blue defeats world chess champion Garry Kasparov in a six-game match, marking a significant milestone in AI's ability to compete with human intelligence in complex tasks. Campbell, M., Hoane, A. J., &amp; Hsu, F. (2002). Deep Blue. </p> <p>Convolutional Neural Networks (CNNs)</p> 1998<p> Revolutionized computer vision and image processing, enabling breakthroughs in object recognition and classification. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton publish a paper on CNNs, introducing the LeNet architecture for handwritten digit recognition. LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. </p> <p>Deep Learning Renaissance</p> 2006<p> Sparked the modern deep learning era by showing that deep networks could be trained efficiently. Geoffrey Hinton and colleagues introduce deep belief networks, demonstrating effective pre-training for deep neural networks. Hinton, G. E., Osindero, S., &amp; Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. </p> <p>AlexNet and the ImageNet Breakthrough</p> 2012<p> Demonstrated the superiority of deep learning in computer vision, leading to widespread adoption. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton\u2019s AlexNet wins the ImageNet competition, achieving unprecedented accuracy in image classification using deep CNNs. Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. </p> <p>Generative Adversarial Networks (GANs)</p> 2014<p> Introduced a novel approach to generative modeling, enabling the creation of realistic synthetic data. Ian Goodfellow and colleagues introduce GANs, a framework for training generative models using adversarial training. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). Generative adversarial nets. </p> <p>DeepMind\u2019s AlphaGo</p> 2015<p> Showcased deep learning\u2019s ability to tackle complex strategic games, advancing AI research. DeepMind\u2019s AlphaGo, using deep reinforcement learning and neural networks, defeats professional Go player Lee Sedol. Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. </p> <p>Transformers and Attention Mechanisms</p> 2017<p> Revolutionized natural language processing and sequence modeling, enabling breakthroughs in machine translation and text generation. Ashish Vaswani and colleagues introduce the Transformer architecture, which uses self-attention mechanisms to process sequences in parallel, significantly improving performance in NLP tasks. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., &amp; Polosukhin, I. (2017). Attention is all you need.  </p> <p>BERT and Pre-trained Language Models</p> 2018<p> Set new standards in NLP by introducing pre-training and fine-tuning techniques, enabling models to understand context and semantics better. Jacob Devlin and colleagues introduce BERT (Bidirectional Encoder Representations from Transformers), a pre-trained language model that achieves state-of-the-art results on various NLP benchmarks. Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. </p> : <p>GPT-3 and Large Language Models</p> 2020<p> Showcased the capabilities of large-scale language models, enabling advancements in natural language understanding and generation. OpenAI releases GPT-3, a 175 billion parameter language model, demonstrating impressive performance in various NLP tasks, including text generation, translation, and question answering. Brown, T. B., Mann, B., Ryder, N., Subbiah, S., Kaplan, J., Dhariwal, P., Neelakantan, S., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, D., Litwin, M., Gray, S., Chess, B., Clark, J., Berridge, S., Zaremba, W., &amp; Amodei, D. (2020). Language models are few-shot learners. </p> <p>DALL-E and Image Generation</p> 2021<p> Enabled the generation of high-quality images from textual descriptions, showcasing the potential of multimodal AI. OpenAI introduces DALL-E, a model capable of generating images from textual descriptions, demonstrating the power of combining language and vision. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., &amp; Sutskever, I. (2021). Zero-Shot Text-to-Image Generation. </p> <ol> <li> <p>Hodgkin\u2013Huxley model. Alan Hodgkin and Andrew Huxley develop a mathematical model of the action potential in neurons, describing how neurons transmit signals through electrical impulses. This model is foundational for understanding neural dynamics and influences the development of artificial neural networks. Hodgkin, A. L., Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. .\u00a0\u21a9</p> </li> <li> <p>Visual Cortex and Monocular Deprivation. David H. Hubel and Torsten N. Wiesel conduct pioneering research on the visual cortex of cats, demonstrating how visual experience shapes neural development. Their work on monocular deprivation shows that depriving one eye of visual input during a critical period leads to permanent changes in the visual cortex, highlighting the importance of experience in neural plasticity. Hubel, D. H., &amp; Wiesel, T. N. (1963). Effects of monocular deprivation in kittens. .\u00a0\u21a9</p> </li> <li> <p>Neocognitron. Kunihiko Fukushima develops the Neocognitron, an early convolutional neural network (CNN) model that mimics the hierarchical structure of the visual cortex. This model is a precursor to modern CNNs and demonstrates the potential of hierarchical feature extraction in image recognition tasks. Fukushima, K. (1980). Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. .\u00a0\u21a9</p> </li> <li> <p>Hopfield Networks. John Hopfield introduces Hopfield networks, a type of recurrent neural network that can serve as associative memory systems. These networks are capable of storing and recalling patterns, laying the groundwork for later developments in neural network architectures. Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. .\u00a0\u21a9</p> </li> <li> <p>Self-Organizing Maps (SOM). Teuvo Kohonen develops Self-Organizing Maps, a type of unsupervised learning algorithm that maps high-dimensional data onto a lower-dimensional grid. SOMs are used for clustering and visualization of complex data, providing insights into the structure of the data. Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. .\u00a0\u21a9</p> </li> <li> <p>Long Short-Term Memory (LSTM) Networks. Sepp Hochreiter and J\u00fcrgen Schmidhuber introduce LSTM networks, a type of recurrent neural network designed to learn long-term dependencies in sequential data. This architecture addresses the vanishing gradient problem in RNNs, enabling effective modeling of long-term dependencies in sequential data. Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. \".\u00a0\u21a9</p> </li> <li> <p>Residual Networks (ResNets). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun introduce Residual Networks (ResNets), a deep learning architecture that uses skip connections to allow gradients to flow more easily through deep networks. This architecture enables the training of very deep neural networks, significantly improving performance on image recognition tasks. He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Deep residual learning for image recognition. \u21a9</p> </li> </ol>"}, {"location": "classes/clip/", "title": "15. CLIP", "text": "<p>More about NLP</p> <p>Check out Tiago Tavares' NLP course that covers Transformers and other advanced NLP topics: https://tiagoft.github.io/nlp_course/.</p> <p>CLIP (Contrastive Language-Image Pretraining) is a multimodal machine learning model developed by OpenAI in 2021. It bridges the gap between vision and language by jointly training an image encoder and a text encoder on a massive dataset of image-text pairs scraped from the internet (around 400 million pairs). The core idea is to learn representations where images and their corresponding textual descriptions are embedded close together in a shared latent space, while non-matching pairs are pushed apart. This enables zero-shot learning capabilities, meaning CLIP can perform tasks like image classification without being explicitly trained on labeled data for those tasks\u2014simply by comparing image embeddings to text embeddings of class descriptions.</p>"}, {"location": "classes/clip/#key-components", "title": "Key Components:", "text": "<ul> <li>Image Encoder: Typically a Vision Transformer (ViT) or a modified ResNet that processes images into fixed-dimensional embeddings (e.g., 512 or 768 dimensions).</li> <li>Text Encoder: A Transformer-based model (like a modified GPT or BERT variant) that encodes text captions into embeddings of the same dimensionality.</li> <li>Training Objective: Contrastive loss (specifically, a symmetric version of InfoNCE loss). For a batch of N image-text pairs, it computes a similarity matrix between all image and text embeddings, treats the diagonal (matching pairs) as positives, and off-diagonals as negatives. The goal is to maximize similarity for positives and minimize for negatives.</li> <li>Inference: To classify an image, encode it and compare its embedding (via cosine similarity) to encoded text prompts like \"a photo of a [class]\". The highest similarity wins.</li> </ul> <p>CLIP architecture overview. During training, image and text encoders are trained jointly with contrastive loss on image-text pairs. (from OpenAI's CLIP paper<sup>2</sup>)</p> <p></p> <p>CLIP architecture overview. At inference, image embeddings are compared to text embeddings of class prompts for zero-shot classification. (from OpenAI's CLIP paper<sup>2</sup>)</p> <p>CLIP's strength lies in its scalability and generalization. It doesn't require task-specific fine-tuning and can handle open-vocabulary tasks, but it has limitations like sensitivity to prompt engineering and biases from internet data.</p> <p>An ImageNet model is good at predicting the 1000 ImageNet categories, but that\u2019s all it can do \u201cout of the box.\u201d If we wish to perform any other task, an ML practitioner needs to build a new dataset, add an output head, and fine-tune the model. In contrast, CLIP can be adapted to perform a wide variety of visual classification tasks without needing additional training examples. To apply CLIP to a new task, all we need to do is \u201ctell\u201d CLIP\u2019s text-encoder the names of the task\u2019s visual concepts, and it will output a linear classifier of CLIP\u2019s visual representations. The accuracy of this classifier is often competitive with fully supervised models.<sup>1</sup></p> <p>Limitations of CLIP</p> <p>While CLIP usually performs well on recognizing common objects, it struggles on more abstract or systematic tasks such as counting the number of objects in an image and on more complex tasks such as predicting how close the nearest car is in a photo. On these two datasets, zero-shot CLIP is only slightly better than random guessing. Zero-shot CLIP also struggles compared to task specific models on very fine-grained classification, such as telling the difference between car models, variants of aircraft, or flower species.</p> <p>CLIP also still has poor generalization to images not covered in its pre-training dataset. For instance, although CLIP learns a capable OCR system, when evaluated on handwritten digits from the MNIST dataset, zero-shot CLIP only achieves 88% accuracy, well below the 99.75% of humans on the dataset. Finally, we\u2019ve observed that CLIP\u2019s zero-shot classifiers can be sensitive to wording or phrasing and sometimes require trial and error \u201cprompt engineering\u201d to perform well.<sup>1</sup></p>"}, {"location": "classes/clip/#numerical-simulation-of-clips-contrastive-loss", "title": "Numerical Simulation of CLIP's Contrastive Loss", "text": "<p>To illustrate how CLIP works numerically, let's simulate a tiny batch with 3 image-text pairs. We'll assume pre-computed embeddings (in practice, these come from the encoders). Each embedding is a 3D vector for simplicity (real CLIP uses higher dimensions like 512).</p>"}, {"location": "classes/clip/#setup", "title": "Setup:", "text": "<ul> <li> <p>Image embeddings (I):  </p> <p>\\( I_1 = [1.0, 0.0, 0.0] \\)  (e.g., for \"cat\") \\( I_2 = [0.0, 1.0, 0.0] \\)  (e.g., for \"dog\") \\( I_3 = [0.0, 0.0, 1.0] \\)  (e.g., for \"bird\")</p> </li> <li> <p>Text embeddings (T):  </p> <p>\\( T_1 = [0.9, 0.1, 0.0] \\)  (close to \\(I_1\\)) \\( T_2 = [0.1, 0.8, 0.1] \\)  (close to \\(I_2\\)) \\( T_3 = [0.0, 0.3, 0.7] \\)  (close to \\(I_3\\))  </p> </li> <li> <p>Batch size (\\(N\\)): \\(3\\)</p> </li> <li> <p>Temperature (\\(\\tau\\)): \\(0.07\\) (a hyperparameter to scale logits; common in CLIP).</p> </li> </ul>"}, {"location": "classes/clip/#step-by-step-calculation", "title": "Step-by-Step Calculation:", "text": "<ol> <li> <p>Normalize Embeddings:</p> <p>CLIP uses L2-normalized embeddings for cosine similarity. Here, they're already unit-length for simplicity (assume they are).</p> </li> <li> <p>Compute Similarity Matrix (Logits):</p> <p>Similarity = \\( \\displaystyle \\frac{(I \\cdot T)}{\\tau} \\)  (dot product scaled by \u03c4).</p> <p>Calculations:</p> <p>\\( \\begin{align*} \\text{Logits}_{I \\to T} &amp;= \\begin{bmatrix} \\text{sim}(I_1, T_1) &amp; \\text{sim}(I_1, T_2) &amp; \\text{sim}(I_1, T_3) \\\\ \\text{sim}(I_2, T_1) &amp; \\text{sim}(I_2, T_2) &amp; \\text{sim}(I_2, T_3) \\\\ \\text{sim}(I_3, T_1) &amp; \\text{sim}(I_3, T_2) &amp; \\text{sim}(I_3, T_3) \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1 \\cdot 0.9 + 0 \\cdot 0.1 + 0 \\cdot 0.0}{0.07} &amp; \\frac{1 \\cdot 0.1 + 0 \\cdot 0.8 + 0 \\cdot 0.1}{0.07} &amp; \\frac{1 \\cdot 0.0 + 0 \\cdot 0.3 + 0 \\cdot 0.7}{0.07} \\\\ \\frac{0 \\cdot 0.9 + 1 \\cdot 0.1 + 0 \\cdot 0.0}{0.07} &amp; \\frac{0 \\cdot 0.1 + 1 \\cdot 0.8 + 0 \\cdot 0.1}{0.07} &amp; \\frac{0 \\cdot 0.0 + 1 \\cdot 0.3 + 0 \\cdot 0.7}{0.07} \\\\ \\frac{0 \\cdot 0.9 + 0 \\cdot 0.1 + 1 \\cdot 0.0}{0.07} &amp; \\frac{0 \\cdot 0.1 + 0 \\cdot 0.8 + 1 \\cdot 0.1}{0.07} &amp; \\frac{0 \\cdot 0.0 + 0 \\cdot 0.3 + 1 \\cdot 0.7}{0.07} \\end{bmatrix} \\\\ &amp;\\approx \\begin{bmatrix} 12.857 &amp;  1.4286 &amp;  0 \\\\ 1.4286 &amp; 11.4286 &amp;  4.2857 \\\\ 0 &amp;  1.4286 &amp; 10 \\end{bmatrix} \\end{align*} \\)</p> <p>Full image-to-text logit matrix:  </p> <p>\\( \\text{Logits}_{I \\to T} \\approx \\begin{bmatrix} 12.857 &amp;  1.4286 &amp;  0 \\\\ 1.4286 &amp; 11.4286 &amp;  4.2857 \\\\ 0 &amp;  1.4286 &amp; 10 \\end{bmatrix} \\)</p> <p>CLIP averages both directions, text-to-image logits are the transpose:</p> \\[ \\text{Logits}_{T \\to I} = \\text{Logits}_{I \\to T}^T \\] </li> <li> <p>Softmax for Probabilities:</p> <p>For each row (image), softmax over logits to get probabilities of matching texts.  </p> <p>\\( \\displaystyle \\text{Softmax}(I) = \\frac{e^{I_i}}{\\sum_{j} e^{I_j}} \\)</p> <p>Calculating exponentials and normalizing:</p> <p>\\( \\begin{align*} \\sum_{j} e^{I_j} &amp;\\approx \\begin{bmatrix} e^{12.857} +  e^{1.4286} + e^{0} \\\\ e^{1.4286} +  e^{11.4286} + e^{4.2857} \\\\ e^{0} +  e^{1.4286} + e^{10} \\end{bmatrix} \\\\ &amp;\\approx \\begin{bmatrix} 383523 \\\\ 91987 \\\\ 22031 \\end{bmatrix} \\end{align*} \\)</p> <p>Then:</p> <p>\\( \\begin{align*} \\text{Softmax}(I) &amp;\\approx \\begin{bmatrix} \\frac{e^{12.857}}{383523} &amp;  \\frac{e^{1.4286}}{383523} &amp;  \\frac{e^{0}}{383523} \\\\ \\frac{e^{1.4286}}{91987} &amp;  \\frac{e^{11.4286}}{91987} &amp;  \\frac{e^{4.2857}}{91987} \\\\ \\frac{e^{0}}{22031} &amp;  \\frac{e^{1.4286}}{22031} &amp;  \\frac{e^{10}}{22031} \\end{bmatrix} \\\\ &amp;\\approx \\begin{bmatrix} 0.9999 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.9992 &amp; 0.0008 \\\\ 0 &amp; 0.0002 &amp; 0.9998 \\end{bmatrix} \\end{align*} \\)</p> <p>The diagonal should have high probs.</p> </li> <li> <p>Contrastive Loss:</p> <p>Negative log-likelihood of correct labels (diagonal).  </p> \\[ \\mathcal{L}_{I \\to T} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(p_{i \\to t}) \\] <p>For this batch:  </p> <p>\\( \\mathcal{L}_{I_1 \\to T_1} = \\log(0.9999) \\approx -0.0000 \\)</p> <p>\\( \\mathcal{L}_{I_2 \\to T_2} = \\log(0.9992) \\approx -0.0004 \\)</p> <p>\\( \\mathcal{L}_{I_3 \\to T_3} = \\log(0.9998) \\approx -0.0001 \\)</p> <p>\\(\\mathcal{L}_{I \\to T} \\approx 0.00016\\) (very low loss since embeddings are well-aligned).</p> <p>CLIP computes symmetric loss:</p> \\[ \\displaystyle \\mathcal{L} = \\frac{1}{2} \\left( \\mathcal{L}_{I \\to T} + \\mathcal{L}_{T \\to I} \\right). \\] <p>In training, gradients update encoders to minimize this. If embeddings were misaligned (e.g., I1 close to T2), loss would be higher.</p> </li> </ol> <p>This is a simplified simulation; real CLIP handles large batches (e.g., 32k) and uses distributed training.</p>"}, {"location": "classes/clip/#additional", "title": "Additional", "text": ""}, {"location": "classes/clip/#l2-normalized-embeddings", "title": "L2-normalized embeddings", "text": "<p>L2-normalized embeddings are vectors whose length is scaled to a unit of 1, meaning their L2 norm (Euclidean length) is equal to one. This is achieved by dividing each component of the original vector by its total L2 norm, making it a common method for ensuring consistent magnitude and improving the effectiveness of distance-based similarity measures like cosine similarity<sup>4</sup>. </p> <p>How it works</p> <ol> <li> <p>Calculate the L2 norm:</p> <p>For a vector \\(v=[v_{1},v_{2},...,v_{n}]\\), the L2 norm (\\(||v||_{2}\\)) is the square root of the sum of the squares of its components: \\(||v||_{2}=\\sqrt{v_{1}^{2}+v_{2}^{2}+...+v_{n}^{2}}\\).</p> </li> <li> <p>Divide each component:</p> <p>Each element of the vector is then divided by this calculated L2 norm. The resulting normalized vector, \\(v^{\\prime }\\), is:</p> <p>\\(v^{\\prime }=[\\frac{v_{1}}{||v||_{2}},\\frac{v_{2}}{||v||_{2}},...,\\frac{v_{n}}{||v||_{2}}]\\).\u00a0</p> </li> </ol> <p>Why it is used</p> <ul> <li>Focus on direction: It helps models focus on the \"direction\" of the vector in a high-dimensional space rather than its magnitude, which can be useful when the magnitude doesn't carry meaningful information. </li> <li>Improves similarity measures: Normalization is crucial for techniques that rely on cosine similarity. L2-normalized embeddings make the similarity score equal to the dot product, simplifying calculations and comparison. </li> <li>Prevents magnitude bias: It ensures that embeddings with large magnitudes don't dominate similarity comparisons, preventing bias from large values. </li> <li>Used in model architecture: Some models use L2 normalization as a constraint to keep embeddings on a hypersphere, which can be beneficial for tasks like face recognition or out-of-distribution detection. </li> </ul> <p>When to use it</p> <ul> <li>When using cosine similarity for tasks like retrieval or recommendation. </li> <li>In deep learning models where the magnitude of the weights can grow uncontrollably and affect performance. </li> <li>When you want to constrain the representation space to a sphere, as it can lead to more stable training. </li> </ul> <ol> <li> <p>CLIP: Connecting Text and Images \u21a9\u21a9</p> </li> <li> <p>Learning Transferable Visual Models From Natural Language Supervision, Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever, 2021.\u00a0\u21a9\u21a9</p> </li> <li> <p>How to Normalize a Vector, Nextbridge.\u00a0\u21a9</p> </li> <li> <p>Cosine Similarity, GeeksforGeeks.\u00a0\u21a9</p> </li> </ol>"}, {"location": "classes/clip/code/", "title": "Code", "text": ""}, {"location": "classes/clip/code/#vanilla-python-implementation", "title": "Vanilla Python Implementation", "text": "<p>Below is a basic \"vanilla\" Python implementation of a toy CLIP-like model using only NumPy (no Torch or other frameworks for the core logic, keeping it simple and from-scratch). This simulates the contrastive loss computation and a minimal training loop on dummy data. In a real scenario, you'd need full encoders, but here we use random vectors as \"embeddings\" for illustration.</p> <pre><code>import numpy as np\n\ndef compute_cosine_similarity(images, texts):\n    # Normalize to unit length\n    images = images / np.linalg.norm(images, axis=1, keepdims=True)\n    texts = texts / np.linalg.norm(texts, axis=1, keepdims=True)\n    return np.dot(images, texts.T)\n\ndef clip_loss(images, texts, temperature=0.07):\n    N = images.shape[0]\n    logits = compute_cosine_similarity(images, texts) / temperature\n\n    # Image-to-text loss\n    labels = np.arange(N)\n    log_probs_i2t = logits - np.max(logits, axis=1, keepdims=True)\n    log_probs_i2t = np.exp(log_probs_i2t) / np.sum(np.exp(log_probs_i2t), axis=1, keepdims=True)\n    loss_i2t = -np.mean(np.log(log_probs_i2t[np.arange(N), labels]))\n\n    # Text-to-image loss (symmetric)\n    logits_t = logits.T\n    log_probs_t2i = logits_t - np.max(logits_t, axis=1, keepdims=True)\n    log_probs_t2i = np.exp(log_probs_t2i) / np.sum(np.exp(log_probs_t2i), axis=1, keepdims=True)\n    loss_t2i = -np.mean(np.log(log_probs_t2i[np.arange(N), labels]))\n\n    return (loss_i2t + loss_t2i) / 2\n\n# Dummy data: 4 pairs, 5D embeddings\nnp.random.seed(42)\nimage_embeds = np.random.randn(4, 5)\ntext_embeds = image_embeds + np.random.randn(4, 5) * 0.1  # Slightly perturb texts to simulate matches\n\n# \"Train\" by nudging text embeds toward images (toy gradient descent)\nlearning_rate = 0.01\nfor epoch in range(10):\n    loss = clip_loss(image_embeds, text_embeds)\n    print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n\n    # Toy update: Move texts closer (not real backprop)\n    similarity = compute_cosine_similarity(image_embeds, text_embeds)\n    grad = (text_embeds - image_embeds) / np.linalg.norm(text_embeds - image_embeds, axis=1, keepdims=True)\n    text_embeds -= learning_rate * grad\n</code></pre>"}, {"location": "classes/clip/code/#explanation-of-the-code", "title": "Explanation of the Code:", "text": "<ul> <li>compute_cosine_similarity: Computes dot products after normalization (cosine sim).</li> <li>clip_loss: Implements the symmetric contrastive loss. It calculates logits, applies stable softmax, and averages the cross-entropy losses for both directions.</li> <li>Dummy Training Loop: Starts with random embeddings, computes loss, and naively updates text embeddings to align better (not real optimization; in practice, use SGD on encoder params).</li> <li>To arrive at this: Start with the InfoNCE formula, derive the symmetric version from CLIP's paper, implement softmax carefully to avoid overflow (subtract max), and average losses.</li> </ul> <p>If you run this, the loss decreases over epochs as alignments improve. For a full CLIP, replace dummy embeds with actual encoder outputs and train on real data. Let me know if you'd like expansions!</p> <p>** This is a simplified simulation; real CLIP handles large batches (e.g., 32k) and uses distributed training.</p>"}, {"location": "classes/concepts/", "title": "1. Concepts", "text": "<p>Artificial Intelligence (AI) is a broad field that encompasses various approaches and techniques for creating intelligent systems capable of performing tasks that typically require human intelligence. These tasks include reasoning, learning, perception, and decision-making.</p> <p>AI can be categorized into three main paradigms, each with its own strengths and weaknesses: Symbolic AI, Connectionist AI, and Neuro-Symbolic AI. Each of these paradigms has its own strengths and weaknesses, and they are often used in different contexts depending on the problem being addressed.</p>"}, {"location": "classes/concepts/#ai-paradigms", "title": "AI Paradigms", "text": "Paradigm Description Symbolic AI Focuses on high-level reasoning and knowledge representation using symbols and rules. It excels in tasks requiring logical reasoning, such as theorem proving and expert systems. However, it struggles with perception and learning from raw data. Examples include logic-based systems, expert systems, and knowledge graphs. Connectionist AI Based on artificial neural networks (ANNs), it excels in pattern recognition, learning from large datasets, and handling noisy data. It is particularly effective in tasks like image and speech recognition. However, it often lacks interpretability and struggles with reasoning tasks. Examples include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers. Neuro-Symbolic AI Combines the strengths of both symbolic and connectionist AI, aiming to create systems that can reason about complex problems while also learning from data. It leverages symbolic reasoning capabilities alongside neural networks to enhance interpretability and reasoning abilities. Examples include neuro-symbolic systems that integrate symbolic logic with neural networks, such as knowledge-augmented language models and graph neural networks. 2025-11-06T12:09:01.411778 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Neuro-Symbolic AI combines symbolic reasoning with neural networks, leveraging the strengths of both approaches. It aims to create systems that can reason about complex problems while also learning from data.</p> <p>This approach is particularly useful in tasks that require both high-level reasoning and the ability to learn from raw data, such as natural language understanding and complex decision-making.</p> <p>There are several approaches to implementing AI. Machine learning (ML) is one of the most common methods, where algorithms learn from data to make predictions or decisions. Neural networks, a subset of ML, are inspired by the structure and function of the human brain and are particularly effective in tasks like image and speech recognition. Deep learning, a more advanced form of neural networks, uses multiple layers of processing to extract complex patterns from large datasets.</p> 2025-11-06T12:09:01.520605 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"}, {"location": "classes/concepts/#machine-learning", "title": "Machine Learning", "text": "<p>In the context of AI, machine learning (ML) techniques are used to enable systems to learn from data and improve their performance over time without being explicitly programmed. These techniques allow AI systems to adapt and generalize from examples, making them capable of handling a wide range of tasks, from image recognition to natural language processing.</p> <p>The techniques are often split into two main categories: supervised learning and unsupervised learning.</p> <p>Supervised Learning</p> <p>Supervised learning involves training a model on labeled data, where the input data is paired with the correct output. This allows the model to learn patterns and make predictions based on new, unseen data.</p> <p>This approach is particularly effective when there is a clear relationship between the input features and the output labels, allowing the model to generalize from the training data to make accurate predictions on new data. Examples include classification tasks (e.g., identifying objects in images) and regression tasks (e.g., predicting house prices based on features).</p> <p>Unsupervised Learning</p> <p>Unsupervised learning, on the other hand, involves training a model on unlabeled data, where the model must find patterns and relationships within the data without explicit guidance.</p> <p>This approach is useful for discovering hidden structures in data, such as clusters or groups, without prior knowledge of the labels. It is often used in exploratory data analysis and feature extraction. Examples include clustering tasks (e.g., grouping similar documents) and dimensionality reduction tasks (e.g., reducing the number of features in a dataset while preserving important information).</p> <p>There are also semi-supervised learning techniques, which combine both labeled and unlabeled data to improve model performance. This approach is particularly useful when labeled data is scarce or expensive to obtain, allowing the model to leverage the abundance of unlabeled data to enhance its learning.</p> <p>Also, there are reinforcement learning techniques, where an agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. This approach is particularly effective for tasks that involve sequential decision-making, such as game playing or robotic control.</p> <p>Machine learning techniques address a wide range of problems, primarily through classification and regression, which are core supervised learning tasks. Classification involves predicting discrete labels or categories based on input features, while regression focuses on predicting continuous values. These approaches are extensively applied across domains such as image recognition, natural language processing, and time series forecasting. However, machine learning also includes other techniques like clustering, dimensionality reduction, reinforcement learning, and anomaly detection, expanding its applicability to diverse challenges.</p> <p>Few examples of machine learning techniques include:</p> Technique Description Decision Trees A tree-like model used for classification and regression tasks, where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome. Random Forest An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting. It works by training multiple decision trees on different subsets of the data and averaging their predictions. Support Vector Machines (SVM) A supervised learning algorithm that finds the optimal hyperplane to separate different classes in the feature space. It is effective for high-dimensional data and can handle both linear and non-linear classification tasks. K-Nearest Neighbors (KNN) A simple algorithm that classifies new instances based on the majority class of their k-nearest neighbors in the feature space. It is a non-parametric method that can be used for both classification and regression tasks. Naive Bayes A probabilistic classifier based on Bayes' theorem, assuming independence between features. It is particularly effective for text classification tasks, such as spam detection and sentiment analysis. Linear Regression A statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. It is commonly used for predicting continuous outcomes based on input features. Logistic Regression A statistical method used for binary classification tasks, where the output is a probability that can be mapped to two classes. It models the relationship between input features and the log-odds of the outcome using a logistic function. K-Means Clustering An unsupervised learning algorithm that partitions data into k clusters based on feature similarity. It iteratively assigns data points to the nearest cluster centroid and updates the centroids until convergence. Principal Component Analysis (PCA) A dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving important features. It identifies the principal components that capture the most variance in the data, making it useful for visualization and feature extraction. Gradient Boosting An ensemble learning technique that builds a series of weak learners (usually decision trees) in a sequential manner, where each new learner corrects the errors of the previous ones. It is effective for both classification and regression tasks and is widely used in machine learning competitions."}, {"location": "classes/concepts/#neural-networks", "title": "Neural Networks", "text": "<p>Neural networks are a class of machine learning models inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) organized in layers, where each connection has an associated weight that is adjusted during training. Neural networks are particularly effective for tasks involving complex patterns, such as image and speech recognition.</p> <p>Neural networks can be categorized into several types, including: </p> <ul> <li>Feedforward Neural Networks (FNNs): The simplest type of neural network where information flows in one direction, from input to output, without cycles. They are commonly used for tasks like classification and regression.</li> <li>Convolutional Neural Networks (CNNs): Specialized neural networks designed for processing grid-like data, such as images. They use convolutional layers to automatically learn spatial hierarchies of features, making them highly effective for image recognition tasks.</li> <li>Recurrent Neural Networks (RNNs): Neural networks designed for sequential data, such as time series or natural language. They have connections that loop back on themselves, allowing them to maintain a memory of previous inputs. This makes them suitable for tasks like language modeling and speech recognition.</li> <li>Transformers: A type of neural network architecture that uses self-attention mechanisms to process sequences of data. They have revolutionized natural language processing tasks, enabling models like BERT and GPT to achieve state-of-the-art performance in various language understanding tasks.</li> </ul>"}, {"location": "classes/concepts/#deep-learning", "title": "Deep Learning", "text": "<p>Deep learning is a subset of machine learning that focuses on using deep neural networks with many layers to learn complex representations of data. It has achieved remarkable success in various domains, including computer vision, natural language processing, and speech recognition. Deep learning models are capable of automatically learning hierarchical features from raw data, eliminating the need for manual feature engineering. This has led to significant advancements in AI applications, enabling systems to perform tasks that were previously considered challenging or impossible.</p>"}, {"location": "classes/concepts/#additional-resources", "title": "Additional Resources", "text": "<ol> <li> <p>Wiki - Neuro-Symbolic AI \u21a9</p> </li> <li> <p>2020, Forbes - Symbolism Versus Connectionism In AI: Is There A Third Way? \u21a9</p> </li> <li> <p>Garcez, A.d., Lamb, L.C. Neurosymbolic AI: the 3rd wave. Artif Intell Rev 56, 12387\u201312406 (2023). doi.org/10.1007/s10462-023-10448-w \u21a9</p> </li> </ol>"}, {"location": "classes/convolutional-neural-networks/", "title": "11. Convolutional", "text": ""}, {"location": "classes/convolutional-neural-networks/#introduction-to-convolutional-neural-networks-cnns", "title": "Introduction to Convolutional Neural Networks (CNNs)", "text": "<p>Convolutional Neural Networks (CNNs) are a class of deep neural networks commonly used for image recognition, video analysis, and other tasks involving grid-like data (e.g., pixels in images). Unlike fully connected networks, CNNs exploit spatial hierarchies through convolutions, which apply learnable filters (kernels) to local regions of the input. This reduces parameters, enables translation invariance, and captures features like edges, textures, or objects.</p> <p>A typical CNN architecture includes:</p> <ul> <li>Convolutional layers: Extract features via convolution.</li> <li>Activation functions (e.g., ReLU): Introduce non-linearity.</li> <li>Pooling layers (e.g., max pooling): Downsample to reduce spatial dimensions and computational load.</li> <li>Fully connected layers: For final classification or regression.</li> <li>Output layer: Often softmax for classification.</li> </ul> <p>Training involves the forward pass (computing predictions) and backward pass (backpropagation to update weights via gradients). Below, I'll focus on the math for the convolutional layer, as it's the core of CNNs. I'll assume 2D convolution for images (input shape: batch_size \u00d7 channels \u00d7 height \u00d7 width).</p>"}, {"location": "classes/convolutional-neural-networks/#forward-pass-in-a-convolutional-layer", "title": "Forward Pass in a Convolutional Layer", "text": "<p>The forward pass computes the output feature map by sliding a kernel over the input.</p>"}, {"location": "classes/convolutional-neural-networks/#key-notations", "title": "Key Notations:", "text": "<ul> <li>Input: \\( X \\in \\mathbb{R}^{B \\times C_{in} \\times H_{in} \\times W_{in}} \\) (batch size \\( B \\), input channels \\( C_{in} \\), height \\( H_{in} \\), width \\( W_{in} \\)).</li> <li>Kernel (weights): \\( W \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w} \\) (output channels \\( C_{out} \\), kernel height \\( K_h \\), width \\( K_w \\)).</li> <li>Bias: \\( b \\in \\mathbb{R}^{C_{out}} \\) (one per output channel).</li> <li>Stride: \\( s \\) (step size for sliding the kernel).</li> <li>Padding: \\( p \\) (zeros added to borders to control output size).</li> <li>Output: \\( Y \\in \\mathbb{R}^{B \\times C_{out} \\times H_{out} \\times W_{out}} \\), where \\( H_{out} = \\lfloor \\frac{H_{in} + 2p - K_h}{s} \\rfloor + 1 \\), similarly for \\( W_{out} \\).</li> </ul>"}, {"location": "classes/convolutional-neural-networks/#convolution-operation", "title": "Convolution Operation:", "text": "<p>For each position \\( (i, j) \\) in the output feature map, for batch item \\( b \\) and output channel \\( c_{out} \\):</p> \\[ Y_{b, c_{out}, i, j} = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} W_{c_{out}, c_{in}, m, n} \\cdot X_{b, c_{in}, s \\cdot i + m - p, s \\cdot j + n - p} + b_{c_{out}} \\] <p>This is essentially a dot product between the kernel and a local patch of the input, summed over input channels, plus bias.</p> <ul> <li>How to arrive at this: The formula derives from cross-correlation (convolution is flipped cross-correlation, but in ML, we often use cross-correlation for simplicity). The indices ensure the kernel slides without going out of bounds (padding handles edges). For valid padding (\\( p=0 \\)), output shrinks; for same padding, \\( p = \\lfloor \\frac{K-1}{2} \\rfloor \\) keeps size.</li> </ul> <p>After convolution, apply activation: \\( Y' = f(Y) \\), e.g., ReLU: \\( f(x) = \\max(0, x) \\).</p> <p>Pooling (e.g., max pooling) over a window (size \\( k \\), stride \\( s \\)) takes the max value in each patch, reducing dimensions.</p>"}, {"location": "classes/convolutional-neural-networks/#backward-pass-in-a-convolutional-layer", "title": "Backward Pass in a Convolutional Layer", "text": "<p>The backward pass computes gradients for weights, biases, and inputs using chain rule, to minimize loss \\( L \\) via gradient descent.</p>"}, {"location": "classes/convolutional-neural-networks/#key-notations_1", "title": "Key Notations:", "text": "<ul> <li>Incoming gradient from next layer: \\( \\frac{\\partial L}{\\partial Y} \\in \\mathbb{R}^{B \\times C_{out} \\times H_{out} \\times W_{out}} \\) (gradient w.r.t. output).</li> <li>We need:</li> <li>\\( \\frac{\\partial L}{\\partial W} \\) (weight gradient).</li> <li>\\( \\frac{\\partial L}{\\partial b} \\) (bias gradient).</li> <li>\\( \\frac{\\partial L}{\\partial X} \\) (input gradient, passed to previous layer).</li> </ul>"}, {"location": "classes/convolutional-neural-networks/#bias-gradient", "title": "Bias Gradient:", "text": "<p>Simple sum over spatial dimensions and batch:</p> \\[ \\frac{\\partial L}{\\partial b_{c_{out}}} = \\sum_{b=0}^{B-1} \\sum_{i=0}^{H_{out}-1} \\sum_{j=0}^{W_{out}-1} \\frac{\\partial L}{\\partial Y_{b, c_{out}, i, j}} \\] <ul> <li>How to arrive at this: Bias is added to every position, so its gradient is the sum of all output gradients per channel.</li> </ul>"}, {"location": "classes/convolutional-neural-networks/#weight-gradient", "title": "Weight Gradient:", "text": "<p>Correlate input with output gradient:</p> \\[ \\frac{\\partial L}{\\partial W_{c_{out}, c_{in}, m, n}} = \\sum_{b=0}^{B-1} \\sum_{i=0}^{H_{out}-1} \\sum_{j=0}^{W_{out}-1} \\frac{\\partial L}{\\partial Y_{b, c_{out}, i, j}} \\cdot X_{b, c_{in}, s \\cdot i + m - p, s \\cdot j + n - p} \\] <ul> <li>How to arrive at this: By chain rule, \\( \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial W} \\). Since \\( Y \\) is linear in \\( W \\), this is like convolving the input with the output gradient (but actually cross-correlation).</li> </ul>"}, {"location": "classes/convolutional-neural-networks/#input-gradient", "title": "Input Gradient:", "text": "<p>\"Full\" convolution of rotated kernel with output gradient (to propagate error back):</p> <p>First, pad the output gradient if needed. Then:</p> \\[ \\frac{\\partial L}{\\partial X_{b, c_{in}, k, l}} = \\sum_{c_{out}=0}^{C_{out}-1} \\sum_{m=0}^{K_h-1} \\sum_{n=0}^{K_w-1} \\frac{\\partial L}{\\partial Y_{b, c_{out}, i, j}} \\cdot W_{c_{out}, c_{in}, m, n} \\] <p>Where \\( i = \\lfloor \\frac{k + p - m}{s} \\rfloor \\), \\( j = \\lfloor \\frac{l + p - n}{s} \\rfloor \\), and only if the division is integer (i.e., aligns with stride).</p> <ul> <li>How to arrive at this: Chain rule: \\( \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial X} \\). This requires \"deconvolving\" or transposing the convolution. In practice, it's implemented as convolving the output gradient with a rotated (180\u00b0) kernel, with appropriate padding.</li> </ul> <p>If there's activation, multiply by its derivative (e.g., ReLU derivative: 1 if &gt;0, else 0).</p> <p>For pooling, backward pass upsamples the gradient (e.g., for max pooling, place gradient only at max position).</p>"}, {"location": "classes/convolutional-neural-networks/#example", "title": "Example", "text": "<p>https://colab.research.google.com/drive/1AOONNT2DS0xP6k3thCq8pog1PoJUKTe6#scrollTo=b2tjGQ76GbeD</p> <p>https://en.wikipedia.org/wiki/Region_Based_Convolutional_Neural_Networks</p> <p>https://ravjot03.medium.com/detecting-vehicles-in-videos-with-faster-r-cnn-a-step-by-step-guide-932bbabf978b</p> <p>https://cs231n.github.io/assignments2025/assignment3 https://cs231n.stanford.edu/assignments.html</p> <p>https://www.kaggle.com/code/christofhenkel/temporal-convolutional-network/notebook</p>"}, {"location": "classes/convolutional-neural-networks/#additional", "title": "Additional", "text": ""}, {"location": "classes/convolutional-neural-networks/#when-is-a-neural-network-considered-deep", "title": "When is a Neural Network Considered \"Deep\"?", "text": "<p>The term \"deep\" in the context of neural networks refers to the architecture's depth, specifically the number of layers (particularly hidden layers) that enable the network to learn hierarchical and abstract representations of data. There is no universally agreed-upon minimum threshold that strictly divides \"shallow\" from \"deep\" neural networks, as it can depend on the context, task, and historical usage in research. However, based on established definitions and expert consensus in machine learning, a neural network is generally considered deep if it has at least two hidden layers (in addition to the input and output layers), resulting in a total of at least four layers overall. This aligns with the concept of a substantial credit assignment path (CAP) depth greater than two, where the CAP represents the chain of transformations from input to output.</p> <ul> <li> <p>Shallow vs. Deep Networks</p> <ul> <li>A shallow neural network typically has 0 or 1 hidden layer (e.g., a basic perceptron or multilayer perceptron with one hidden layer). These are sufficient for simple tasks but struggle with complex, hierarchical data patterns.</li> <li>Deep networks, by contrast, stack multiple hidden layers to capture increasingly abstract features (e.g., edges in early layers for image recognition, evolving to objects in deeper layers). The \"deep\" descriptor emphasizes this multi-layer stacking, often with non-linear activations.</li> </ul> </li> <li> <p>Historical and Theoretical Basis</p> <ul> <li>Early deep learning models, such as those from Geoffrey Hinton's group in the 2000s, featured three hidden layers and were pivotal in reviving interest in deep architectures.</li> <li>J\u00fcrgen Schmidhuber's work defines \"very deep\" learning as having CAP depth &gt;10, but the baseline for \"deep\" starts at multiple non-linear layers (CAP &gt;2).</li> <li>The universal approximation theorem shows that even a single hidden layer can theoretically approximate any function, but in practice, deeper networks (with fewer neurons per layer) are more efficient for complex tasks, avoiding the need for exponentially more neurons in shallow setups.</li> </ul> </li> <li> <p>Common Thresholds from Sources</p> <ul> <li>Most researchers and textbooks agree on at least 2 hidden layers as the minimum for \"deep\" (total layers: input + 2 hidden + output = 4).</li> <li>Some sources, like IBM's overview, specify more than 3 total layers (inclusive of input and output), which equates to at least 1 hidden layer\u2014but this is often critiqued as too low, as it would classify basic MLPs as deep.</li> <li>In practice, modern deep networks (e.g., CNNs or transformers) have dozens or hundreds of layers, but the minimum remains the focus for foundational classification.</li> </ul> </li> <li> <p>Why the Ambiguity?</p> <ul> <li>The term \"deep\" originated as somewhat informal or \"marketing\" in early literature but has solidified around the multi-layer criterion.</li> <li>For recurrent neural networks (RNNs), depth can be effectively unlimited due to signal propagation through layers over time, but the static layer count still applies for feedforward cases.</li> <li>In educational or certification contexts (e.g., CFA materials), there's occasional debate between 2 or 3 hidden layers, but evidence leans toward 2 as the practical minimum.</li> </ul> </li> </ul> <p>If you're implementing a network, start with 2\u20133 hidden layers for \"deep\" experiments and tune based on your dataset and performance metrics (e.g., via cross-validation). For specific architectures like CNNs, the minimum might vary slightly due to convolutional layers.</p>"}, {"location": "classes/data/", "title": "2. Data", "text": "<p>All the concepts of Machine Learning are based on data. The quality and quantity of available data are fundamental to the success of any machine learning model. In this context, it is important to understand how data is structured, processed, and used to train models.</p>"}, {"location": "classes/data/#nature-of-data", "title": "Nature of Data", "text": "<p>Data can be thought of as a collection of features or attributes that describe a particular phenomenon or object. In machine learning, data is often represented as a matrix, where each row corresponds to an example and each column corresponds to a feature. This representation is known as the feature space.</p> <p>A feature is a measurable property or characteristic of the phenomenon being studied. Features can be numerical (e.g., height, weight) or categorical (e.g., color, type). The set of features used to describe the data is known as the feature set or feature vector. Features are used to describe the data and can be used as input to machine learning models. Each feature is a dimension of the feature space, and the dataset is represented as a point in this space.</p> <p>Features can be numerical or categorical:</p> <ul> <li>Numerical features are those that can take continuous values, such as height or weight;</li> <li>Categorical features are those that take discrete values, such as color or type.</li> </ul> <p>Depending on the type of machine learning algorithm, features may be treated differently. For example, some algorithms work better with numerical features, while others are more suited for categorical features. In this context, it is necessary to convert categorical features into a format that algorithms can understand, such as using one-hot encoding<sup>1</sup> or label encoding<sup>2</sup>.</p> <p>Additionally, numerical features, such as height or weight, are often normalized to ensure that all features contribute equally to the model. Normalization is a preprocessing technique that adjusts the values of features to a common scale, typically between 0 and 1 or -1 and 1. A common approach to normalization is min-max scaling, which transforms the data by subtracting the minimum value and dividing by the range (maximum - minimum). This ensures that all features are on the same scale and can improve the performance of many machine learning algorithms.</p>"}, {"location": "classes/data/#datasets", "title": "Datasets", "text": "<p>Data is often stored in datasets, which are structured collections of data that can be easily accessed, managed, and updated. Datasets can be relational (e.g., SQL databases) or non-relational (e.g., NoSQL databases). Relational datasets store data in tables with predefined schemas, while non-relational datasets allow for more flexible data structures. Some common types of datasets used in machine learning include:</p> <ul> <li>UCI Machine Learning Repository: a collection of datasets for machine learning tasks, including classification, regression, and clustering.</li> <li>Kaggle Datasets: a platform that offers a wide variety of datasets for different machine learning tasks, from text classification to image recognition.</li> <li>OpenML: a collaborative platform for sharing and organizing machine learning datasets and experiments.</li> <li>Google Dataset Search: a search engine for datasets across the web, allowing users to find datasets for various machine learning tasks.</li> <li>AWS Open Data Registry: a collection of publicly available datasets hosted on Amazon Web Services, covering a wide range of domains, including climate, healthcare, and transportation.</li> <li>Data.gov: a repository of datasets provided by the U.S. government, covering various topics such as agriculture, health, and energy.</li> <li>FiveThirtyEight Data: a collection of datasets used in articles by FiveThirtyEight, covering topics such as politics, sports, and economics.</li> <li>Awesome Public Datasets: a curated list of high-quality public datasets for various domains.</li> <li>The World Bank Open Data: a collection of global development data, including economic, social, and environmental indicators.</li> <li>IMDB Datasets: a collection of datasets related to movies, TV shows, and actors, useful for natural language processing and recommendation systems.</li> <li>Yelp Open Dataset: a dataset containing business reviews, user data, and check-ins, useful for sentiment analysis and recommendation systems.</li> </ul>"}, {"location": "classes/data/#data-quality", "title": "Data Quality", "text": "<p>Data quality is a critical aspect of machine learning, as the performance of models heavily depends on the quality of the data used for training. Poor quality data can lead to inaccurate predictions and unreliable models. Common issues with data quality include:</p> <ul> <li>Missing data: values that are not available for some variables;</li> <li>Duplicate data: records that appear more than once in the dataset;</li> <li>Noisy data: values that are inconsistent or incorrect;</li> <li>Imbalanced data: when one class is much more frequent than another, which can lead to a biased model.</li> <li>Inconsistent data: when the data does not follow a consistent pattern or format, making it difficult to analyze and train the model.</li> <li>Irrelevant data: variables that do not contribute to the machine learning task and may harm the model's performance.</li> </ul> <p>To address these issues, it is common to perform a data cleaning and preprocessing process, which may include:</p> <ul> <li>Removing missing data: excluding records with missing values or imputing values based on other observations.</li> <li>Removing duplicates: identifying and removing duplicate records.</li> <li>Handling noisy data: applying smoothing or filtering techniques to reduce noise in the data.</li> <li>Balancing classes: techniques such as undersampling or oversampling - data augmentation<sup>6</sup> - to deal with imbalanced classes.</li> <li>Normalization: adjusting the values of variables to a common scale, ensuring that all variables contribute equally to the model.</li> <li>Transforming variables: applying techniques such as logarithm, square root, or Box-Cox to transform non-linear variables into linear ones.</li> <li>Encoding categorical variables: converting categorical variables into a format that algorithms can understand, such as using one-hot encoding or label encoding.</li> </ul> <p>Additionally, it is important to consider the order of the data, especially in time series problems, where the sequence of the data is crucial for analysis and modeling.</p>"}, {"location": "classes/data/#data-volume-and-balance", "title": "Data Volume and Balance", "text": "<p>The volume and balance of data are also important factors to consider in machine learning. Data volume refers to the amount of data available for training and testing machine learning models. The larger the volume of data, the more information the model can learn, which usually results in better performance. However, it is also important to consider the quality of the data, as noisy or irrelevant data can harm the model's performance.</p> <p>Additionally, it is important to consider class balancing, especially in classification problems. Class balancing refers to the equitable distribution of classes in the dataset. If one class is much more frequent than another, this can lead to a biased model, which tends to predict the majority class. To address this issue, techniques such as undersampling or oversampling can be used to balance the classes. Undersampling involves removing records from the majority class, while oversampling involves duplicating records from the minority class or generating synthetic data.</p> <p>For supervised learning models, it is essential to have a labeled dataset, where each example has an input (features) and an output (label). This allows the model to learn to map the inputs to the correct outputs.</p> <p>Furthermore, the data can be classified into three main categories:</p> Set Description Train Used to train the model, allowing it to learn the patterns and relationships between features and labels. Test Used to tune the model's hyperparameters and prevent overfitting, ensuring it generalizes well to new examples. Validation Used to evaluate the model's performance on unseen data, ensuring it generalizes well to new examples."}, {"location": "classes/data/#some-examples-of-datasets", "title": "Some Examples of Datasets", "text": ""}, {"location": "classes/data/#salmon-vs-seabass", "title": "Salmon vs Seabass", "text": "<p>A fictional dataset about salmon and seabass, where each record is labeled as \"salmon\" or \"seabass\". The goal is to better understand how the data can be used to differentiate between the two species. In this context, the features may include, for example: size and brightness<sup>5</sup>.</p>"}, {"location": "classes/data/#problem", "title": "Problem", "text": "<p>Imagine you have a fish sorting machine. Every day, fishing boats dump tons of fish onto a conveyor belt, and the goal of the machine is to separate the fish, classifying them as \"salmon\" or \"seabass\" based on their characteristics.</p> <p>The conveyor belt has sensors that measure the size and brightness of the fish. Based on these measurements, the machine must decide whether the fish is a salmon or a seabass.</p> \\[ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\] <p>where \\(x_1\\) is the size of the fish and \\(x_2\\) is the brightness of the fish. The machine must learn to classify the fish based on these characteristics, using a function \\(f\\) that maps the input features to the output class: salmon or seabass.</p>"}, {"location": "classes/data/#sample-data", "title": "Sample Data", "text": "<p>To better understand the data, a sample of fish was taken, where each fish is described by its size and brightness characteristics. The table below presents a sample of the collected data:</p> Size (cm) Brightness (0-10) Species 60 6 salmon 45 5 seabass 78 7 salmon 90 5.2 salmon 71 9 salmon 80 3 seabass 64 6 salmon 58 2 seabass 63 6.8 seabass 50 4 seabass <p>When plotting the data, we can visualize the size and brightness of each fish in a two-dimensional space. Each fish is represented by a point in this space, where the x-axis represents the size and the y-axis represents the brightness. The points are colored according to their species: salmon or seabass.</p> 2025-11-06T12:09:01.682100 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Sample data of salmon and seabass, where each fish is described by its size and brightness characteristics. The points are colored according to their species: salmon (blue) or seabass (orange). For 1-dimensional data, the points are plotted along the x-axis, representing the size and brightness of the fish.</p> 2025-11-06T12:09:01.741767 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Sample data of salmon and seabass, where each fish is described by its size and brightness characteristics. The points are colored according to their species: salmon (blue) or seabass (orange). The x-axis represents the size of the fish, while the y-axis represents its brightness.</p> <p>The machine must learn to draw a line that separates the two classes, salmon and seabass, based on the size and brightness characteristics. This line is called a decision boundary. So that, as soon as a new fish is placed on the conveyor belt, the machine can decide whether it is a salmon or a seabass based on its size and brightness characteristics - as shown in the figure on the right.</p> <p>In general, in the context of classification, the machine must learn to draw decision boundaries in a multidimensional feature space. Allowing, when a new example is presented, the machine to decide which class it belongs to based on the characteristics of the example.</p> <p>Attention</p> <p>The decision boundary is not always linear. In some cases, the data may be distributed in a way that requires a non-linear decision boundary to separate the classes effectively. In such cases, more complex models, such as neural networks or support vector machines with kernels, may be needed to find an appropriate separation.</p>"}, {"location": "classes/data/#iris-dataset", "title": "Iris Dataset", "text": "<p>UCI Machine Learning Repository: the Iris Dataset is a classic dataset used for classification tasks in machine learning. It was introduced by Sir Ronald A. Fisher in 1936<sup>3</sup> and has since become one of the most widely used datasets in the field<sup>4</sup>.</p> <p>The Iris Dataset is a classic and real dataset used for flower classification. It contains 150 samples of three different species of Iris flowers (Iris setosa, Iris versicolor, and Iris virginica), with four features: petal and sepal length and width.</p> <p></p> <p>The dataset is widely used to demonstrate machine learning algorithms, especially for classification tasks. It is simple enough to be easily understood, but also presents interesting challenges for more complex models.</p> <p>A sample of the Iris dataset is presented in the table below:</p> sepal length(cm) sepal width(cm) petal length(cm) petal width(cm) class 5.7 3.0 4.2 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.2 2.9 4.3 1.3 versicolor 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 6.7 3.0 5.2 2.3 virginica 6.3 2.5 5.0 1.9 virginica 6.5 3.0 5.2 2.0 virginica <p>Sample of the Iris dataset, containing features such as sepal length, sepal width, petal length, and petal width, along with the class of the flower. The dataset is widely used for classification tasks in machine learning.</p> <p>Below there is a code snippet that loads the Iris dataset using the <code>pandas</code> library and visualizes it using <code>matplotlib</code>. The dataset is loaded from a CSV file, and the features are plotted in a scatter plot, with different colors representing the different classes of flowers.</p> <p> </p> Editor (session: default) Run <pre>import pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Carregar o conjunto de dados Iris\niris = load_iris()\n\n# Transforma em DataFrame\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=['sepal_l', 'sepal_w', 'petal_l', 'petal_w']\n)\ndf['class'] = iris.target_names[iris.target]\n\n# Imprime os dados\nprint(df)</pre> Output Clear <pre><code></code></pre> <p></p> <p>Also, the dataset can be visualized using the <code>seaborn</code> library, which provides a high-level interface for drawing attractive statistical graphics:</p> 2025-11-06T12:09:04.165970 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Dataset visualization of the Iris dataset using the <code>seaborn</code> library. The scatter plot shows the relationship between the features of the flowers, with different colors representing the different classes. The diagonal plots show the distribution of each feature, allowing for a better understanding of the data.</p> <p>In this visualization, each feature is represented by an axis, and the flowers are plotted in a multidimensional space. The colors represent the different classes of flowers, allowing for the identification of patterns and separations between the classes. Note that for some configurations, such as petal length vs petal width, the classes are well separated, while in others, such as sepal length vs sepal width, the classes overlap.</p> <p>Real World</p> <p>The Iris dataset is a classic example of a dataset used in machine learning, particularly for classification tasks. It is simple enough to be easily understood, but also presents interesting challenges for more complex models. The dataset is widely used in educational contexts to teach concepts of machine learning and data analysis.</p> <p>One can imagine that in more complex problems, such as image recognition or natural language processing, the data can be much more complex and challenging. Not allowing for a clear visualization of the spatial distribution of features. However, the fundamental principles of machine learning remain the same: understanding the data, properly preprocessing it, and choosing the right model for the task.</p>"}, {"location": "classes/data/#other-datasets", "title": "Other Datasets", "text": "<p>Data distribution is a crucial aspect of machine learning, as it directly affects the model's ability to learn and generalize. Usually, the nature of the data can be visualized in scatter plots, histograms, or boxplots, allowing for the identification of patterns, trends, and anomalies in the data - of course, when the data has a low number of dimensions (2 or 3).</p> <p>Illustrations of some distributions with only two dimensions are presented below:</p> 2025-11-06T12:09:04.649981 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Data distributions in two dimensions in different spatial formats. For each surface, the separation between classes is made based on the characteristics of the data. The distribution of the data can affect the model's ability to learn and generalize.</p> <p>The figure above presents four different data distributions in two dimensions, each with its own spatial characteristics. The separation between classes is made based on the characteristics of the data, and the distribution of the data can affect the model's ability to learn and generalize. In general, the function of a machine learning technique is to find a separation between classes in order to maximize the model's accuracy.</p>"}, {"location": "classes/data/#summary", "title": "Summary", "text": "<p>Data is the foundation of any machine learning model. The quality, quantity, and nature of the available data are critical to the model's success. It is important to understand how the data is structured, processed, and used to train models, as well as to consider the volume of data and the balance of classes.</p> <p>In addition, it is essential to perform proper data preprocessing, which may include cleaning, transformation, and normalization, to ensure that models can learn effectively and make accurate predictions.</p> <p>The great challenge in machine learning is to seek the best separation between classes in order to maximize the model's accuracy. This involves not only the choice of algorithm but also a deep understanding of the data and the relationships between variables.</p> <ol> <li> <p>One-Hot Encoding - Wikipedia \u21a9</p> </li> <li> <p>Label Encoding - Scikit-learn \u21a9</p> </li> <li> <p>Fisher, R. A.. 1936. Iris. UCI Machine Learning Repository. https://doi.org/10.24432/C56C76. \u21a9</p> </li> <li> <p>Iris Dataset - Wikipedia \u21a9</p> </li> <li> <p>Richard O. Duda, Peter E. Hart, and David G. Stork. 2000. Pattern Classification (2nd Edition). Wiley-Interscience, USA.\u00a0\u21a9</p> </li> <li> <p>Data Augmentation - Wikipedia \u21a9</p> </li> <li> <p>DataCamp - Complete Guide to Data Augmentation \u21a9</p> </li> </ol>"}, {"location": "classes/deep-learning/", "title": "10. Deep Learning", "text": "<p>Deep learning is a subset of machine learning (which itself is part of artificial intelligence) that focuses on training artificial neural networks with multiple layers to learn and make predictions from complex data. These networks are inspired by the human brain's structure, where \"neurons\" process information and pass it along.</p> <p>Unlike traditional machine learning algorithms (e.g., linear regression or decision trees), which often require manual feature engineering (hand-picking important data characteristics), deep learning models automatically extract features from raw data through layers of processing. This makes them powerful for tasks like image recognition, natural language processing, speech synthesis, and more.</p> <p>Deep learning excels with large datasets and high computational power (e.g., GPUs), but it can be \"black-box\" in nature\u2014meaning it's sometimes hard to interpret why a model makes a specific decision.</p> <p>The core building block is the artificial neural network (ANN), which consists of interconnected nodes (neurons) organized into layers. Data flows from the input layer, through hidden layers (where the \"deep\" part comes in, with many layers stacked), to the output layer. Training involves adjusting weights (connections between neurons) using algorithms like backpropagation to minimize errors.</p>"}, {"location": "classes/deep-learning/#key-components", "title": "Key Components", "text": "<p>A typical neural network has three main parts:</p> <ul> <li>Input Layer: The entry point where raw data (e.g., pixel values from an image) is fed into the network. It doesn't perform computations; it just passes data forward.</li> <li>Hidden Layers: The \"depth\" of deep learning. These are where the magic happens\u2014multiple stacked layers that transform the data through mathematical operations. Each layer learns increasingly abstract representations (e.g., from edges in an image to full objects).</li> <li>Output Layer: The final layer that produces the prediction or classification (e.g., \"cat\" or \"dog\" in an image classifier).</li> </ul>"}, {"location": "classes/deep-learning/#different-types-of-layers", "title": "Different Types of Layers", "text": "<p>Deep learning models use various specialized layers depending on the task and architecture. Here's an overview of common layer types, grouped by their typical use. The following table summarizes their characteristics:</p> Layer Type Description Common Use Cases How It Works Dense (Fully Connected) Every neuron in this layer is connected to every neuron in the previous layer. It's the most basic type. General-purpose networks, like simple classifiers or regressors. Often used in the final stages of more complex models. Applies a linear transformation (weights * inputs + bias) followed by an activation function (e.g., ReLU) to introduce non-linearity. Convolutional Uses filters (kernels) to scan input data, detecting local patterns like edges or textures. Key to \"convolutional neural networks\" (CNNs). Image and video processing, computer vision (e.g., object detection in photos). Slides filters over the input, computing dot products to create feature maps. Reduces spatial dimensions while preserving important features. Pooling Downsamples the output from convolutional layers, reducing computational load and preventing overfitting. Types include max pooling (takes the maximum value) and average pooling. Follows convolutional layers in CNNs to summarize features. Aggregates values in small regions (e.g., 2x2 grid) into a single value, making the model more robust to variations like translations. Recurrent (e.g., RNN, LSTM, GRU) Handles sequential data by maintaining a \"memory\" of previous inputs via loops. LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are advanced variants that address vanishing gradient issues. Time-series forecasting, natural language processing (e.g., machine translation), speech recognition. Processes inputs one step at a time, using hidden states to carry information forward. Good for sequences but can struggle with long dependencies. Embedding Converts categorical data (e.g., words) into dense vectors of fixed size, capturing semantic relationships. NLP tasks like word embeddings (e.g., Word2Vec). Often the first layer in text-based models. Maps high-dimensional sparse data (e.g., one-hot encoded words) to lower-dimensional continuous space. Attention (used in Transformers) Allows the model to focus on relevant parts of the input dynamically, weighing their importance. Self-attention computes relationships between all elements. Modern NLP (e.g., GPT models), machine translation, and even vision tasks. Uses queries, keys, and values to compute attention scores, enabling parallel processing of sequences (unlike RNNs). Normalization (e.g., Batch Normalization, Layer Normalization) Stabilizes training by normalizing activations within a layer, reducing internal covariate shift. Almost all deep networks to speed up training and improve performance. Adjusts and scales activations (e.g., mean to 0, variance to 1) across mini-batches or individual layers. Dropout Randomly \"drops out\" (ignores) a fraction of neurons during training to prevent overfitting. Regularization in any network, especially dense or convolutional ones. Temporarily removes connections, forcing the network to learn redundant representations. Inactive during inference. Flatten Converts multi-dimensional data (e.g., from convolutional layers) into a 1D vector for dense layers. Transitioning from feature extraction (CNN) to classification. Reshapes tensors without changing values, e.g., turning a 2D feature map into a flat array. Activation Applies a non-linear function to the output of other layers (though often built into them). Common ones: ReLU (Rectified Linear Unit), Sigmoid, Tanh, Softmax. Everywhere, to add non-linearity and control output ranges (e.g., Softmax for probabilities). Transforms linear outputs; e.g., ReLU sets negative values to 0 for faster training."}, {"location": "classes/deep-learning/#common-deep-learning-architectures", "title": "Common Deep Learning Architectures", "text": "<p>These layers are combined into architectures tailored to specific problems:</p> <ul> <li>Feedforward Neural Networks (FNN): Basic stack of dense layers for simple tasks.</li> <li>Convolutional Neural Networks (CNN): Convolutional + pooling layers for spatial data like images (e.g., ResNet, VGG).</li> <li>Recurrent Neural Networks (RNN): Recurrent layers for sequences (e.g., LSTM for text generation).</li> <li>Transformers: Attention layers for handling long-range dependencies (e.g., BERT for NLP, Vision Transformers for images).</li> <li>Autoencoders: Encoder (convolutional/dense) + decoder layers for unsupervised learning like denoising.</li> <li>Generative Adversarial Networks (GANs): Combines generator and discriminator networks (often convolutional) for generating realistic data.</li> </ul>"}, {"location": "classes/deep-learning/#forward-and-backward-pass-for-each-layer", "title": "Forward and Backward Pass for Each Layer", "text": "<p>The forward pass computes the output of each layer given the input, while the backward pass computes gradients for learning.</p> <p>Backpropagation computes the gradient of the loss with respect to the layer's inputs and parameters (e.g., weights, biases) to update them via optimizers like gradient descent. Assume a scalar loss \\( L \\), and upstream gradient \\( \\displaystyle \\frac{\\partial L}{\\partial y} \\) (where \\( y \\) is the layer's output) is provided from the next layer.</p>"}, {"location": "classes/deep-learning/#a-dense-fully-connected", "title": "A. Dense (Fully Connected)", "text": "<p>Every neuron in this layer is connected to every neuron in the previous layer. It's the most basic type. General-purpose networks, like simple classifiers or regressors. Often used in the final stages of more complex models.</p> <p></p> <p>A sample of a small fully-connected layer with four input and eight output neurons. Source: Linear/Fully-Connected Layers User's Guide</p> <p>Parameters:</p> <ul> <li> <p>x: Input vector.</p> <p>W: Weight matrix.</p> <p>b: Bias vector.</p> </li> <li> <p>\\( x = [2, 3] \\)</p> <p>\\( W = \\begin{bmatrix} 1 &amp; 2 \\\\ 0 &amp; -1 \\end{bmatrix} \\)</p> <p>\\( b = [1, -1] \\)</p> </li> </ul> <p>Forward Pass:</p> <ul> <li> <p>\\( y = Wx + b \\),</p> <p>then apply activation (e.g., ReLU: \\( y = \\max(0, y) \\)).</p> </li> <li> <p>\\( y = [9, -4] \\),</p> <p>ReLU: [9, 0].</p> </li> </ul> <p>Backward Pass:</p> <ul> <li> <ol> <li> <p>Gradient w.r.t. input:</p> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial x} = W^T \\cdot \\frac{\\partial L}{\\partial y'} \\)</p> <p>where \\( y' \\) is post-activation,</p> <p>and \\( \\displaystyle \\frac{\\partial L}{\\partial y'} \\) is adjusted for activation,</p> <p>e.g., for ReLU: 1 if \\( y &gt; 0 \\), else 0).</p> </li> <li> <p>Gradient w.r.t. weights:</p> <p>\\( \\displaystyle  \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y'} \\cdot x^T \\).</p> </li> <li> <p>Gradient w.r.t. bias:</p> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial y'} \\).</p> </li> </ol> </li> <li> <p>Assume loss gradient \\( \\frac{\\partial L}{\\partial y'} = [0.5, -0.2] \\) (post-ReLU). For ReLU: mask = [1, 0], so \\( \\frac{\\partial L}{\\partial y} = [0.5, 0] \\).</p> <ol> <li> <p>\\( \\begin{align*}     \\displaystyle \\frac{\\partial L}{\\partial x} &amp;= W^T \\cdot [0.5, 0] \\\\     &amp; = \\begin{bmatrix} 1 &amp; 0 \\\\ 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0 \\end{bmatrix} \\\\     &amp; = [0.5, 1.0]     \\end{align*} \\).</p> </li> <li> <p>\\( \\begin{align*}     \\displaystyle \\frac{\\partial L}{\\partial W} &amp;= [0.5, 0]^T \\cdot [2, 3] \\\\     &amp; = \\begin{bmatrix} 1 &amp; 1.5 \\\\ 0 &amp; 0 \\end{bmatrix}     \\end{align*} \\).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial b} = [0.5, 0] \\).</p> </li> </ol> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef dense_forward(x, W, b):\n    y_linear = np.dot(W, x) + b\n    y = np.maximum(0, y_linear)  # ReLU\n    return y, y_linear  # Cache linear for backprop\n\ndef dense_backward(dy_post_act, x, W, y_linear):\n    # dy_post_act: \u2202L/\u2202y (post-ReLU)\n    dy_linear = dy_post_act * (y_linear &gt; 0)  # ReLU derivative\n    dx = np.dot(W.T, dy_linear)\n    dW = np.outer(dy_linear, x)\n    db = dy_linear\n    return dx, dW, db\n\n# Example\nx = np.array([2, 3])\nW = np.array([[1, 2], [0, -1]])\nb = np.array([1, -1])\ny, y_linear = dense_forward(x, W, b)\ndy_post_act = np.array([0.5, -0.2])\ndx, dW, db = dense_backward(dy_post_act, x, W, y_linear)\nprint(\"Forward y:\", y)  # [9, 0]\nprint(\"dx:\", dx)  # [0.5, 1.0]\nprint(\"dW:\", dW)  # [[1, 1.5], [0, 0]]\nprint(\"db:\", db)  # [0.5, 0]</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#b-convolutional", "title": "B. Convolutional", "text": "<p>Uses filters (kernels) to scan input data, detecting local patterns like edges or textures. Key to \"convolutional neural networks\" (CNNs). Image and video processing, computer vision (e.g., object detection in photos). Slides filters over the input, computing dot products to create feature maps. Reduces spatial dimensions while preserving important features.</p> <p></p> <p>Convolution of an image with an edge detector convolution kernel. Sources: Deep Learning in a Nutshell: Core Concepts</p> <p></p> <p>Calculating convolution by sliding image patches over the entire image. One image patch (yellow) of the original image (green) is multiplied by the kernel (red numbers in the yellow patch), and its sum is written to one feature map pixel (red cell in convolved feature). Image source: Deep Learning in a Nutshell: Core Concepts</p> <p>Parameters:</p> <ul> <li> <p>X: Input matrix (e.g., image).</p> <p>K: Convolution kernel (filter).</p> <p>b: Bias term.</p> </li> <li> <p>(2D, stride=1, no padding):</p> <p>\\( X = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\)</p> <p>\\( K = \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 1 \\end{bmatrix} \\)</p> <p>\\( b=1 \\)</p> </li> </ul> <p>Forward Pass:</p> <ul> <li> <p>Convolution:</p> <p>\\( \\displaystyle Y[i,j] = \\sum_{m,n} X[i+m, j+n] \\cdot K[m,n] + b \\).    </p> </li> <li> <p>Convolution:</p> <p>\\( \\begin{bmatrix}     \\begin{array}{ll}     =&amp; 1 \\times 1 + 2 \\times 0 \\\\     &amp;+ 4 \\times (-1) + 5 \\times 1 \\\\     &amp;+ 1 \\end{array} &amp;     \\begin{array}{ll}     =&amp; 2 \\times 1 + 3 \\times 0 \\\\     &amp;+ 5 \\times (-1) + 6 \\times 1 \\\\     &amp;+ 1 \\end{array} \\\\     \\begin{array}{ll}     =&amp; 4 \\times 1 + 5 \\times 0 \\\\     &amp;+ 7 \\times (-1) + 8 \\times 1 \\\\     &amp;+ 1 \\end{array} &amp;     \\begin{array}{ll}     =&amp; 5 \\times 1 + 6 \\times 0 \\\\     &amp;+ 8 \\times (-1) + 9 \\times 1 \\\\     &amp;+ 1 \\end{array} \\end{bmatrix} \\)</p> <p>\\( Y = \\begin{bmatrix} 3 &amp; 3 \\\\ -1 &amp; -1 \\end{bmatrix} \\)</p> </li> </ul> <p>Backward Pass:</p> <ul> <li> <ol> <li> <p>Gradient w.r.t. input:</p> <p>Convolve upstream gradient \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\) with rotated kernel (full convolution).</p> </li> <li> <p>Gradient w.r.t. kernel:</p> <p>Convolve input \\( X \\) with \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\).</p> </li> <li> <p>Gradient w.r.t. bias:</p> <p>Sum of \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\).</p> </li> </ol> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial Y} = \\begin{bmatrix} 0.5 &amp; -0.5 \\\\ 1 &amp; 0 \\end{bmatrix} \\).</p> <ol> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial X} \\):</p> <p>Full conv with rotated K (\\( \\begin{bmatrix} 1 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix} \\)) and padded \\( dY \\), approx. \\( \\begin{bmatrix} 0.5 &amp; -0.5 &amp; -0.5 \\\\ 0 &amp; 1.5 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix} \\) (simplified calc).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial K} = \\) Conv X with dY:</p> <p>\\( \\begin{bmatrix} 0.5*1 + (-0.5)*2 + 1*4 + 0*5 \\\\ \\ldots \\end{bmatrix} \\)</p> <p>(detailed in code).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial b} = 0.5 -0.5 +1 +0 = 1 \\).</p> </li> </ol> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nfrom scipy.signal import correlate2d, convolve2d\n\ndef conv_forward(X, K, b):\n    Y = correlate2d(X, K, mode='valid') + b  # SciPy correlate for conv\n    return Y, X  # Cache X\n\ndef conv_backward(dY, X, K):\n    # Rotate kernel 180 degrees for full conv\n    K_rot = np.rot90(K, 2)\n    # Pad dY to match X shape for dx\n    pad_h, pad_w = K.shape[0]-1, K.shape[1]-1\n    dY_padded = np.pad(dY, ((pad_h//2, pad_h-pad_h//2), (pad_w//2, pad_w-pad_w//2)))\n    dX = convolve2d(dY_padded, K_rot, mode='valid')\n    dK = correlate2d(X, dY, mode='valid')\n    db = np.sum(dY)\n    return dX, dK, db\n\n# Example\nX = np.array([[1,2,3],[4,5,6],[7,8,9]])\nK = np.array([[1,0],[-1,1]])\nb = 1\nY, _ = conv_forward(X, K, b)\ndY = np.array([[0.5, -0.5],[1, 0]])\ndX, dK, db = conv_backward(dY, X, K)\nprint(\"Forward Y:\\n\", Y)\nprint(\"dX:\\n\", dX)\nprint(\"dK:\\n\", dK)\nprint(\"db:\", db)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#c-pooling-max-pooling", "title": "C. Pooling (Max Pooling)", "text": "<p>Downsamples the output from convolutional layers, reducing computational load and preventing overfitting. Types include max pooling (takes the maximum value) and average pooling. Follows convolutional layers in CNNs to summarize features. Aggregates values in small regions (e.g., 2x2 grid) into a single value, making the model more robust to variations like translations.</p> <p>Forward Pass:</p> <ul> <li> <p>\\( Y[i,j] = \\max(X[i:i+k, j:j+k]) \\) for pool size \\( k \\).</p> </li> <li> <p>\\( X = \\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\\\ 9 &amp; 10 &amp; 11 &amp; 12 \\\\ 13 &amp; 14 &amp; 15 &amp; 16 \\end{bmatrix} \\), pool=2, stride=2,</p> <p>\\( Y = \\begin{bmatrix} 6 &amp; 8 \\\\ 14 &amp; 16 \\end{bmatrix} \\).</p> <ul> <li>Max positions: e.g., 6 from X[1,1]=6, etc.</li> </ul> </li> </ul> <p>Backward Pass:</p> <ul> <li> <p>Distribute upstream gradient \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\) to the max position in each window; 0 elsewhere.</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial Y} = \\begin{bmatrix} 0.5 &amp; -0.5 \\\\ 1 &amp; 0 \\end{bmatrix} \\).</p> <ul> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial X} \\):</p> <ul> <li>0.5 to pos of 6 (1,1),</li> <li>-0.5 to pos of 8 (1,3),</li> <li>1 to pos of 14 (3,1),</li> <li>0 to pos of 16 (3,3).</li> <li>Other positions 0.</li> </ul> </li> </ul> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef max_pool_forward(X, pool_size=2, stride=2):\n    H, W = X.shape\n    out_H, out_W = H // stride, W // stride\n    Y = np.zeros((out_H, out_W))\n    max_idx = np.zeros_like(X, dtype=bool)  # For backprop\n    for i in range(0, H, stride):\n        for j in range(0, W, stride):\n            slice = X[i:i+pool_size, j:j+pool_size]\n            max_val = np.max(slice)\n            Y[i//stride, j//stride] = max_val\n            max_idx[i:i+pool_size, j:j+pool_size] = (slice == max_val)\n    return Y, max_idx\n\ndef max_pool_backward(dY, max_idx, pool_size=2, stride=2):\n    dX = np.zeros_like(max_idx, dtype=float)\n    for i in range(dY.shape[0]):\n        for j in range(dY.shape[1]):\n            dX[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size] = dY[i,j] * max_idx[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size]\n    return dX\n\n# Example\nX = np.arange(1,17).reshape(4,4)\nY, max_idx = max_pool_forward(X)\ndY = np.array([[0.5, -0.5],[1, 0]])\ndX = max_pool_backward(dY, max_idx)\nprint(\"Forward Y:\\n\", Y)\nprint(\"dX:\\n\", dX)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#d-recurrent-lstm", "title": "D. Recurrent (LSTM)", "text": "<p>Recurrent Neural Networks (RNNs) are powerful for sequence data. Long Short-Term Memory (LSTM) networks are a type of RNN designed to capture long-term dependencies and mitigate issues like vanishing gradients.</p> <p>Parameters:</p> <ul> <li> <p>(Simplified to hidden size=1 for clarity):</p> <ul> <li> <p>Inputs:</p> <p>\\( x_t = [0.5] \\),</p> <p>\\( h_{t-1} = [0.1] \\),</p> <p>\\( C_{t-1} = [0.2] \\)</p> </li> <li> <p>Weights:</p> <p>\\( W_f = [[0.5, 0.5]] \\),</p> <p>\\( W_i = [[0.4, 0.4]] \\),</p> <p>\\( W_C = [[0.3, 0.3]] \\),</p> <p>\\( W_o = [[0.2, 0.2]] \\)</p> </li> <li> <p>Biases: \\( b_f = b_i = b_C = b_o = [0.0] \\)</p> </li> </ul> </li> </ul> <p>Forward Pass:</p> <ul> <li> <ul> <li>Concatenate: \\( \\text{concat} = [h_{t-1}, x_t] \\)</li> <li>Forget gate: \\( f_t = \\sigma(W_f \\cdot \\text{concat} + b_f) \\)</li> <li>Input gate: \\( i_t = \\sigma(W_i \\cdot \\text{concat} + b_i) \\)</li> <li>Cell candidate: \\( \\tilde{C}_t = \\tanh(W_C \\cdot \\text{concat} + b_C) \\)</li> <li>Cell state: \\( C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t \\)</li> <li>Output gate: \\( o_t = \\sigma(W_o \\cdot \\text{concat} + b_o) \\)</li> <li>Hidden state: \\( h_t = o_t \\cdot \\tanh(C_t) \\)</li> </ul> </li> <li> <ul> <li>concat = [0.1, 0.5]</li> <li>\\( f_t = \\sigma(0.3) \\approx 0.5744 \\)</li> <li>\\( i_t = \\sigma(0.24) \\approx 0.5597 \\)</li> <li>\\( \\tilde{C}_t = \\tanh(0.18) \\approx 0.1785 \\)</li> <li>\\( C_t \\approx 0.5744 \\cdot 0.2 + 0.5597 \\cdot 0.1785 \\approx 0.2146 \\)</li> <li>\\( o_t = \\sigma(0.12) \\approx 0.5300 \\)</li> <li>\\( h_t \\approx 0.5300 \\cdot \\tanh(0.2146) \\approx 0.1120 \\)</li> </ul> </li> </ul> <p>Backward Pass:</p> <ul> <li> <p>Gradients are computed via chain rule:</p> <ul> <li>\\( dC_t = dh_t \\cdot o_t \\cdot (1 - \\tanh^2(C_t)) + dC_{next} \\) (dC_next from future timestep)</li> <li>\\( do_t = dh_t \\cdot \\tanh(C_t) \\cdot \\sigma'(o_t) \\)</li> <li>\\( d\\tilde{C}_t = dC_t \\cdot i_t \\cdot (1 - \\tilde{C}_t^2) \\)</li> <li>\\( di_t = dC_t \\cdot \\tilde{C}_t \\cdot \\sigma'(i_t) \\)</li> <li>\\( df_t = dC_t \\cdot C_{t-1} \\cdot \\sigma'(f_t) \\)</li> <li>\\( dC_{prev} = dC_t \\cdot f_t \\)</li> <li>Then, backpropagate to concat: \\( d\\text{concat} = W_o^T \\cdot do_t + W_C^T \\cdot d\\tilde{C}_t + W_i^T \\cdot di_t + W_f^T \\cdot df_t \\)</li> <li>Split \\( d\\text{concat} \\) into \\( dh_{prev} \\) and \\( dx_t \\)</li> <li>Parameter gradients: \\( dW_f = df_t \\cdot \\text{concat}^T \\), \\( db_f = df_t \\), and similarly for others.</li> </ul> </li> <li> <p>(Assume upstream: \\( dh_t = [0.1] \\), \\( dC_t = [0.05] \\) from next timestep):</p> <ul> <li>\\( dC_t \\approx 0.1 \\cdot 0.5300 \\cdot (1 - \\tanh^2(0.2146)) + 0.05 \\approx 0.1028 + 0.05 = 0.1528 \\) (detailed steps in code output)</li> <li>Resulting gradients match the executed values below (e.g., \\( dx_t \\approx [0.0216] \\), etc.).</li> </ul> </li> </ul> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef dsigmoid(y):\n    return y * (1 - y)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(y):\n    return 1 - y**2\n\n# Forward pass\ndef lstm_forward(x_t, h_prev, C_prev, W_f, W_i, W_C, W_o, b_f, b_i, b_C, b_o):\n    concat = np.concatenate((h_prev, x_t), axis=0)\n    f_t = sigmoid(np.dot(W_f, concat) + b_f)\n    i_t = sigmoid(np.dot(W_i, concat) + b_i)\n    C_tilde = tanh(np.dot(W_C, concat) + b_C)\n    C_t = f_t * C_prev + i_t * C_tilde\n    o_t = sigmoid(np.dot(W_o, concat) + b_o)\n    h_t = o_t * tanh(C_t)\n    cache = (concat, f_t, i_t, C_tilde, o_t, C_t)\n    return h_t, C_t, cache\n\n# Backward pass\ndef lstm_backward(dh_next, dC_next, cache, W_f, W_i, W_C, W_o):\n    concat, f_t, i_t, C_tilde, o_t, C_t = cache\n\n    # Derivatives\n    dC_t = dh_next * o_t * dtanh(tanh(C_t)) + dC_next\n    do_t = dh_next * tanh(C_t) * dsigmoid(o_t)\n    dC_tilde = dC_t * i_t * dtanh(C_tilde)\n    di_t = dC_t * C_tilde * dsigmoid(i_t)\n    df_t = dC_t * C_prev * dsigmoid(f_t)\n    dC_prev = dC_t * f_t\n\n    # Gradients for gates\n    dconcat_o = np.dot(W_o.T, do_t)\n    dconcat_C = np.dot(W_C.T, dC_tilde)\n    dconcat_i = np.dot(W_i.T, di_t)\n    dconcat_f = np.dot(W_f.T, df_t)\n    dconcat = dconcat_f + dconcat_i + dconcat_C + dconcat_o\n\n    # Split for h_prev and x_t\n    hidden_size = h_prev.shape[0]\n    dh_prev = dconcat[:hidden_size]\n    dx_t = dconcat[hidden_size:]\n\n    # Parameter gradients\n    dW_f = np.outer(df_t, concat)\n    db_f = df_t\n    dW_i = np.outer(di_t, concat)\n    db_i = di_t\n    dW_C = np.outer(dC_tilde, concat)\n    db_C = dC_tilde\n    dW_o = np.outer(do_t, concat)\n    db_o = do_t\n\n    return dx_t, dh_prev, dC_prev, dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o\n\n# Numerical example (hidden size = 1)\nx_t = np.array([0.5])\nh_prev = np.array([0.1])\nC_prev = np.array([0.2])\nW_f = np.array([[0.5, 0.5]])\nW_i = np.array([[0.4, 0.4]])\nW_C = np.array([[0.3, 0.3]])\nW_o = np.array([[0.2, 0.2]])\nb_f = np.array([0.0])\nb_i = np.array([0.0])\nb_C = np.array([0.0])\nb_o = np.array([0.0])\n\n# Forward\nh_t, C_t, cache = lstm_forward(x_t, h_prev, C_prev, W_f, W_i, W_C, W_o, b_f, b_i, b_C, b_o)\nprint(\"Forward h_t:\", h_t)  # Output: [0.11199714]\nprint(\"Forward C_t:\", C_t)  # Output: [0.2145628]\n\n# Backward example: assume dh_next = [0.1], dC_next = [0.05]\ndh_next = np.array([0.1])\ndC_next = np.array([0.05])\ndx_t, dh_prev, dC_prev, dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o = lstm_backward(dh_next, dC_next, cache, W_f, W_i, W_C, W_o)\n\nprint(\"Backward dx_t:\", dx_t)  # Output: [0.02164056]\nprint(\"Backward dh_prev:\", dh_prev)  # Output: [0.02164056]\nprint(\"Backward dC_prev:\", dC_prev)  # Output: [0.05780591]\nprint(\"Backward dW_f:\", dW_f)  # Output: [[0.00049199 0.00245997]]\nprint(\"Backward db_f:\", db_f)  # Output: [0.00491995]\nprint(\"Backward dW_i:\", dW_i)  # Output: [[0.00044162 0.00220808]]\nprint(\"Backward db_i:\", db_i)  # Output: [0.00441615]\nprint(\"Backward dW_C:\", dW_C)  # Output: [[0.00545376 0.02726878]]\nprint(\"Backward db_C:\", db_C)  # Output: [0.05453756]\nprint(\"Backward dW_o:\", dW_o)  # Output: [[0.00052643 0.00263213]]\nprint(\"Backward db_o:\", db_o)  # Output: [0.00526427]</pre> Output Clear <pre></pre> <p></p> <p>Notes</p> <ul> <li>This is a single-timestep LSTM with hidden size 1 for simplicity. In practice, LSTMs process sequences (multiple timesteps) and have larger hidden sizes; backpropagation through time (BPTT) unrolls the network over timesteps.</li> <li>The code uses NumPy; for real models, use PyTorch or TensorFlow for automatic differentiation and batching.</li> <li>Outputs are approximate due to floating-point precision but match the manual calculations.</li> <li>If you need a multi-timestep example, sequence processing, or integration into a full RNN, let me know!</li> </ul>"}, {"location": "classes/deep-learning/#e-embedding", "title": "E. Embedding", "text": "<p>Forward Pass:</p> <ul> <li> <p>\\( y = E[i] \\),</p> <p>where \\( E \\) is the embedding matrix,</p> <p>\\( i \\) is the input index.</p> </li> <li> <p>Index 1,</p> <p>\\( E = \\begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 \\\\ 0.4 &amp; 0.5 &amp; 0.6 \\end{bmatrix}\\),</p> <p>\\( y = [0.4,0.5,0.6] \\).</p> </li> </ul> <p>Backward Pass:</p> <ul> <li> <p>Gradient \\( \\displaystyle \\frac{\\partial L}{\\partial E[i]} += \\frac{\\partial L}{\\partial y} \\); other rows 0. (Sparse update).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial y} = [0.1, -0.1, 0.2] \\),</p> <p>so add to E[1].</p> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef embedding_forward(index, E):\n    return E[index]\n\ndef embedding_backward(dy, index, E_shape):\n    dE = np.zeros(E_shape)\n    dE[index] = dy\n    return dE\n\n# Example\nE = np.array([[0.1,0.2,0.3],[0.4,0.5,0.6]])\nindex = 1\ny = embedding_forward(index, E)\ndy = np.array([0.1, -0.1, 0.2])\ndE = embedding_backward(dy, index, E.shape)\nprint(\"dE:\\n\", dE)\n</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#f-attention-scaled-dot-product", "title": "F. Attention (Scaled Dot-Product)", "text": "<p>Forward Pass: \\( \\text{Attention} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V \\).</p> <p>Backward Pass: Gradients for Q, K, V via chain rule on softmax and matmuls.</p> <p>Numerical Example: Use previous. Backward is matrix derivs; code handles.</p> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef attention_forward(Q, K, V):\n    d = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d)\n    weights = softmax(scores)\n    attn = np.dot(weights, V)\n    return attn, (scores, weights, K, V)  # Cache\n\ndef attention_backward(dattn, cache):\n    scores, weights, K, V = cache\n    dweights = np.dot(dattn, V.T)\n    dscores = weights * (dweights - np.sum(weights * dweights, axis=-1, keepdims=True))\n    dQ = np.dot(dscores, K) / np.sqrt(K.shape[-1])\n    dK = np.dot(dscores.T, Q) / np.sqrt(K.shape[-1])\n    dV = np.dot(weights.T, dattn)\n    return dQ, dK, dV\n\n# Example\nQ = K = V = np.array([[1.,0.],[0.,1.]])\nattn, cache = attention_forward(Q, K, V)\ndattn = np.array([[0.1,0.2],[ -0.1,0.3]])\ndQ, dK, dV = attention_backward(dattn, cache)\nprint(\"dQ:\\n\", dQ)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#g-normalization-batch-normalization", "title": "G. Normalization (Batch Normalization)", "text": "<p>Forward Pass: Normalize, scale, shift.</p> <p>Backward Pass: Gradients for input, gamma, beta via chain rule on mean/var.</p> <p>Numerical Example: Previous forward. Backward computes dx, dgamma, dbeta.</p> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef batch_norm_forward(x, gamma, beta, epsilon=1e-4):\n    mu = np.mean(x)\n    var = np.var(x)\n    x_hat = (x - mu) / np.sqrt(var + epsilon)\n    y = gamma * x_hat + beta\n    return y, (x_hat, mu, var)\n\ndef batch_norm_backward(dy, cache, gamma):\n    x_hat, mu, var = cache\n    N = dy.shape[0]\n    dx_hat = dy * gamma\n    dvar = np.sum(dx_hat * (x - mu) * -0.5 * (var + epsilon)**(-1.5), axis=0)\n    dmu = np.sum(dx_hat * -1 / np.sqrt(var + epsilon), axis=0) + dvar * np.mean(-2 * (x - mu), axis=0)\n    dx = dx_hat / np.sqrt(var + epsilon) + dvar * 2 * (x - mu) / N + dmu / N\n    dgamma = np.sum(dy * x_hat, axis=0)\n    dbeta = np.sum(dy, axis=0)\n    return dx, dgamma, dbeta\n\n# Example\nx = np.array([1,2,3,4.])\ngamma, beta = 1, 0\ny, cache = batch_norm_forward(x, gamma, beta)\ndy = np.array([0.1,0.2,-0.1,0.3])\ndx, dgamma, dbeta = batch_norm_backward(dy, cache, gamma)\nprint(\"dx:\", dx)\nprint(\"dgamma:\", dgamma)\nprint(\"dbeta:\", dbeta)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#h-dropout", "title": "H. Dropout", "text": "<p>Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped. Source: Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p> <p>Forward Pass: Mask during training.</p> <p>Backward Pass: Same mask applied to upstream gradient (scale by 1/(1-p)).</p> <p>Numerical Example: Same as forward; backward passes dy through mask.</p> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef dropout_forward(x, p, training=True):\n    if training:\n        mask = np.random.binomial(1, 1-p, size=x.shape) / (1-p)\n        y = x * mask\n        return y, mask\n    return x, None\n\ndef dropout_backward(dy, mask):\n    if mask is None:\n        return dy\n    return dy * mask\n\n# Example\nnp.random.seed(0)\nx = np.array([1,2,3,4.])\np = 0.5\ny, mask = dropout_forward(x, p)\ndy = np.array([0.1,0.2,0.3,0.4])\ndx = dropout_backward(dy, mask)\nprint(\"dx:\", dx)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#i-flatten", "title": "I. Flatten", "text": "<p>Forward Pass: Reshape to 1D.</p> <p>Backward Pass: Reshape upstream gradient back to original shape.</p> <p>Numerical Example: - Forward: 2x2 to [1,2,3,4]. - Backward: dy = [0.1,0.2,0.3,0.4] -&gt; reshape to 2x2.</p> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef flatten_forward(x):\n    return x.flatten(), x.shape\n\ndef flatten_backward(dy, orig_shape):\n    return dy.reshape(orig_shape)\n\n# Example\nx = np.array([[1,2],[3,4]])\ny, shape = flatten_forward(x)\ndy = np.array([0.1,0.2,0.3,0.4])\ndx = flatten_backward(dy, shape)\nprint(\"dx:\\n\", dx)\n</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/#j-activation-relu", "title": "J. Activation (ReLU)", "text": "<p>Forward Pass:</p> <ul> <li>\\( y = \\max(0, x) \\). </li> <li> <p>\\( x = [-1, 0, 2, -3] \\),</p> <p>\\( y = [0, 0, 2, 0] \\).</p> </li> </ul> <p>Backward Pass:</p> <ul> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot (x &gt; 0) \\).</p> </li> <li> <p>\\( dy = [0.5,-0.5,1,0] \\),</p> <p>\\( dx = [0,0,1,0] \\) (masked).</p> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef relu_forward(x):\n    return np.maximum(0, x), x\n\ndef relu_backward(dy, x_cache):\n    return dy * (x_cache &gt; 0)\n\n# Example\nx = np.array([-1,0,2,-3])\ny, x_cache = relu_forward(x)\ndy = np.array([0.5,-0.5,1,0])\ndx = relu_backward(dy, x_cache)\nprint(\"dx:\", dx)\n</pre> Output Clear <pre></pre> <p></p> <ol> <li> <p>Mohd Halim Mohd Noor, Ayokunle Olalekan Ige: A Survey on State-of-the-art Deep Learning Applications and Challenges, 2025.\u00a0\u21a9</p> </li> <li> <p>Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola: Dive into Deep Learning, 2020.\u00a0\u21a9</p> </li> <li> <p>Ian Goodfellow, Yoshua Bengio, Aaron Courville: Deep Learning, 2016.\u00a0\u21a9</p> </li> <li> <p>Johannes Schneider, Michalis Vlachos: A Survey of Deep Learning: From Activations to Transformers, 2024.\u00a0\u21a9</p> </li> <li> <p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.\u00a0\u21a9</p> </li> <li> <p>Adit Deshpande is a comprehensive resource for learning about deep learning, covering various topics such as neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), and more. It provides detailed explanations, code examples, and practical applications of deep learning concepts, making it suitable for both beginners and advanced learners.\u00a0\u21a9</p> </li> <li> <p>Fran\u00e7ois Fleuret offers a collection of deep learning resources, including lectures, tutorials, and research papers, aimed at helping learners understand and apply deep learning techniques effectively.\u00a0\u21a9</p> </li> </ol>"}, {"location": "classes/deep-learning/convolutional/", "title": "Convolutional", "text": "<p>Uses filters (kernels) to scan input data, detecting local patterns like edges or textures. Key to \"convolutional neural networks\" (CNNs). Image and video processing, computer vision (e.g., object detection in photos). Slides filters over the input, computing dot products to create feature maps. Reduces spatial dimensions while preserving important features.</p> <p></p> <p>Convolution of an image with an edge detector convolution kernel. Sources: Deep Learning in a Nutshell: Core Concepts</p> <p></p> <p>Calculating convolution by sliding image patches over the entire image. One image patch (yellow) of the original image (green) is multiplied by the kernel (red numbers in the yellow patch), and its sum is written to one feature map pixel (red cell in convolved feature). Image source: Deep Learning in a Nutshell: Core Concepts</p> <p>Parameters:</p> <ul> <li> <p>X: Input matrix (e.g., image).</p> <p>K: Convolution kernel (filter).</p> <p>b: Bias term.</p> </li> <li> <p>(2D, stride=1, no padding):</p> <p>\\( X = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\)</p> <p>\\( K = \\begin{bmatrix} 1 &amp; 0 \\\\ -1 &amp; 1 \\end{bmatrix} \\)</p> <p>\\( b=1 \\)</p> </li> </ul> <p>Forward Pass:</p> <ul> <li> <p>Convolution:</p> <p>\\( \\displaystyle Y[i,j] = \\sum_{m,n} X[i+m, j+n] \\cdot K[m,n] + b \\).    </p> </li> <li> <p>Convolution:</p> <p>\\( \\begin{bmatrix}     \\begin{array}{ll}     =&amp; 1 \\times 1 + 2 \\times 0 \\\\     &amp;+ 4 \\times (-1) + 5 \\times 1 \\\\     &amp;+ 1 \\end{array} &amp;     \\begin{array}{ll}     =&amp; 2 \\times 1 + 3 \\times 0 \\\\     &amp;+ 5 \\times (-1) + 6 \\times 1 \\\\     &amp;+ 1 \\end{array} \\\\     \\begin{array}{ll}     =&amp; 4 \\times 1 + 5 \\times 0 \\\\     &amp;+ 7 \\times (-1) + 8 \\times 1 \\\\     &amp;+ 1 \\end{array} &amp;     \\begin{array}{ll}     =&amp; 5 \\times 1 + 6 \\times 0 \\\\     &amp;+ 8 \\times (-1) + 9 \\times 1 \\\\     &amp;+ 1 \\end{array} \\end{bmatrix} \\)</p> <p>\\( Y = \\begin{bmatrix} 3 &amp; 3 \\\\ -1 &amp; -1 \\end{bmatrix} \\)</p> </li> </ul> <p>Backward Pass:</p> <ul> <li> <ol> <li> <p>Gradient w.r.t. input:</p> <p>Convolve upstream gradient \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\) with rotated kernel (full convolution).</p> </li> <li> <p>Gradient w.r.t. kernel:</p> <p>Convolve input \\( X \\) with \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\).</p> </li> <li> <p>Gradient w.r.t. bias:</p> <p>Sum of \\( \\displaystyle \\frac{\\partial L}{\\partial Y} \\).</p> </li> </ol> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial Y} = \\begin{bmatrix} 0.5 &amp; -0.5 \\\\ 1 &amp; 0 \\end{bmatrix} \\).</p> <ol> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial X} \\):</p> <p>Full conv with rotated K (\\( \\begin{bmatrix} 1 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix} \\)) and padded \\( dY \\), approx. \\( \\begin{bmatrix} 0.5 &amp; -0.5 &amp; -0.5 \\\\ 0 &amp; 1.5 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix} \\) (simplified calc).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial K} = \\) Conv X with dY:</p> <p>\\( \\begin{bmatrix} 0.5*1 + (-0.5)*2 + 1*4 + 0*5 \\\\ \\ldots \\end{bmatrix} \\)</p> <p>(detailed in code).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial b} = 0.5 -0.5 +1 +0 = 1 \\).</p> </li> </ol> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\nfrom scipy.signal import correlate2d, convolve2d\n\ndef conv_forward(X, K, b):\n    Y = correlate2d(X, K, mode='valid') + b  # SciPy correlate for conv\n    return Y, X  # Cache X\n\ndef conv_backward(dY, X, K):\n    # Rotate kernel 180 degrees for full conv\n    K_rot = np.rot90(K, 2)\n    # Pad dY to match X shape for dx\n    pad_h, pad_w = K.shape[0]-1, K.shape[1]-1\n    dY_padded = np.pad(dY, ((pad_h//2, pad_h-pad_h//2), (pad_w//2, pad_w-pad_w//2)))\n    dX = convolve2d(dY_padded, K_rot, mode='valid')\n    dK = correlate2d(X, dY, mode='valid')\n    db = np.sum(dY)\n    return dX, dK, db\n\n# Example\nX = np.array([[1,2,3],[4,5,6],[7,8,9]])\nK = np.array([[1,0],[-1,1]])\nb = 1\nY, _ = conv_forward(X, K, b)\ndY = np.array([[0.5, -0.5],[1, 0]])\ndX, dK, db = conv_backward(dY, X, K)\nprint(\"Forward Y:\\n\", Y)\nprint(\"dX:\\n\", dX)\nprint(\"dK:\\n\", dK)\nprint(\"db:\", db)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/dense/", "title": "Dense", "text": "<p>Every neuron in this layer is connected to every neuron in the previous layer. It's the most basic type. General-purpose networks, like simple classifiers or regressors. Often used in the final stages of more complex models.</p> <p></p> <p>A sample of a small fully-connected layer with four input and eight output neurons. Source: Linear/Fully-Connected Layers User's Guide</p> <p>Parameters:</p> <ul> <li> <p>x: Input vector.</p> <p>W: Weight matrix.</p> <p>b: Bias vector.</p> </li> <li> <p>\\( x = [2, 3] \\)</p> <p>\\( W = \\begin{bmatrix} 1 &amp; 2 \\\\ 0 &amp; -1 \\end{bmatrix} \\)</p> <p>\\( b = [1, -1] \\)</p> </li> </ul> <p>Forward Pass:</p> <ul> <li> <p>\\( y = Wx + b \\),</p> <p>then apply activation (e.g., ReLU: \\( y = \\max(0, y) \\)).</p> </li> <li> <p>\\( y = [9, -4] \\),</p> <p>ReLU: [9, 0].</p> </li> </ul> <p>Backward Pass:</p> <ul> <li> <ol> <li> <p>Gradient w.r.t. input:</p> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial x} = W^T \\cdot \\frac{\\partial L}{\\partial y'} \\)</p> <p>where \\( y' \\) is post-activation,</p> <p>and \\( \\displaystyle \\frac{\\partial L}{\\partial y'} \\) is adjusted for activation,</p> <p>e.g., for ReLU: 1 if \\( y &gt; 0 \\), else 0).</p> </li> <li> <p>Gradient w.r.t. weights:</p> <p>\\( \\displaystyle  \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y'} \\cdot x^T \\).</p> </li> <li> <p>Gradient w.r.t. bias:</p> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial y'} \\).</p> </li> </ol> </li> <li> <p>Assume loss gradient \\( \\frac{\\partial L}{\\partial y'} = [0.5, -0.2] \\) (post-ReLU). For ReLU: mask = [1, 0], so \\( \\frac{\\partial L}{\\partial y} = [0.5, 0] \\).</p> <ol> <li> <p>\\( \\begin{align*}     \\displaystyle \\frac{\\partial L}{\\partial x} &amp;= W^T \\cdot [0.5, 0] \\\\     &amp; = \\begin{bmatrix} 1 &amp; 0 \\\\ 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0 \\end{bmatrix} \\\\     &amp; = [0.5, 1.0]     \\end{align*} \\).</p> </li> <li> <p>\\( \\begin{align*}     \\displaystyle \\frac{\\partial L}{\\partial W} &amp;= [0.5, 0]^T \\cdot [2, 3] \\\\     &amp; = \\begin{bmatrix} 1 &amp; 1.5 \\\\ 0 &amp; 0 \\end{bmatrix}     \\end{align*} \\).</p> </li> <li> <p>\\( \\displaystyle \\frac{\\partial L}{\\partial b} = [0.5, 0] \\).</p> </li> </ol> </li> </ul> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef dense_forward(x, W, b):\n    y_linear = np.dot(W, x) + b\n    y = np.maximum(0, y_linear)  # ReLU\n    return y, y_linear  # Cache linear for backprop\n\ndef dense_backward(dy_post_act, x, W, y_linear):\n    # dy_post_act: \u2202L/\u2202y (post-ReLU)\n    dy_linear = dy_post_act * (y_linear &gt; 0)  # ReLU derivative\n    dx = np.dot(W.T, dy_linear)\n    dW = np.outer(dy_linear, x)\n    db = dy_linear\n    return dx, dW, db\n\n# Example\nx = np.array([2, 3])\nW = np.array([[1, 2], [0, -1]])\nb = np.array([1, -1])\ny, y_linear = dense_forward(x, W, b)\ndy_post_act = np.array([0.5, -0.2])\ndx, dW, db = dense_backward(dy_post_act, x, W, y_linear)\nprint(\"Forward y:\", y)  # [9, 0]\nprint(\"dx:\", dx)  # [0.5, 1.0]\nprint(\"dW:\", dW)  # [[1, 1.5], [0, 0]]\nprint(\"db:\", db)  # [0.5, 0]</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/dropout/", "title": "Dropout", "text": "<p>Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped. Source: Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p> <p>Forward Pass: Mask during training.</p> <p>Backward Pass: Same mask applied to upstream gradient (scale by 1/(1-p)).</p> <p>Numerical Example: Same as forward; backward passes dy through mask.</p> <p>Implementation:</p> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef dropout_forward(x, p, training=True):\n    if training:\n        mask = np.random.binomial(1, 1-p, size=x.shape) / (1-p)\n        y = x * mask\n        return y, mask\n    return x, None\n\ndef dropout_backward(dy, mask):\n    if mask is None:\n        return dy\n    return dy * mask\n\n# Example\nnp.random.seed(0)\nx = np.array([1,2,3,4.])\np = 0.5\ny, mask = dropout_forward(x, p)\ndy = np.array([0.1,0.2,0.3,0.4])\ndx = dropout_backward(dy, mask)\nprint(\"dx:\", dx)</pre> Output Clear <pre></pre> <p></p>"}, {"location": "classes/deep-learning/lstm/", "title": "Lstm", "text": "<p>Recurrent Neural Networks (RNNs) are powerful for sequence data. Long Short-Term Memory (LSTM) networks are a type of RNN designed to capture long-term dependencies and mitigate issues like vanishing gradients.</p> <p>Parameters:</p> <ul> <li> <p>(Simplified to hidden size=1 for clarity):</p> <ul> <li> <p>Inputs:</p> <p>\\( x_t = [0.5] \\),</p> <p>\\( h_{t-1} = [0.1] \\),</p> <p>\\( C_{t-1} = [0.2] \\)</p> </li> <li> <p>Weights:</p> <p>\\( W_f = [[0.5, 0.5]] \\),</p> <p>\\( W_i = [[0.4, 0.4]] \\),</p> <p>\\( W_C = [[0.3, 0.3]] \\),</p> <p>\\( W_o = [[0.2, 0.2]] \\)</p> </li> <li> <p>Biases: \\( b_f = b_i = b_C = b_o = [0.0] \\)</p> </li> </ul> </li> </ul> <p>Forward Pass:</p> <ul> <li> <ul> <li>Concatenate: \\( \\text{concat} = [h_{t-1}, x_t] \\)</li> <li>Forget gate: \\( f_t = \\sigma(W_f \\cdot \\text{concat} + b_f) \\)</li> <li>Input gate: \\( i_t = \\sigma(W_i \\cdot \\text{concat} + b_i) \\)</li> <li>Cell candidate: \\( \\tilde{C}_t = \\tanh(W_C \\cdot \\text{concat} + b_C) \\)</li> <li>Cell state: \\( C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t \\)</li> <li>Output gate: \\( o_t = \\sigma(W_o \\cdot \\text{concat} + b_o) \\)</li> <li>Hidden state: \\( h_t = o_t \\cdot \\tanh(C_t) \\)</li> </ul> </li> <li> <ul> <li>concat = [0.1, 0.5]</li> <li>\\( f_t = \\sigma(0.3) \\approx 0.5744 \\)</li> <li>\\( i_t = \\sigma(0.24) \\approx 0.5597 \\)</li> <li>\\( \\tilde{C}_t = \\tanh(0.18) \\approx 0.1785 \\)</li> <li>\\( C_t \\approx 0.5744 \\cdot 0.2 + 0.5597 \\cdot 0.1785 \\approx 0.2146 \\)</li> <li>\\( o_t = \\sigma(0.12) \\approx 0.5300 \\)</li> <li>\\( h_t \\approx 0.5300 \\cdot \\tanh(0.2146) \\approx 0.1120 \\)</li> </ul> </li> </ul> <p>Backward Pass:</p> <ul> <li> <p>Gradients are computed via chain rule:</p> <ul> <li>\\( dC_t = dh_t \\cdot o_t \\cdot (1 - \\tanh^2(C_t)) + dC_{next} \\) (dC_next from future timestep)</li> <li>\\( do_t = dh_t \\cdot \\tanh(C_t) \\cdot \\sigma'(o_t) \\)</li> <li>\\( d\\tilde{C}_t = dC_t \\cdot i_t \\cdot (1 - \\tilde{C}_t^2) \\)</li> <li>\\( di_t = dC_t \\cdot \\tilde{C}_t \\cdot \\sigma'(i_t) \\)</li> <li>\\( df_t = dC_t \\cdot C_{t-1} \\cdot \\sigma'(f_t) \\)</li> <li>\\( dC_{prev} = dC_t \\cdot f_t \\)</li> <li>Then, backpropagate to concat: \\( d\\text{concat} = W_o^T \\cdot do_t + W_C^T \\cdot d\\tilde{C}_t + W_i^T \\cdot di_t + W_f^T \\cdot df_t \\)</li> <li>Split \\( d\\text{concat} \\) into \\( dh_{prev} \\) and \\( dx_t \\)</li> <li>Parameter gradients: \\( dW_f = df_t \\cdot \\text{concat}^T \\), \\( db_f = df_t \\), and similarly for others.</li> </ul> </li> <li> <p>(Assume upstream: \\( dh_t = [0.1] \\), \\( dC_t = [0.05] \\) from next timestep):</p> <ul> <li>\\( dC_t \\approx 0.1 \\cdot 0.5300 \\cdot (1 - \\tanh^2(0.2146)) + 0.05 \\approx 0.1028 + 0.05 = 0.1528 \\) (detailed steps in code output)</li> <li>Resulting gradients match the executed values below (e.g., \\( dx_t \\approx [0.0216] \\), etc.).</li> </ul> </li> </ul> <p> </p> Editor (session: default) Run <pre>import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef dsigmoid(y):\n    return y * (1 - y)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(y):\n    return 1 - y**2\n\n# Forward pass\ndef lstm_forward(x_t, h_prev, C_prev, W_f, W_i, W_C, W_o, b_f, b_i, b_C, b_o):\n    concat = np.concatenate((h_prev, x_t), axis=0)\n    f_t = sigmoid(np.dot(W_f, concat) + b_f)\n    i_t = sigmoid(np.dot(W_i, concat) + b_i)\n    C_tilde = tanh(np.dot(W_C, concat) + b_C)\n    C_t = f_t * C_prev + i_t * C_tilde\n    o_t = sigmoid(np.dot(W_o, concat) + b_o)\n    h_t = o_t * tanh(C_t)\n    cache = (concat, f_t, i_t, C_tilde, o_t, C_t)\n    return h_t, C_t, cache\n\n# Backward pass\ndef lstm_backward(dh_next, dC_next, cache, W_f, W_i, W_C, W_o):\n    concat, f_t, i_t, C_tilde, o_t, C_t = cache\n\n    # Derivatives\n    dC_t = dh_next * o_t * dtanh(tanh(C_t)) + dC_next\n    do_t = dh_next * tanh(C_t) * dsigmoid(o_t)\n    dC_tilde = dC_t * i_t * dtanh(C_tilde)\n    di_t = dC_t * C_tilde * dsigmoid(i_t)\n    df_t = dC_t * C_prev * dsigmoid(f_t)\n    dC_prev = dC_t * f_t\n\n    # Gradients for gates\n    dconcat_o = np.dot(W_o.T, do_t)\n    dconcat_C = np.dot(W_C.T, dC_tilde)\n    dconcat_i = np.dot(W_i.T, di_t)\n    dconcat_f = np.dot(W_f.T, df_t)\n    dconcat = dconcat_f + dconcat_i + dconcat_C + dconcat_o\n\n    # Split for h_prev and x_t\n    hidden_size = h_prev.shape[0]\n    dh_prev = dconcat[:hidden_size]\n    dx_t = dconcat[hidden_size:]\n\n    # Parameter gradients\n    dW_f = np.outer(df_t, concat)\n    db_f = df_t\n    dW_i = np.outer(di_t, concat)\n    db_i = di_t\n    dW_C = np.outer(dC_tilde, concat)\n    db_C = dC_tilde\n    dW_o = np.outer(do_t, concat)\n    db_o = do_t\n\n    return dx_t, dh_prev, dC_prev, dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o\n\n# Numerical example (hidden size = 1)\nx_t = np.array([0.5])\nh_prev = np.array([0.1])\nC_prev = np.array([0.2])\nW_f = np.array([[0.5, 0.5]])\nW_i = np.array([[0.4, 0.4]])\nW_C = np.array([[0.3, 0.3]])\nW_o = np.array([[0.2, 0.2]])\nb_f = np.array([0.0])\nb_i = np.array([0.0])\nb_C = np.array([0.0])\nb_o = np.array([0.0])\n\n# Forward\nh_t, C_t, cache = lstm_forward(x_t, h_prev, C_prev, W_f, W_i, W_C, W_o, b_f, b_i, b_C, b_o)\nprint(\"Forward h_t:\", h_t)  # Output: [0.11199714]\nprint(\"Forward C_t:\", C_t)  # Output: [0.2145628]\n\n# Backward example: assume dh_next = [0.1], dC_next = [0.05]\ndh_next = np.array([0.1])\ndC_next = np.array([0.05])\ndx_t, dh_prev, dC_prev, dW_f, db_f, dW_i, db_i, dW_C, db_C, dW_o, db_o = lstm_backward(dh_next, dC_next, cache, W_f, W_i, W_C, W_o)\n\nprint(\"Backward dx_t:\", dx_t)  # Output: [0.02164056]\nprint(\"Backward dh_prev:\", dh_prev)  # Output: [0.02164056]\nprint(\"Backward dC_prev:\", dC_prev)  # Output: [0.05780591]\nprint(\"Backward dW_f:\", dW_f)  # Output: [[0.00049199 0.00245997]]\nprint(\"Backward db_f:\", db_f)  # Output: [0.00491995]\nprint(\"Backward dW_i:\", dW_i)  # Output: [[0.00044162 0.00220808]]\nprint(\"Backward db_i:\", db_i)  # Output: [0.00441615]\nprint(\"Backward dW_C:\", dW_C)  # Output: [[0.00545376 0.02726878]]\nprint(\"Backward db_C:\", db_C)  # Output: [0.05453756]\nprint(\"Backward dW_o:\", dW_o)  # Output: [[0.00052643 0.00263213]]\nprint(\"Backward db_o:\", db_o)  # Output: [0.00526427]</pre> Output Clear <pre></pre> <p></p> <p>Notes</p> <ul> <li>This is a single-timestep LSTM with hidden size 1 for simplicity. In practice, LSTMs process sequences (multiple timesteps) and have larger hidden sizes; backpropagation through time (BPTT) unrolls the network over timesteps.</li> <li>The code uses NumPy; for real models, use PyTorch or TensorFlow for automatic differentiation and batching.</li> <li>Outputs are approximate due to floating-point precision but match the manual calculations.</li> <li>If you need a multi-timestep example, sequence processing, or integration into a full RNN, let me know!</li> </ul>"}, {"location": "classes/diffusion-models/", "title": "Index", "text": ""}, {"location": "classes/diffusion-models/#introduction-to-stable-diffusion", "title": "Introduction to Stable Diffusion", "text": "<p>Stable Diffusion is a type of generative AI model based on diffusion models, specifically a Latent Diffusion Model (LDM). It generates images from text prompts by learning to reverse a noise-adding process. The core idea comes from Denoising Diffusion Probabilistic Models (DDPMs), where data (e.g., images) is gradually corrupted with noise (forward process), and a neural network learns to reverse this by predicting and removing noise (backward process). This allows sampling new data from noise.</p> <p>Key components in Stable Diffusion:</p> <ul> <li>VAE (Variational Autoencoder): Compresses images to a lower-dimensional latent space for efficiency (e.g., from 512x512 pixels to 64x64 latents).</li> <li>U-Net: A CNN-like architecture (with attention for text conditioning) that predicts noise at each step.</li> <li>Text Encoder (e.g., CLIP): Converts prompts to embeddings for conditioning.</li> <li>Scheduler: Controls the noise addition/removal schedule (e.g., linear beta schedule).</li> </ul> <p>The \"forward pass\" refers to the diffusion (noising) process during training. The \"backward pass\" is the reverse diffusion (denoising) for generation, but training involves backpropagation to update the model. I'll focus on the math for the core DDPM, then note Stable Diffusion's extensions. Assume images as vectors \\( \\mathbf{x} \\in \\mathbb{R}^D \\) (flattened), time steps \\( T \\) (e.g., 1000).</p>"}, {"location": "classes/diffusion-models/#forward-pass-in-diffusion-models-noising-process", "title": "Forward Pass in Diffusion Models (Noising Process)", "text": "<p>The forward pass is a Markov chain that progressively adds Gaussian noise to the data until it's pure noise. This is non-learnable; it's fixed.</p>"}, {"location": "classes/diffusion-models/#key-notations", "title": "Key Notations:", "text": "<ul> <li>Clean data: \\( \\mathbf{x}_0 \\sim q(\\mathbf{x}_0) \\) (from dataset).</li> <li>Time step: \\( t = 1 \\) to \\( T \\).</li> <li>Noise schedule: \\( \\beta_t \\in (0,1) \\) (variance, often increasing linearly from ~0.0001 to 0.02).</li> <li>Cumulative: \\( \\alpha_t = 1 - \\beta_t \\), \\( \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s \\).</li> <li>Noise: \\( \\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\).</li> </ul>"}, {"location": "classes/diffusion-models/#forward-transition", "title": "Forward Transition:", "text": "<p>At each step:</p> \\[ q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\alpha_t} \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I}) \\] <p>Direct sampling from \\( \\mathbf{x}_0 \\) to any \\( \\mathbf{x}_t \\) (key for training):</p> \\[ \\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\mathbf{\\epsilon} \\] <ul> <li>How to arrive at this: This closed-form derives from reparameterizing the Gaussian transitions. Starting from \\( \\mathbf{x}_1 = \\sqrt{\\alpha_1} \\mathbf{x}_0 + \\sqrt{\\beta_1} \\mathbf{\\epsilon}_1 \\), inductively, the mean scales by \\( \\sqrt{\\bar{\\alpha}_t} \\), and variance accumulates to \\( 1 - \\bar{\\alpha}_t \\). At \\( t=T \\), \\( \\mathbf{x}_T \\approx \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\).</li> </ul> <p>In Stable Diffusion, this happens in latent space: First, encode image \\( \\mathbf{z}_0 = \\text{Encoder}(\\mathbf{x}_0) \\), then diffuse \\( \\mathbf{z}_t \\).</p>"}, {"location": "classes/diffusion-models/#backward-pass-in-diffusion-models-denoising-process", "title": "Backward Pass in Diffusion Models (Denoising Process)", "text": "<p>The backward pass learns to reverse the forward process, starting from noise \\( \\mathbf{x}_T \\) and iteratively denoising to \\( \\mathbf{x}_0 \\). A neural network \\( \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\) (e.g., U-Net) predicts the noise \\( \\mathbf{\\epsilon} \\) added at step t.</p>"}, {"location": "classes/diffusion-models/#key-notations_1", "title": "Key Notations:", "text": "<ul> <li>Reverse transition: \\( p_\\theta(\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t), \\sigma_t^2 \\mathbf{I}) \\).</li> <li>Predicted mean: \\( \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\right) \\).</li> <li>Variance: \\( \\sigma_t^2 = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t \\) (or simplified in DDPM).</li> </ul>"}, {"location": "classes/diffusion-models/#training-objective", "title": "Training Objective:", "text": "<p>Minimize the difference between predicted and actual noise. Loss (simplified variational bound):</p> \\[ L = \\mathbb{E}_{t, \\mathbf{x}_0, \\mathbf{\\epsilon}} \\left[ \\| \\mathbf{\\epsilon} - \\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\|^2 \\right] \\] <p>Where \\( \\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\mathbf{\\epsilon} \\), t uniform in [1,T].</p> <ul> <li>Backpropagation: Standard gradient descent on \\( L \\) w.r.t. \\( \\theta \\) (U-Net params). The \"backward pass\" here means both the reverse sampling and the autograd backward for training.</li> </ul>"}, {"location": "classes/diffusion-models/#inference-generation", "title": "Inference (Generation):", "text": "<p>Start from \\( \\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\), then for t=T to 1:</p> \\[ \\mathbf{x}_{t-1} = \\mathbf{\\mu}_\\theta(\\mathbf{x}_t, t) + \\sigma_t \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\] <ul> <li>How to arrive at this: The reverse is derived by matching the forward Gaussian posterior. The mean \\( \\mathbf{\\mu}_\\theta \\) is obtained by Bayes' rule on the forward process, approximating the reverse with the noise predictor. The loss is a reweighted ELBO from the diffusion model's probabilistic framework.</li> </ul> <p>In Stable Diffusion: - Conditioning: \\( \\mathbf{\\epsilon}_\\theta(\\mathbf{z}_t, t, \\mathbf{c}) \\), where \\( \\mathbf{c} \\) is text embedding. - After denoising \\( \\mathbf{z}_0 \\), decode to image: \\( \\mathbf{x}_0 = \\text{Decoder}(\\mathbf{z}_0) \\). - Schedulers like DDIM allow fewer steps (e.g., 50 instead of 1000) by deterministic sampling.</p>"}, {"location": "classes/diffusion-models/#example-code-simple-ddpm-in-pytorch", "title": "Example Code: Simple DDPM in PyTorch", "text": "<p>Here's a minimal PyTorch example of a DDPM (basis for Stable Diffusion). It trains a small MLP (instead of U-Net for simplicity) to denoise 1D data (e.g., toy Gaussians). For images, replace with a U-Net and 2D tensors. This shows forward (noising), training loss, and backward (denoising inference + autograd).</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Simple noise predictor (MLP instead of U-Net for toy example)\nclass NoisePredictor(nn.Module):\n    def __init__(self, dim=32):\n        super(NoisePredictor, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(dim + 1, 64),  # Input: x_t + t (embedded)\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, dim)  # Output: predicted epsilon\n        )\n\n    def forward(self, x, t):\n        t = t.unsqueeze(1) / 1000.0  # Normalize t\n        input = torch.cat([x, t], dim=1)\n        return self.fc(input)\n\n# Diffusion parameters\nT = 1000\nbeta = torch.linspace(1e-4, 0.02, T)\nalpha = 1 - beta\nalpha_bar = torch.cumprod(alpha, dim=0)\n\ndef forward_diffusion(x0, t, noise):\n    sqrt_alpha_bar = torch.sqrt(alpha_bar[t-1]).view(-1, 1)  # t starts from 1\n    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar[t-1]).view(-1, 1)\n    return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n\n# Training loop snippet\nmodel = NoisePredictor(dim=32)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Toy data: batch of 1D vectors from N(0,1)\nbatch_size = 64\nx0 = torch.randn(batch_size, 32)  # Example data\n\nfor epoch in range(100):  # Simplified\n    t = torch.randint(1, T+1, (batch_size,))\n    noise = torch.randn_like(x0)\n    xt = forward_diffusion(x0, t, noise)\n\n    pred_noise = model(xt, t.float())\n    loss = nn.MSELoss()(pred_noise, noise)\n\n    optimizer.zero_grad()\n    loss.backward()  # Backward pass (autograd)\n    optimizer.step()\n\nprint(\"Training Loss:\", loss.item())\n\n# Inference: Denoise from pure noise\ndef denoise(model, shape):\n    x = torch.randn(shape)  # Start from x_T\n    for t in range(T, 0, -1):\n        t_tensor = torch.full((shape[0],), t, dtype=torch.long)\n        pred_noise = model(x, t_tensor.float())\n        alpha_t = alpha[t-1]\n        beta_t = beta[t-1]\n        alpha_bar_t = alpha_bar[t-1]\n\n        mu = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * pred_noise)\n        sigma = torch.sqrt(beta_t)  # Simplified\n        x = mu + sigma * torch.randn_like(x) if t &gt; 1 else mu\n    return x\n\ngenerated = denoise(model, (1, 32))\nprint(\"Generated Sample:\", generated[0][:5])  # First 5 elements\n</code></pre>"}, {"location": "classes/diffusion-models/#running-this-code", "title": "Running this Code:", "text": "<p>This trains on random data and generates new samples. Outputs might look like: - Training Loss: 0.85 (decreases over epochs) - Generated Sample: tensor([0.1234, -0.5678, 0.9101, ...])</p> <p>For Stable Diffusion, use libraries like <code>diffusers</code> from Hugging Face for real implementation (e.g., <code>StableDiffusionPipeline</code>). The math scales up: U-Net predicts noise on latents, with cross-attention for text.</p> <p>https://github.com/openai/CLIP https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/multimodal/vlm/clip.html https://openai.com/index/clip/ https://huggingface.co/docs/transformers/model_doc/clip https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training https://en.wikipedia.org/wiki/U-Net</p> <ol> <li> <p>The Illustrated Transformer \u21a9</p> </li> </ol>"}, {"location": "classes/flow-matching/", "title": "17. Flow-Matching", "text": "<ol> <li> <p>Flow-Matching: A New Paradigm for Generative Modeling, 2022.\u00a0\u21a9</p> </li> <li> <p>Flux: A General Framework for Diffusion Models, 2024.\u00a0\u21a9</p> </li> </ol>"}, {"location": "classes/generative-adversarial-networks/", "title": "13. GAN", "text": "<p>Generative Adversarial Networks (GAN) help machines to create new, realistic data by learning from existing examples. It is introduced by Ian Goodfellow and his team in 2014 and they have transformed how computers generate images, videos, music and more. Unlike traditional models that only recognize or classify data, they take a creative way by generating entirely new content that closely resembles real-world data. This ability helped various fields such as art, gaming, healthcare and data science. In this article, we will see more about GANs and its core concepts.<sup>1</sup></p> <p>GAN consists of two main components: the Generator and the Discriminator. The Generator creates new data instances, while the Discriminator evaluates them for authenticity.</p> <p></p> <p>Generative Adversarial Network (GAN) consists of two main components: the Generator and the Discriminator. The Generator creates new data instances, while the Discriminator evaluates them for authenticity. Image Source: <sup></sup>4</p> <p>The goal of the Generator is to produce data that is indistinguishable from real data, while the Discriminator aims to correctly identify whether the data is real or generated. This creates a dynamic where both components improve over time, leading to the generation of highly realistic data.</p> <p></p> <p>Illustration of how GANs work. The Generator creates fake data from random noise, and the Discriminator evaluates both real and fake data to determine their authenticity. Over time, both networks improve through this adversarial process. Image Source: Caltech - What is Generative Adversarial Network?</p>"}, {"location": "classes/generative-adversarial-networks/#generator", "title": "Generator", "text": "<p>The Generator is a deep neural network that takes random noise as input and generates data that mimics the real data distribution. It aims to produce outputs that are so realistic that the Discriminator cannot distinguish them from actual data. The Generator is trained to minimize the difference between the generated data distribution and the real data distribution, often using measures like Kullback\u2013Leibler Divergence.<sup>2</sup></p> <p>Generator Loss Function:</p> \\[ L_G = -\\mathbb{E}_{z \\sim p_z(z)}[\\log(D(G(z)))] \\] <p>Where:</p> <ul> <li>\\( L_G \\) is the loss for the Generator.</li> <li>\\( z \\) is the input noise vector sampled from a prior distribution \\( p_z(z) \\).</li> <li>\\( G(z) \\) is the generated data from the noise vector.</li> <li>\\( D(G(z)) \\) is the Discriminator's output for the generated data.</li> </ul> <p>The generator aims to maximize \\( D(G(z)) \\) meaning it wants the discriminator to classify its fake data as real (probability close to 1).</p> <p>The Generator is typically implemented as a deep neural network that takes the noise vector \\( z \\) as input and produces a data sample \\( G(z) \\) as output.</p>"}, {"location": "classes/generative-adversarial-networks/#discriminator", "title": "Discriminator", "text": "<p>The Discriminator is another deep neural network that takes both real and generated data as input and tries to distinguish between the two. It outputs a probability score indicating whether the input data is real (from the training set) or fake (generated by the Generator). The Discriminator is trained to maximize its accuracy in classifying real and generated data, often using binary cross-entropy loss.</p> <p>Discriminator Loss Function:</p> \\[ L_D = -\\mathbb{E}_{x \\sim p_{data}(x)}[\\log(D(x))] - \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] \\] <p>Where:</p> <ul> <li>\\( L_D \\) is the loss for the Discriminator.</li> <li>\\( x \\) is a real data sample from the true data distribution \\( p_{data}(x) \\).</li> <li>\\( z \\) is the input noise vector sampled from a prior distribution \\( p_z(z) \\).</li> <li>\\( G(z) \\) is the generated data from the noise vector.</li> <li>\\( D(x) \\) is the Discriminator's output for the real data.</li> <li>\\( D(G(z)) \\) is the Discriminator's output for the generated data.</li> </ul> <p>The discriminator wants to correctly classify real data as real (maximize \\( D(x) \\)) and generated data as fake (minimize \\( D(G(z)) \\)).</p> <p>During training, the Discriminator receives feedback on its performance, which helps it become better at identifying subtle differences between real and generated data. As the Generator improves and produces more realistic data, the Discriminator must also adapt to maintain its ability to differentiate between the two.</p> <p>The interplay between the Generator and Discriminator creates a competitive environment, driving both networks to improve continuously. This adversarial process is what gives GANs their name and is key to their success in generating high-quality, realistic data.</p>"}, {"location": "classes/generative-adversarial-networks/#training-process", "title": "Training Process", "text": "<p>The training of GANs involves an iterative process where both the Generator and Discriminator are updated in turns. The typical training loop consists of the following steps:</p> <ol> <li> <p>Generator's First Move</p> <p>The generator starts by taking a random noise vector (often sampled from a normal or uniform distribution) and transforms it into a fake data sample.</p> </li> <li> <p>Discriminator's Turn</p> <p>The discriminator receives two types of data:</p> <ul> <li>Real data from the training dataset.</li> <li>Fake data generated by the generator.</li> </ul> <p>The discriminator has to classify each input as real or fake. It produces a probability score for each input, indicating the likelihood of it being real, ranging from 0 (fake) to 1 (real).</p> </li> <li> <p>Adversarial Learning</p> <ul> <li> <p>If the discriminator correctly identifies real and fake data, it gets rewarded.</p> </li> <li> <p>If the generator successfully fools the discriminator into thinking fake data is real, it gets rewarded, and the discriminator is penalized.</p> </li> </ul> </li> <li> <p>Generator's Improvement</p> <p>The generator updates its weights based on the feedback from the discriminator. It aims to produce more convincing fake data that can better deceive the discriminator in future iterations.</p> </li> <li> <p>Discriminator's Adaptation</p> <p>The discriminator also updates its weights based on its performance. It learns to become more adept at distinguishing real data from the increasingly realistic fake data produced by the generator.</p> </li> <li> <p>Training Progression</p> <p>As training continues, the generator becomes highly proficient at producing realistic data. Eventually the discriminator struggles to distinguish real from fake shows that the GAN has reached a well-trained state. At this point, the generator can produce high-quality synthetic data that can be used for different applications.</p> </li> </ol> <p>MinMax Loss Function:</p> <p>GANs are trained using a MinMax game between the Generator and Discriminator. The Generator tries to minimize the probability of the Discriminator correctly identifying fake data, while the Discriminator tries to maximize its accuracy in distinguishing real from fake data.</p> <p>The overall objective of GANs can be summarized with the following MinMax loss function:</p> \\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log(D(x))] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] \\] <p>Where:</p> <ul> <li>\\( V(D, G) \\) is the value function representing the adversarial game between the Generator and Discriminator.</li> <li>The Generator \\( G \\) aims to minimize this value function, while the Discriminator \\( D \\) aims to maximize it.</li> <li>\\( x \\) is a real data sample from the true data distribution \\( p_{data}(x) \\).</li> <li>\\( z \\) is the input noise vector sampled from a prior distribution \\( p_z(z) \\).</li> <li>\\( G(z) \\) is the generated data from the noise vector.</li> <li>\\( D(x) \\) is the Discriminator's output for the real data.</li> </ul> <p>This process is repeated for many iterations, with the Generator and Discriminator continuously improving in response to each other's performance. The training continues until the Generator produces data that is indistinguishable from real data, or until a predefined number of iterations is reached.</p>"}, {"location": "classes/generative-adversarial-networks/#variants-of-gans", "title": "Variants of GANs", "text": "<p>Over the years, several variants of GANs have been developed to address specific challenges and improve performance. Some notable variants include:</p> Variant Innovation Math Highlight Description Applications DCGAN (Deep Convolutional GAN) Uses convolutional layers in both the Generator and Discriminator Convolutional Neural Networks (CNNs) Introduces convolutional layers to better capture spatial hierarchies in image data. Image generation, style transfer WGAN (Wasserstein GAN) Uses Wasserstein distance as a loss metric Wasserstein Distance Improves training stability and addresses mode collapse by using the Wasserstein distance as a loss function. CGAN (Conditional GAN) Conditions the generation process on additional information Conditional Probability Allows the generation of data conditioned on specific attributes or labels. Image-to-image translation, text-to-image synthesis StyleGAN Introduces style-based architecture Style Transfer Enables control over different levels of detail in generated images through a style-based architecture. High-resolution image synthesis, face generation CycleGAN Uses cycle consistency loss for unpaired image-to-image translation Cycle Consistency Loss Enables image-to-image translation without paired training data by enforcing cycle consistency. Style transfer, domain adaptation"}, {"location": "classes/generative-adversarial-networks/#challenges-in-training-gans", "title": "Challenges in Training GANs", "text": "<p>Training GANs can be challenging due to several factors:</p> Challenge Description Mode Collapse The generator may produce a limited variety of outputs, leading to a lack of diversity in the generated data.<sup>6</sup> Training Instability The adversarial nature of GANs can lead to unstable training dynamics, making it difficult to achieve convergence. Evaluation Metrics Assessing the quality of generated data can be subjective and challenging, as traditional metrics may not fully capture the realism of the outputs. Hyperparameter Tuning Finding the right hyperparameters (e.g., learning rates, batch sizes) for both the Generator and Discriminator is crucial for stable training but can be time-consuming and complex."}, {"location": "classes/generative-adversarial-networks/#applications-of-gans", "title": "Applications of GANs", "text": "<p>GANs have a wide range of applications across various fields:</p> Application Description Image Generation GANs can create high-quality images that are indistinguishable from real photos. Data Augmentation GANs can generate additional training data to improve the performance of machine learning models. Style Transfer GANs can apply the style of one image to another, enabling artistic transformations. Super-Resolution GANs can enhance the resolution of images, making them clearer and more detailed. Text-to-Image Synthesis GANs can generate images from textual descriptions, enabling creative content generation. Anomaly Detection GANs can be used to identify anomalies in data by learning the normal data distribution."}, {"location": "classes/generative-adversarial-networks/#implementations", "title": "Implementations", "text": ""}, {"location": "classes/generative-adversarial-networks/#cifar-10", "title": "CIFAR-10", "text": "CIFAR-10 <p>Below is a basic example of how to create and train a GAN on the CIFAR-10 dataset.</p> <p>Example extracted from https://www.geeksforgeeks.org/deep-learning/generative-adversarial-network-gan/</p> <ol> <li> <p>Geeks for Geeks: Generative Adversarial Network (GAN) \u21a9</p> </li> <li> <p>Kullback\u2013Leibler Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. In the context of GANs, it can be used to quantify the difference between the distribution of real data and the distribution of data generated by the Generator. Minimizing this divergence helps improve the quality of the generated data, making it more similar to the real data.\u00a0\u21a9</p> </li> <li> <p>Wasserstein Distance is another metric used to measure the distance between two probability distributions. In the context of GANs, the Wasserstein GAN (WGAN) uses this distance to provide a more stable training process and to mitigate issues like mode collapse.\u00a0\u21a9</p> </li> <li> <p>Intro to GANs by Sthelles Silva provides a comprehensive introduction to the concepts and workings of Generative Adversarial Networks (GANs). The article covers the fundamental principles behind GANs, including the roles of the Generator and Discriminator, the adversarial training process, and common challenges faced during training. It also discusses various applications of GANs in fields such as image generation, data augmentation, and more. This resource is valuable for anyone looking to understand how GANs function and their significance in the field of machine learning.\u00a0\u21a9</p> </li> <li> <p>Caltech - What is Generative Adversarial Network? \u21a9</p> </li> <li> <p>Mode Collapse is a common problem in GAN training where the Generator produces a limited variety of outputs, often collapsing to a few modes of the data distribution. This results in a lack of diversity in the generated samples, which can be detrimental to the overall performance of the GAN.\u00a0\u21a9</p> </li> <li> <p>GAN Lab is an interactive visualization tool that helps users understand how Generative Adversarial Networks (GANs) work. It provides a hands-on experience of training GANs, allowing users to visualize the generator and discriminator networks, observe their interactions, and see how they evolve during the training process. This tool is particularly useful for those new to GANs or for educators looking to demonstrate the concepts behind adversarial training in a more intuitive way.\u00a0\u21a9</p> </li> <li> <p>TensorFlow GAN Tutorial provides a step-by-step guide on how to implement a Deep Convolutional Generative Adversarial Network (DCGAN) using TensorFlow and Keras. The tutorial covers the essential components of GANs, including the architecture of the Generator and Discriminator, the training process, and how to generate new images after training. It is a practical resource for those looking to gain hands-on experience with GANs and understand their implementation in a deep learning framework.\u00a0\u21a9</p> </li> <li> <p>ThisPersonDoesNotExist is a website that uses Generative Adversarial Networks (GANs) to generate realistic images of human faces that do not belong to any real person. Each time the page is refreshed, a new, unique face is created by the GAN model, showcasing the impressive capabilities of AI in generating lifelike images. The site serves as a fascinating demonstration of how GANs can be used for creative and practical applications in image synthesis. |   https://github.com/NVlabs/stylegan2 |  Analyzing and Improving the Image Quality of StyleGAN.\u00a0\u21a9</p> </li> <li> <p>Artbreeder is an online platform that leverages Generative Adversarial Networks (GANs) to allow users to create and explore new images by blending and evolving existing ones. Users can manipulate various parameters to generate unique artworks, portraits, landscapes, and more. Artbreeder provides an intuitive interface for experimenting with AI-generated art, making it accessible for both artists and enthusiasts interested in the creative possibilities of GANs.\u00a0\u21a9</p> </li> <li> <p>A Short Introduction to Generative Adversarial Networks \u21a9</p> </li> <li> <p>DeepLearning Book - Generative Adversarial Networks \u21a9</p> </li> </ol>"}, {"location": "classes/generative-adversarial-networks/#1-importing-libraries", "title": "1. Importing Libraries", "text": "<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#2-defining-image-transformations", "title": "2. Defining Image Transformations", "text": "<pre><code>transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#3-loaging-the-cifar-10-dataset", "title": "3. Loaging the CIFAR-10 Dataset", "text": "<pre><code>train_dataset = datasets.CIFAR10(root='./data',\\\n              train=True, download=True, transform=transform)\ndataloader = torch.utils.data.DataLoader(train_dataset, \\\n                                batch_size=32, shuffle=True)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:16&lt;00:00, 10.5MB/s]\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#4-defining-gan-hyperparameters", "title": "4. Defining GAN Hyperparameters", "text": "<p>Set important training parameters:</p> <ul> <li>latent_dim: Dimensionality of the noise vector.</li> <li>lr: Learning rate of the optimizer.</li> <li>beta1, beta2: Beta parameters for Adam optimizer (e.g 0.5, 0.999)</li> <li>num_epochs: Number of times the entire dataset will be processed (e.g 10)</li> </ul> <pre><code>latent_dim = 100\nlr = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\nnum_epochs = 30\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#5-building-the-generator", "title": "5. Building the Generator", "text": "<p>Create a neural network that converts random noise into images. Use transpose convolutional layers, batch normalization and ReLU activations. The final layer uses Tanh activation to scale outputs to the range [-1, 1].</p> <ul> <li>nn.Linear(latent_dim, 128 * 8 * 8): Defines a fully connected layer that projects the noise vector into a higher dimensional feature space.</li> <li>nn.Upsample(scale_factor=2): Doubles the spatial resolution of the feature maps by upsampling.</li> <li>nn.Conv2d(128, 128, kernel_size=3, padding=1): Applies a convolutional layer keeping the number of channels the same to refine features.</li> </ul> <pre><code>class Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128 * 8 * 8),\n            nn.ReLU(),\n            nn.Unflatten(1, (128, 8, 8)),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128, momentum=0.78),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64, momentum=0.78),\n            nn.ReLU(),\n            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        return img\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#6-building-the-discriminator", "title": "6. Building the Discriminator", "text": "<p>Create a binary classifier network that distinguishes real from fake images. Use convolutional layers, batch normalization, dropout, LeakyReLU activation and a Sigmoid output layer to give a probability between 0 and 1.</p> <ul> <li>nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1): Second convolutional layer increasing channels to 64, downsampling further.</li> <li>nn.BatchNorm2d(256, momentum=0.8): Batch normalization for 256 feature maps with momentum 0.8.</li> </ul> <pre><code>class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n        nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n        nn.LeakyReLU(0.2),\n        nn.Dropout(0.25),\n        nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n        nn.ZeroPad2d((0, 1, 0, 1)),\n        nn.BatchNorm2d(64, momentum=0.82),\n        nn.LeakyReLU(0.25),\n        nn.Dropout(0.25),\n        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n        nn.BatchNorm2d(128, momentum=0.82),\n        nn.LeakyReLU(0.2),\n        nn.Dropout(0.25),\n        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(256, momentum=0.8),\n        nn.LeakyReLU(0.25),\n        nn.Dropout(0.25),\n        nn.Flatten(),\n        nn.Linear(256 * 5 * 5, 1),\n        nn.Sigmoid()\n    )\n\n    def forward(self, img):\n        validity = self.model(img)\n        return validity\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#7-initializing", "title": "7. Initializing", "text": "<ul> <li>Generator and Discriminator are initialized on the available device (GPU or CPU).</li> <li>Binary Cross-Entropy (BCE) Loss is chosen as the loss function.</li> <li>Adam optimizers are defined separately for the generator and discriminator with specified learning rates and betas.</li> </ul> <pre><code>generator = Generator(latent_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\nadversarial_loss = nn.BCELoss()\n\noptimizer_G = optim.Adam(generator.parameters()\\\n                         , lr=lr, betas=(beta1, beta2))\noptimizer_D = optim.Adam(discriminator.parameters()\\\n                         , lr=lr, betas=(beta1, beta2))\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/#8-training", "title": "8. Training", "text": "<p>Train the discriminator on real and fake images, then update the generator to improve its fake image quality. Track losses and visualize generated images after each epoch.</p> <ul> <li>valid = torch.ones(real_images.size(0), 1, device=device): Create a tensor of ones representing real labels for the discriminator.</li> <li>fake = torch.zeros(real_images.size(0), 1, device=device): Create a tensor of zeros representing fake labels for the discriminator.</li> <li>z = torch.randn(real_images.size(0), latent_dim, device=device): Generate random noise vectors as input for the generator.</li> <li>g_loss = adversarial_loss(discriminator(gen_images), valid): Calculate generator loss based on the discriminator classifying fake images as real.</li> <li>grid = torchvision.utils.make_grid(generated, nrow=4, normalize=True): Arrange generated images into a grid for display, normalizing pixel values.</li> </ul> <pre><code>for epoch in range(num_epochs):\n    for i, batch in enumerate(dataloader):\n\n        real_images = batch[0].to(device)\n\n        # Create a tensor of ones representing real labels for the discriminator.\n        valid = torch.ones(real_images.size(0), 1, device=device)\n        # Create a tensor of zeros representing fake labels for the discriminator.\n        fake = torch.zeros(real_images.size(0), 1, device=device)\n\n        real_images = real_images.to(device)\n\n        optimizer_D.zero_grad()\n\n        # Generate random noise vectors as input for the generator.\n        z = torch.randn(real_images.size(0), latent_dim, device=device)\n\n        fake_images = generator(z)\n\n        real_loss = adversarial_loss(discriminator\\\n                                     (real_images), valid)\n        fake_loss = adversarial_loss(discriminator\\\n                                     (fake_images.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        optimizer_G.zero_grad()\n\n        gen_images = generator(z)\n\n        # Calculate generator loss based on the discriminator classifying fake images as real.\n        g_loss = adversarial_loss(discriminator(gen_images), valid)\n        g_loss.backward()\n        optimizer_G.step()\n\n        if (i + 1) % 100 == 0:\n            print(\n                f\"Epoch [{epoch+1}/{num_epochs}]\\\n                        Batch {i+1}/{len(dataloader)} \"\n                f\"Discriminator Loss: {d_loss.item():.4f} \"\n                f\"Generator Loss: {g_loss.item():.4f}\"\n            )\n    if (epoch + 1) % 10 == 0:\n        with torch.no_grad():\n            z = torch.randn(16, latent_dim, device=device)\n            generated = generator(z).detach().cpu()\n            # Arrange generated images into a grid for display, normalizing pixel values.\n            grid = torchvision.utils.make_grid(generated,\\\n                                        nrow=4, normalize=True)\n            plt.imshow(np.transpose(grid, (1, 2, 0)))\n            plt.axis(\"off\")\n            plt.show()\n\nplt.close()\n</code></pre> <pre><code>Epoch [1/30]                        Batch 100/1563 Discriminator Loss: 0.6310 Generator Loss: 1.0856\nEpoch [1/30]                        Batch 200/1563 Discriminator Loss: 0.7243 Generator Loss: 1.0722\nEpoch [1/30]                        Batch 300/1563 Discriminator Loss: 0.7337 Generator Loss: 1.1005\nEpoch [1/30]                        Batch 400/1563 Discriminator Loss: 0.6917 Generator Loss: 1.1717\nEpoch [1/30]                        Batch 500/1563 Discriminator Loss: 0.5310 Generator Loss: 1.1474\nEpoch [1/30]                        Batch 600/1563 Discriminator Loss: 0.7355 Generator Loss: 0.8708\nEpoch [1/30]                        Batch 700/1563 Discriminator Loss: 0.6745 Generator Loss: 1.2315\nEpoch [1/30]                        Batch 800/1563 Discriminator Loss: 0.7858 Generator Loss: 0.8245\nEpoch [1/30]                        Batch 900/1563 Discriminator Loss: 0.6330 Generator Loss: 0.9612\nEpoch [1/30]                        Batch 1000/1563 Discriminator Loss: 0.6853 Generator Loss: 1.1186\nEpoch [1/30]                        Batch 1100/1563 Discriminator Loss: 0.5872 Generator Loss: 1.3537\nEpoch [1/30]                        Batch 1200/1563 Discriminator Loss: 0.6135 Generator Loss: 1.0075\nEpoch [1/30]                        Batch 1300/1563 Discriminator Loss: 0.5315 Generator Loss: 1.4053\nEpoch [1/30]                        Batch 1400/1563 Discriminator Loss: 0.7756 Generator Loss: 0.8813\nEpoch [1/30]                        Batch 1500/1563 Discriminator Loss: 0.6498 Generator Loss: 0.8439\nEpoch [2/30]                        Batch 100/1563 Discriminator Loss: 0.6724 Generator Loss: 0.7996\nEpoch [2/30]                        Batch 200/1563 Discriminator Loss: 0.8157 Generator Loss: 0.7308\nEpoch [2/30]                        Batch 300/1563 Discriminator Loss: 0.5542 Generator Loss: 1.0256\nEpoch [2/30]                        Batch 400/1563 Discriminator Loss: 0.5977 Generator Loss: 1.1029\nEpoch [2/30]                        Batch 500/1563 Discriminator Loss: 0.6931 Generator Loss: 0.8717\nEpoch [2/30]                        Batch 600/1563 Discriminator Loss: 0.5693 Generator Loss: 1.3365\nEpoch [2/30]                        Batch 700/1563 Discriminator Loss: 0.6488 Generator Loss: 0.9503\nEpoch [2/30]                        Batch 800/1563 Discriminator Loss: 0.6761 Generator Loss: 0.8909\nEpoch [2/30]                        Batch 900/1563 Discriminator Loss: 0.5708 Generator Loss: 0.9972\nEpoch [2/30]                        Batch 1000/1563 Discriminator Loss: 0.6001 Generator Loss: 1.1221\nEpoch [2/30]                        Batch 1100/1563 Discriminator Loss: 0.6978 Generator Loss: 0.8663\nEpoch [2/30]                        Batch 1200/1563 Discriminator Loss: 0.7402 Generator Loss: 1.1249\nEpoch [2/30]                        Batch 1300/1563 Discriminator Loss: 0.6372 Generator Loss: 1.0888\nEpoch [2/30]                        Batch 1400/1563 Discriminator Loss: 0.6317 Generator Loss: 0.9584\nEpoch [2/30]                        Batch 1500/1563 Discriminator Loss: 0.5677 Generator Loss: 1.0163\nEpoch [3/30]                        Batch 100/1563 Discriminator Loss: 0.6370 Generator Loss: 1.3195\nEpoch [3/30]                        Batch 200/1563 Discriminator Loss: 0.5646 Generator Loss: 1.3755\nEpoch [3/30]                        Batch 300/1563 Discriminator Loss: 0.7698 Generator Loss: 0.8023\nEpoch [3/30]                        Batch 400/1563 Discriminator Loss: 0.5839 Generator Loss: 1.0319\nEpoch [3/30]                        Batch 500/1563 Discriminator Loss: 0.5613 Generator Loss: 1.2391\nEpoch [3/30]                        Batch 600/1563 Discriminator Loss: 0.5798 Generator Loss: 1.1867\nEpoch [3/30]                        Batch 700/1563 Discriminator Loss: 0.6622 Generator Loss: 1.4665\nEpoch [3/30]                        Batch 800/1563 Discriminator Loss: 0.7389 Generator Loss: 1.3259\nEpoch [3/30]                        Batch 900/1563 Discriminator Loss: 0.6855 Generator Loss: 1.4243\nEpoch [3/30]                        Batch 1000/1563 Discriminator Loss: 0.5348 Generator Loss: 1.5141\nEpoch [3/30]                        Batch 1100/1563 Discriminator Loss: 0.4696 Generator Loss: 1.3948\nEpoch [3/30]                        Batch 1200/1563 Discriminator Loss: 0.5467 Generator Loss: 0.6435\nEpoch [3/30]                        Batch 1300/1563 Discriminator Loss: 0.6741 Generator Loss: 0.9211\nEpoch [3/30]                        Batch 1400/1563 Discriminator Loss: 0.4664 Generator Loss: 1.0165\nEpoch [3/30]                        Batch 1500/1563 Discriminator Loss: 0.4799 Generator Loss: 0.9388\nEpoch [4/30]                        Batch 100/1563 Discriminator Loss: 0.6887 Generator Loss: 0.9108\nEpoch [4/30]                        Batch 200/1563 Discriminator Loss: 0.7852 Generator Loss: 0.8538\nEpoch [4/30]                        Batch 300/1563 Discriminator Loss: 0.5036 Generator Loss: 0.7702\nEpoch [4/30]                        Batch 400/1563 Discriminator Loss: 0.4476 Generator Loss: 1.3409\nEpoch [4/30]                        Batch 500/1563 Discriminator Loss: 0.6550 Generator Loss: 1.2383\nEpoch [4/30]                        Batch 600/1563 Discriminator Loss: 0.7891 Generator Loss: 1.6518\nEpoch [4/30]                        Batch 700/1563 Discriminator Loss: 0.5458 Generator Loss: 1.3395\nEpoch [4/30]                        Batch 800/1563 Discriminator Loss: 0.6637 Generator Loss: 0.9561\nEpoch [4/30]                        Batch 900/1563 Discriminator Loss: 0.6408 Generator Loss: 1.4673\nEpoch [4/30]                        Batch 1000/1563 Discriminator Loss: 0.5879 Generator Loss: 0.8866\nEpoch [4/30]                        Batch 1100/1563 Discriminator Loss: 0.5762 Generator Loss: 1.2028\nEpoch [4/30]                        Batch 1200/1563 Discriminator Loss: 0.4307 Generator Loss: 1.8184\nEpoch [4/30]                        Batch 1300/1563 Discriminator Loss: 0.8079 Generator Loss: 0.7371\nEpoch [4/30]                        Batch 1400/1563 Discriminator Loss: 0.5739 Generator Loss: 0.9877\nEpoch [4/30]                        Batch 1500/1563 Discriminator Loss: 0.5317 Generator Loss: 1.7946\nEpoch [5/30]                        Batch 100/1563 Discriminator Loss: 0.4036 Generator Loss: 1.1946\nEpoch [5/30]                        Batch 200/1563 Discriminator Loss: 0.4312 Generator Loss: 1.4645\nEpoch [5/30]                        Batch 300/1563 Discriminator Loss: 0.4584 Generator Loss: 1.0263\nEpoch [5/30]                        Batch 400/1563 Discriminator Loss: 0.5096 Generator Loss: 1.4634\nEpoch [5/30]                        Batch 500/1563 Discriminator Loss: 0.6048 Generator Loss: 1.7682\nEpoch [5/30]                        Batch 600/1563 Discriminator Loss: 0.7375 Generator Loss: 1.0852\nEpoch [5/30]                        Batch 700/1563 Discriminator Loss: 0.6004 Generator Loss: 1.3448\nEpoch [5/30]                        Batch 800/1563 Discriminator Loss: 0.5066 Generator Loss: 1.0084\nEpoch [5/30]                        Batch 900/1563 Discriminator Loss: 0.6089 Generator Loss: 1.0463\nEpoch [5/30]                        Batch 1000/1563 Discriminator Loss: 0.4824 Generator Loss: 1.2175\nEpoch [5/30]                        Batch 1100/1563 Discriminator Loss: 0.7544 Generator Loss: 0.8785\nEpoch [5/30]                        Batch 1200/1563 Discriminator Loss: 0.4876 Generator Loss: 1.2693\nEpoch [5/30]                        Batch 1300/1563 Discriminator Loss: 0.4419 Generator Loss: 0.9601\nEpoch [5/30]                        Batch 1400/1563 Discriminator Loss: 0.5805 Generator Loss: 1.0689\nEpoch [5/30]                        Batch 1500/1563 Discriminator Loss: 0.5700 Generator Loss: 1.5043\nEpoch [6/30]                        Batch 100/1563 Discriminator Loss: 0.3690 Generator Loss: 1.8759\nEpoch [6/30]                        Batch 200/1563 Discriminator Loss: 0.4619 Generator Loss: 1.5484\nEpoch [6/30]                        Batch 300/1563 Discriminator Loss: 0.5607 Generator Loss: 1.1555\nEpoch [6/30]                        Batch 400/1563 Discriminator Loss: 0.3786 Generator Loss: 1.5910\nEpoch [6/30]                        Batch 500/1563 Discriminator Loss: 0.6497 Generator Loss: 0.9485\nEpoch [6/30]                        Batch 600/1563 Discriminator Loss: 0.4534 Generator Loss: 1.4235\nEpoch [6/30]                        Batch 700/1563 Discriminator Loss: 0.5807 Generator Loss: 0.6237\nEpoch [6/30]                        Batch 800/1563 Discriminator Loss: 0.5381 Generator Loss: 1.4090\nEpoch [6/30]                        Batch 900/1563 Discriminator Loss: 0.4686 Generator Loss: 1.3971\nEpoch [6/30]                        Batch 1000/1563 Discriminator Loss: 0.4787 Generator Loss: 1.4606\nEpoch [6/30]                        Batch 1100/1563 Discriminator Loss: 0.6769 Generator Loss: 1.3275\nEpoch [6/30]                        Batch 1200/1563 Discriminator Loss: 0.4402 Generator Loss: 1.3996\nEpoch [6/30]                        Batch 1300/1563 Discriminator Loss: 0.6449 Generator Loss: 1.2970\nEpoch [6/30]                        Batch 1400/1563 Discriminator Loss: 0.7675 Generator Loss: 1.1988\nEpoch [6/30]                        Batch 1500/1563 Discriminator Loss: 0.6860 Generator Loss: 1.0752\nEpoch [7/30]                        Batch 100/1563 Discriminator Loss: 0.5656 Generator Loss: 0.5764\nEpoch [7/30]                        Batch 200/1563 Discriminator Loss: 0.4979 Generator Loss: 1.2684\nEpoch [7/30]                        Batch 300/1563 Discriminator Loss: 0.6714 Generator Loss: 0.9369\nEpoch [7/30]                        Batch 400/1563 Discriminator Loss: 0.6402 Generator Loss: 1.0803\nEpoch [7/30]                        Batch 500/1563 Discriminator Loss: 0.5717 Generator Loss: 1.3460\nEpoch [7/30]                        Batch 600/1563 Discriminator Loss: 0.4720 Generator Loss: 1.2296\nEpoch [7/30]                        Batch 700/1563 Discriminator Loss: 0.5592 Generator Loss: 1.6116\nEpoch [7/30]                        Batch 800/1563 Discriminator Loss: 0.7146 Generator Loss: 1.3487\nEpoch [7/30]                        Batch 900/1563 Discriminator Loss: 0.4830 Generator Loss: 1.3931\nEpoch [7/30]                        Batch 1000/1563 Discriminator Loss: 0.4894 Generator Loss: 1.7213\nEpoch [7/30]                        Batch 1100/1563 Discriminator Loss: 0.8574 Generator Loss: 1.3492\nEpoch [7/30]                        Batch 1200/1563 Discriminator Loss: 0.8195 Generator Loss: 0.9998\nEpoch [7/30]                        Batch 1300/1563 Discriminator Loss: 0.7399 Generator Loss: 1.3046\nEpoch [7/30]                        Batch 1400/1563 Discriminator Loss: 0.5688 Generator Loss: 0.9374\nEpoch [7/30]                        Batch 1500/1563 Discriminator Loss: 0.4341 Generator Loss: 1.2548\nEpoch [8/30]                        Batch 100/1563 Discriminator Loss: 0.6533 Generator Loss: 1.1707\nEpoch [8/30]                        Batch 200/1563 Discriminator Loss: 0.4287 Generator Loss: 1.1481\nEpoch [8/30]                        Batch 300/1563 Discriminator Loss: 0.5281 Generator Loss: 1.7373\nEpoch [8/30]                        Batch 400/1563 Discriminator Loss: 0.5967 Generator Loss: 0.9080\nEpoch [8/30]                        Batch 500/1563 Discriminator Loss: 0.6105 Generator Loss: 1.0256\nEpoch [8/30]                        Batch 600/1563 Discriminator Loss: 0.5247 Generator Loss: 1.5951\nEpoch [8/30]                        Batch 700/1563 Discriminator Loss: 0.7669 Generator Loss: 1.3100\nEpoch [8/30]                        Batch 800/1563 Discriminator Loss: 0.5892 Generator Loss: 1.1584\nEpoch [8/30]                        Batch 900/1563 Discriminator Loss: 0.8667 Generator Loss: 0.8409\nEpoch [8/30]                        Batch 1000/1563 Discriminator Loss: 0.8007 Generator Loss: 0.6958\nEpoch [8/30]                        Batch 1100/1563 Discriminator Loss: 0.6248 Generator Loss: 1.0027\nEpoch [8/30]                        Batch 1200/1563 Discriminator Loss: 0.9791 Generator Loss: 1.2779\nEpoch [8/30]                        Batch 1300/1563 Discriminator Loss: 0.6273 Generator Loss: 1.3308\nEpoch [8/30]                        Batch 1400/1563 Discriminator Loss: 0.4397 Generator Loss: 2.0570\nEpoch [8/30]                        Batch 1500/1563 Discriminator Loss: 0.5385 Generator Loss: 1.1689\nEpoch [9/30]                        Batch 100/1563 Discriminator Loss: 0.8302 Generator Loss: 1.3391\nEpoch [9/30]                        Batch 200/1563 Discriminator Loss: 0.4323 Generator Loss: 1.5036\nEpoch [9/30]                        Batch 300/1563 Discriminator Loss: 0.6001 Generator Loss: 2.3062\nEpoch [9/30]                        Batch 400/1563 Discriminator Loss: 0.5612 Generator Loss: 1.0222\nEpoch [9/30]                        Batch 500/1563 Discriminator Loss: 0.5333 Generator Loss: 1.5129\nEpoch [9/30]                        Batch 600/1563 Discriminator Loss: 0.4739 Generator Loss: 0.9228\nEpoch [9/30]                        Batch 700/1563 Discriminator Loss: 0.6237 Generator Loss: 1.3650\nEpoch [9/30]                        Batch 800/1563 Discriminator Loss: 0.6755 Generator Loss: 0.7273\nEpoch [9/30]                        Batch 900/1563 Discriminator Loss: 0.5411 Generator Loss: 1.7899\nEpoch [9/30]                        Batch 1000/1563 Discriminator Loss: 0.5930 Generator Loss: 1.0269\nEpoch [9/30]                        Batch 1100/1563 Discriminator Loss: 0.5476 Generator Loss: 1.8822\nEpoch [9/30]                        Batch 1200/1563 Discriminator Loss: 0.7660 Generator Loss: 1.4145\nEpoch [9/30]                        Batch 1300/1563 Discriminator Loss: 0.5074 Generator Loss: 1.5135\nEpoch [9/30]                        Batch 1400/1563 Discriminator Loss: 0.5582 Generator Loss: 1.1927\nEpoch [9/30]                        Batch 1500/1563 Discriminator Loss: 0.8168 Generator Loss: 1.2279\nEpoch [10/30]                        Batch 100/1563 Discriminator Loss: 0.2423 Generator Loss: 1.7337\nEpoch [10/30]                        Batch 200/1563 Discriminator Loss: 0.9663 Generator Loss: 1.0028\nEpoch [10/30]                        Batch 300/1563 Discriminator Loss: 0.5886 Generator Loss: 0.9407\nEpoch [10/30]                        Batch 400/1563 Discriminator Loss: 0.7274 Generator Loss: 1.0736\nEpoch [10/30]                        Batch 500/1563 Discriminator Loss: 0.6266 Generator Loss: 0.8596\nEpoch [10/30]                        Batch 600/1563 Discriminator Loss: 0.5225 Generator Loss: 2.0611\nEpoch [10/30]                        Batch 700/1563 Discriminator Loss: 0.4990 Generator Loss: 1.6639\nEpoch [10/30]                        Batch 800/1563 Discriminator Loss: 0.8305 Generator Loss: 0.8100\nEpoch [10/30]                        Batch 900/1563 Discriminator Loss: 0.5522 Generator Loss: 1.2785\nEpoch [10/30]                        Batch 1000/1563 Discriminator Loss: 0.8009 Generator Loss: 1.3512\nEpoch [10/30]                        Batch 1100/1563 Discriminator Loss: 0.6749 Generator Loss: 1.1727\nEpoch [10/30]                        Batch 1200/1563 Discriminator Loss: 0.8850 Generator Loss: 1.1001\nEpoch [10/30]                        Batch 1300/1563 Discriminator Loss: 0.6387 Generator Loss: 1.4558\nEpoch [10/30]                        Batch 1400/1563 Discriminator Loss: 0.6613 Generator Loss: 2.5028\nEpoch [10/30]                        Batch 1500/1563 Discriminator Loss: 0.5191 Generator Loss: 0.9412\n</code></pre> <p></p> <pre><code>Epoch [11/30]                        Batch 100/1563 Discriminator Loss: 0.6146 Generator Loss: 1.1872\nEpoch [11/30]                        Batch 200/1563 Discriminator Loss: 0.7205 Generator Loss: 1.0533\nEpoch [11/30]                        Batch 300/1563 Discriminator Loss: 0.6223 Generator Loss: 1.1095\nEpoch [11/30]                        Batch 400/1563 Discriminator Loss: 0.5828 Generator Loss: 0.7897\nEpoch [11/30]                        Batch 500/1563 Discriminator Loss: 0.5107 Generator Loss: 1.1712\nEpoch [11/30]                        Batch 600/1563 Discriminator Loss: 0.5452 Generator Loss: 1.1918\nEpoch [11/30]                        Batch 700/1563 Discriminator Loss: 0.4733 Generator Loss: 1.6043\nEpoch [11/30]                        Batch 800/1563 Discriminator Loss: 0.6569 Generator Loss: 1.1823\nEpoch [11/30]                        Batch 900/1563 Discriminator Loss: 0.7731 Generator Loss: 0.9131\nEpoch [11/30]                        Batch 1000/1563 Discriminator Loss: 0.7283 Generator Loss: 0.9966\nEpoch [11/30]                        Batch 1100/1563 Discriminator Loss: 0.6360 Generator Loss: 1.4022\nEpoch [11/30]                        Batch 1200/1563 Discriminator Loss: 0.6779 Generator Loss: 1.6380\nEpoch [11/30]                        Batch 1300/1563 Discriminator Loss: 0.5721 Generator Loss: 1.0732\nEpoch [11/30]                        Batch 1400/1563 Discriminator Loss: 0.5500 Generator Loss: 1.6293\nEpoch [11/30]                        Batch 1500/1563 Discriminator Loss: 0.6260 Generator Loss: 1.6629\nEpoch [12/30]                        Batch 100/1563 Discriminator Loss: 0.2396 Generator Loss: 1.8887\nEpoch [12/30]                        Batch 200/1563 Discriminator Loss: 0.8208 Generator Loss: 0.7347\nEpoch [12/30]                        Batch 300/1563 Discriminator Loss: 0.7865 Generator Loss: 1.2288\nEpoch [12/30]                        Batch 400/1563 Discriminator Loss: 0.8839 Generator Loss: 1.5515\nEpoch [12/30]                        Batch 500/1563 Discriminator Loss: 0.4100 Generator Loss: 1.6313\nEpoch [12/30]                        Batch 600/1563 Discriminator Loss: 0.5070 Generator Loss: 0.7579\nEpoch [12/30]                        Batch 700/1563 Discriminator Loss: 0.4470 Generator Loss: 1.4843\nEpoch [12/30]                        Batch 800/1563 Discriminator Loss: 0.3804 Generator Loss: 1.9155\nEpoch [12/30]                        Batch 900/1563 Discriminator Loss: 0.8740 Generator Loss: 0.9627\nEpoch [12/30]                        Batch 1000/1563 Discriminator Loss: 0.6253 Generator Loss: 1.2338\nEpoch [12/30]                        Batch 1100/1563 Discriminator Loss: 0.5772 Generator Loss: 0.7276\nEpoch [12/30]                        Batch 1200/1563 Discriminator Loss: 0.6984 Generator Loss: 1.2770\nEpoch [12/30]                        Batch 1300/1563 Discriminator Loss: 0.5281 Generator Loss: 1.6734\nEpoch [12/30]                        Batch 1400/1563 Discriminator Loss: 0.7011 Generator Loss: 1.5269\nEpoch [12/30]                        Batch 1500/1563 Discriminator Loss: 0.6542 Generator Loss: 1.1141\nEpoch [13/30]                        Batch 100/1563 Discriminator Loss: 0.3590 Generator Loss: 1.6407\nEpoch [13/30]                        Batch 200/1563 Discriminator Loss: 0.6046 Generator Loss: 0.6858\nEpoch [13/30]                        Batch 300/1563 Discriminator Loss: 0.5175 Generator Loss: 2.0719\nEpoch [13/30]                        Batch 400/1563 Discriminator Loss: 0.6386 Generator Loss: 1.1742\nEpoch [13/30]                        Batch 500/1563 Discriminator Loss: 0.5023 Generator Loss: 1.7407\nEpoch [13/30]                        Batch 600/1563 Discriminator Loss: 0.4565 Generator Loss: 1.8108\nEpoch [13/30]                        Batch 700/1563 Discriminator Loss: 0.3615 Generator Loss: 0.7747\nEpoch [13/30]                        Batch 800/1563 Discriminator Loss: 0.5649 Generator Loss: 1.1583\nEpoch [13/30]                        Batch 900/1563 Discriminator Loss: 0.5132 Generator Loss: 1.2286\nEpoch [13/30]                        Batch 1000/1563 Discriminator Loss: 0.6191 Generator Loss: 1.4292\nEpoch [13/30]                        Batch 1100/1563 Discriminator Loss: 0.4916 Generator Loss: 1.4518\nEpoch [13/30]                        Batch 1200/1563 Discriminator Loss: 0.5353 Generator Loss: 1.0762\nEpoch [13/30]                        Batch 1300/1563 Discriminator Loss: 0.6027 Generator Loss: 0.7657\nEpoch [13/30]                        Batch 1400/1563 Discriminator Loss: 0.6896 Generator Loss: 1.5476\nEpoch [13/30]                        Batch 1500/1563 Discriminator Loss: 0.5865 Generator Loss: 0.7674\nEpoch [14/30]                        Batch 100/1563 Discriminator Loss: 0.3891 Generator Loss: 1.4870\nEpoch [14/30]                        Batch 200/1563 Discriminator Loss: 0.4392 Generator Loss: 0.7682\nEpoch [14/30]                        Batch 300/1563 Discriminator Loss: 0.9042 Generator Loss: 0.7021\nEpoch [14/30]                        Batch 400/1563 Discriminator Loss: 0.5336 Generator Loss: 1.6118\nEpoch [14/30]                        Batch 500/1563 Discriminator Loss: 0.6186 Generator Loss: 1.0359\nEpoch [14/30]                        Batch 600/1563 Discriminator Loss: 0.7181 Generator Loss: 0.8545\nEpoch [14/30]                        Batch 700/1563 Discriminator Loss: 0.8549 Generator Loss: 0.8246\nEpoch [14/30]                        Batch 800/1563 Discriminator Loss: 0.8046 Generator Loss: 1.3709\nEpoch [14/30]                        Batch 900/1563 Discriminator Loss: 0.4450 Generator Loss: 1.2583\nEpoch [14/30]                        Batch 1000/1563 Discriminator Loss: 0.4912 Generator Loss: 1.8047\nEpoch [14/30]                        Batch 1100/1563 Discriminator Loss: 0.4890 Generator Loss: 1.1271\nEpoch [14/30]                        Batch 1200/1563 Discriminator Loss: 0.7053 Generator Loss: 1.3877\nEpoch [14/30]                        Batch 1300/1563 Discriminator Loss: 0.8480 Generator Loss: 0.9890\nEpoch [14/30]                        Batch 1400/1563 Discriminator Loss: 0.3800 Generator Loss: 1.6878\nEpoch [14/30]                        Batch 1500/1563 Discriminator Loss: 0.4641 Generator Loss: 1.5752\nEpoch [15/30]                        Batch 100/1563 Discriminator Loss: 0.6491 Generator Loss: 1.2644\nEpoch [15/30]                        Batch 200/1563 Discriminator Loss: 0.5590 Generator Loss: 0.9048\nEpoch [15/30]                        Batch 300/1563 Discriminator Loss: 0.7712 Generator Loss: 1.0408\nEpoch [15/30]                        Batch 400/1563 Discriminator Loss: 0.6446 Generator Loss: 1.2220\nEpoch [15/30]                        Batch 500/1563 Discriminator Loss: 0.5891 Generator Loss: 0.8177\nEpoch [15/30]                        Batch 600/1563 Discriminator Loss: 0.4539 Generator Loss: 1.5968\nEpoch [15/30]                        Batch 700/1563 Discriminator Loss: 0.9036 Generator Loss: 1.2203\nEpoch [15/30]                        Batch 800/1563 Discriminator Loss: 0.6573 Generator Loss: 1.2048\nEpoch [15/30]                        Batch 900/1563 Discriminator Loss: 0.5110 Generator Loss: 0.8048\nEpoch [15/30]                        Batch 1000/1563 Discriminator Loss: 0.6662 Generator Loss: 1.6221\nEpoch [15/30]                        Batch 1100/1563 Discriminator Loss: 0.4542 Generator Loss: 1.4416\nEpoch [15/30]                        Batch 1200/1563 Discriminator Loss: 0.5644 Generator Loss: 1.5006\nEpoch [15/30]                        Batch 1300/1563 Discriminator Loss: 0.5741 Generator Loss: 1.5613\nEpoch [15/30]                        Batch 1400/1563 Discriminator Loss: 0.5394 Generator Loss: 1.6310\nEpoch [15/30]                        Batch 1500/1563 Discriminator Loss: 0.2746 Generator Loss: 1.2002\nEpoch [16/30]                        Batch 100/1563 Discriminator Loss: 0.9168 Generator Loss: 1.0404\nEpoch [16/30]                        Batch 200/1563 Discriminator Loss: 0.3522 Generator Loss: 2.0820\nEpoch [16/30]                        Batch 300/1563 Discriminator Loss: 1.0174 Generator Loss: 1.2016\nEpoch [16/30]                        Batch 400/1563 Discriminator Loss: 0.9039 Generator Loss: 0.9897\nEpoch [16/30]                        Batch 500/1563 Discriminator Loss: 0.7441 Generator Loss: 1.5617\nEpoch [16/30]                        Batch 600/1563 Discriminator Loss: 0.6339 Generator Loss: 0.7029\nEpoch [16/30]                        Batch 700/1563 Discriminator Loss: 0.6705 Generator Loss: 1.2205\nEpoch [16/30]                        Batch 800/1563 Discriminator Loss: 0.4608 Generator Loss: 1.9291\nEpoch [16/30]                        Batch 900/1563 Discriminator Loss: 0.3003 Generator Loss: 2.2707\nEpoch [16/30]                        Batch 1000/1563 Discriminator Loss: 0.4935 Generator Loss: 0.9453\nEpoch [16/30]                        Batch 1100/1563 Discriminator Loss: 0.6426 Generator Loss: 2.0306\nEpoch [16/30]                        Batch 1200/1563 Discriminator Loss: 0.3256 Generator Loss: 1.9450\nEpoch [16/30]                        Batch 1300/1563 Discriminator Loss: 0.4460 Generator Loss: 1.3895\nEpoch [16/30]                        Batch 1400/1563 Discriminator Loss: 0.5697 Generator Loss: 1.8839\nEpoch [16/30]                        Batch 1500/1563 Discriminator Loss: 0.3510 Generator Loss: 1.2868\nEpoch [17/30]                        Batch 100/1563 Discriminator Loss: 0.4061 Generator Loss: 1.5765\nEpoch [17/30]                        Batch 200/1563 Discriminator Loss: 0.6059 Generator Loss: 1.4607\nEpoch [17/30]                        Batch 300/1563 Discriminator Loss: 0.3888 Generator Loss: 2.0352\nEpoch [17/30]                        Batch 400/1563 Discriminator Loss: 0.9055 Generator Loss: 1.5344\nEpoch [17/30]                        Batch 500/1563 Discriminator Loss: 0.4919 Generator Loss: 2.0243\nEpoch [17/30]                        Batch 600/1563 Discriminator Loss: 0.5129 Generator Loss: 1.3443\nEpoch [17/30]                        Batch 700/1563 Discriminator Loss: 0.5908 Generator Loss: 1.3944\nEpoch [17/30]                        Batch 800/1563 Discriminator Loss: 0.6198 Generator Loss: 1.9401\nEpoch [17/30]                        Batch 900/1563 Discriminator Loss: 0.4722 Generator Loss: 1.3389\nEpoch [17/30]                        Batch 1000/1563 Discriminator Loss: 0.6337 Generator Loss: 1.2892\nEpoch [17/30]                        Batch 1100/1563 Discriminator Loss: 0.4962 Generator Loss: 1.3870\nEpoch [17/30]                        Batch 1200/1563 Discriminator Loss: 0.4610 Generator Loss: 0.8908\nEpoch [17/30]                        Batch 1300/1563 Discriminator Loss: 0.7297 Generator Loss: 1.1273\nEpoch [17/30]                        Batch 1400/1563 Discriminator Loss: 0.7066 Generator Loss: 0.8907\nEpoch [17/30]                        Batch 1500/1563 Discriminator Loss: 0.4157 Generator Loss: 1.4886\nEpoch [18/30]                        Batch 100/1563 Discriminator Loss: 0.4349 Generator Loss: 2.3884\nEpoch [18/30]                        Batch 200/1563 Discriminator Loss: 0.6107 Generator Loss: 0.9189\nEpoch [18/30]                        Batch 300/1563 Discriminator Loss: 0.6645 Generator Loss: 0.9639\nEpoch [18/30]                        Batch 400/1563 Discriminator Loss: 0.6846 Generator Loss: 1.6454\nEpoch [18/30]                        Batch 500/1563 Discriminator Loss: 0.6662 Generator Loss: 1.3799\nEpoch [18/30]                        Batch 600/1563 Discriminator Loss: 0.3959 Generator Loss: 1.6886\nEpoch [18/30]                        Batch 700/1563 Discriminator Loss: 0.7645 Generator Loss: 0.4036\nEpoch [18/30]                        Batch 800/1563 Discriminator Loss: 0.3437 Generator Loss: 1.5305\nEpoch [18/30]                        Batch 900/1563 Discriminator Loss: 0.7294 Generator Loss: 1.9114\nEpoch [18/30]                        Batch 1000/1563 Discriminator Loss: 0.3701 Generator Loss: 2.1974\nEpoch [18/30]                        Batch 1100/1563 Discriminator Loss: 0.3689 Generator Loss: 1.6446\nEpoch [18/30]                        Batch 1200/1563 Discriminator Loss: 0.4035 Generator Loss: 1.3158\nEpoch [18/30]                        Batch 1300/1563 Discriminator Loss: 0.6356 Generator Loss: 1.4208\nEpoch [18/30]                        Batch 1400/1563 Discriminator Loss: 0.6068 Generator Loss: 1.0243\nEpoch [18/30]                        Batch 1500/1563 Discriminator Loss: 0.7674 Generator Loss: 1.6801\nEpoch [19/30]                        Batch 100/1563 Discriminator Loss: 0.4846 Generator Loss: 1.2064\nEpoch [19/30]                        Batch 200/1563 Discriminator Loss: 0.6070 Generator Loss: 1.7031\nEpoch [19/30]                        Batch 300/1563 Discriminator Loss: 0.7450 Generator Loss: 1.0996\nEpoch [19/30]                        Batch 400/1563 Discriminator Loss: 0.6458 Generator Loss: 1.0814\nEpoch [19/30]                        Batch 500/1563 Discriminator Loss: 0.4784 Generator Loss: 1.4959\nEpoch [19/30]                        Batch 600/1563 Discriminator Loss: 0.3522 Generator Loss: 1.8114\nEpoch [19/30]                        Batch 700/1563 Discriminator Loss: 0.6813 Generator Loss: 1.4926\nEpoch [19/30]                        Batch 800/1563 Discriminator Loss: 0.3082 Generator Loss: 1.2779\nEpoch [19/30]                        Batch 900/1563 Discriminator Loss: 0.4817 Generator Loss: 1.6282\nEpoch [19/30]                        Batch 1000/1563 Discriminator Loss: 0.6549 Generator Loss: 1.8198\nEpoch [19/30]                        Batch 1100/1563 Discriminator Loss: 0.5999 Generator Loss: 2.1225\nEpoch [19/30]                        Batch 1200/1563 Discriminator Loss: 0.2709 Generator Loss: 1.7114\nEpoch [19/30]                        Batch 1300/1563 Discriminator Loss: 0.5228 Generator Loss: 1.3422\nEpoch [19/30]                        Batch 1400/1563 Discriminator Loss: 0.6171 Generator Loss: 1.0001\nEpoch [19/30]                        Batch 1500/1563 Discriminator Loss: 0.5075 Generator Loss: 1.5396\nEpoch [20/30]                        Batch 100/1563 Discriminator Loss: 0.7957 Generator Loss: 1.0529\nEpoch [20/30]                        Batch 200/1563 Discriminator Loss: 0.4388 Generator Loss: 0.9466\nEpoch [20/30]                        Batch 300/1563 Discriminator Loss: 0.3565 Generator Loss: 1.0153\nEpoch [20/30]                        Batch 400/1563 Discriminator Loss: 0.5776 Generator Loss: 1.3406\nEpoch [20/30]                        Batch 500/1563 Discriminator Loss: 0.5628 Generator Loss: 1.7318\nEpoch [20/30]                        Batch 600/1563 Discriminator Loss: 0.6587 Generator Loss: 1.0502\nEpoch [20/30]                        Batch 700/1563 Discriminator Loss: 0.3938 Generator Loss: 1.5252\nEpoch [20/30]                        Batch 800/1563 Discriminator Loss: 0.5031 Generator Loss: 1.4253\nEpoch [20/30]                        Batch 900/1563 Discriminator Loss: 0.8166 Generator Loss: 0.8745\nEpoch [20/30]                        Batch 1000/1563 Discriminator Loss: 0.8261 Generator Loss: 0.7913\nEpoch [20/30]                        Batch 1100/1563 Discriminator Loss: 0.3707 Generator Loss: 1.8016\nEpoch [20/30]                        Batch 1200/1563 Discriminator Loss: 0.5905 Generator Loss: 1.0425\nEpoch [20/30]                        Batch 1300/1563 Discriminator Loss: 0.4316 Generator Loss: 1.2454\nEpoch [20/30]                        Batch 1400/1563 Discriminator Loss: 0.6296 Generator Loss: 1.1763\nEpoch [20/30]                        Batch 1500/1563 Discriminator Loss: 0.3132 Generator Loss: 1.5287\n</code></pre> <p></p> <pre><code>Epoch [21/30]                        Batch 100/1563 Discriminator Loss: 0.6704 Generator Loss: 0.9994\nEpoch [21/30]                        Batch 200/1563 Discriminator Loss: 0.4294 Generator Loss: 1.6680\nEpoch [21/30]                        Batch 300/1563 Discriminator Loss: 0.3051 Generator Loss: 1.7558\nEpoch [21/30]                        Batch 400/1563 Discriminator Loss: 0.5725 Generator Loss: 1.0703\nEpoch [21/30]                        Batch 500/1563 Discriminator Loss: 0.5754 Generator Loss: 0.8874\nEpoch [21/30]                        Batch 600/1563 Discriminator Loss: 0.5921 Generator Loss: 0.9687\nEpoch [21/30]                        Batch 700/1563 Discriminator Loss: 0.3578 Generator Loss: 2.0567\nEpoch [21/30]                        Batch 800/1563 Discriminator Loss: 0.3976 Generator Loss: 1.6124\nEpoch [21/30]                        Batch 900/1563 Discriminator Loss: 0.5874 Generator Loss: 1.4437\nEpoch [21/30]                        Batch 1000/1563 Discriminator Loss: 0.3392 Generator Loss: 1.8707\nEpoch [21/30]                        Batch 1100/1563 Discriminator Loss: 0.6126 Generator Loss: 0.8583\nEpoch [21/30]                        Batch 1200/1563 Discriminator Loss: 0.6787 Generator Loss: 1.2624\nEpoch [21/30]                        Batch 1300/1563 Discriminator Loss: 0.4979 Generator Loss: 1.1952\nEpoch [21/30]                        Batch 1400/1563 Discriminator Loss: 0.4609 Generator Loss: 1.2393\nEpoch [21/30]                        Batch 1500/1563 Discriminator Loss: 0.4389 Generator Loss: 1.0301\nEpoch [22/30]                        Batch 100/1563 Discriminator Loss: 0.5087 Generator Loss: 0.8303\nEpoch [22/30]                        Batch 200/1563 Discriminator Loss: 0.5395 Generator Loss: 1.7216\nEpoch [22/30]                        Batch 300/1563 Discriminator Loss: 0.7014 Generator Loss: 1.3354\nEpoch [22/30]                        Batch 400/1563 Discriminator Loss: 0.4920 Generator Loss: 1.2890\nEpoch [22/30]                        Batch 500/1563 Discriminator Loss: 0.6030 Generator Loss: 0.8941\nEpoch [22/30]                        Batch 600/1563 Discriminator Loss: 0.5582 Generator Loss: 0.8958\nEpoch [22/30]                        Batch 700/1563 Discriminator Loss: 0.5013 Generator Loss: 2.0577\nEpoch [22/30]                        Batch 800/1563 Discriminator Loss: 0.6041 Generator Loss: 1.3337\nEpoch [22/30]                        Batch 900/1563 Discriminator Loss: 0.5399 Generator Loss: 0.9913\nEpoch [22/30]                        Batch 1000/1563 Discriminator Loss: 0.6801 Generator Loss: 0.9158\nEpoch [22/30]                        Batch 1100/1563 Discriminator Loss: 0.4355 Generator Loss: 1.8976\nEpoch [22/30]                        Batch 1200/1563 Discriminator Loss: 0.6911 Generator Loss: 1.3129\nEpoch [22/30]                        Batch 1300/1563 Discriminator Loss: 0.5185 Generator Loss: 1.0487\nEpoch [22/30]                        Batch 1400/1563 Discriminator Loss: 0.7541 Generator Loss: 1.2654\nEpoch [22/30]                        Batch 1500/1563 Discriminator Loss: 0.4369 Generator Loss: 1.2044\nEpoch [23/30]                        Batch 100/1563 Discriminator Loss: 0.5012 Generator Loss: 0.8639\nEpoch [23/30]                        Batch 200/1563 Discriminator Loss: 0.7417 Generator Loss: 1.0816\nEpoch [23/30]                        Batch 300/1563 Discriminator Loss: 0.5031 Generator Loss: 1.6705\nEpoch [23/30]                        Batch 400/1563 Discriminator Loss: 0.7800 Generator Loss: 1.1085\nEpoch [23/30]                        Batch 500/1563 Discriminator Loss: 0.6581 Generator Loss: 0.7194\nEpoch [23/30]                        Batch 600/1563 Discriminator Loss: 0.6880 Generator Loss: 1.2136\nEpoch [23/30]                        Batch 700/1563 Discriminator Loss: 0.8604 Generator Loss: 1.2978\nEpoch [23/30]                        Batch 800/1563 Discriminator Loss: 0.4069 Generator Loss: 1.5116\nEpoch [23/30]                        Batch 900/1563 Discriminator Loss: 0.6084 Generator Loss: 1.3218\nEpoch [23/30]                        Batch 1000/1563 Discriminator Loss: 0.3370 Generator Loss: 2.3237\nEpoch [23/30]                        Batch 1100/1563 Discriminator Loss: 0.5806 Generator Loss: 1.6243\nEpoch [23/30]                        Batch 1200/1563 Discriminator Loss: 0.5700 Generator Loss: 1.3030\nEpoch [23/30]                        Batch 1300/1563 Discriminator Loss: 0.4817 Generator Loss: 1.0187\nEpoch [23/30]                        Batch 1400/1563 Discriminator Loss: 0.5350 Generator Loss: 0.9343\nEpoch [23/30]                        Batch 1500/1563 Discriminator Loss: 0.3988 Generator Loss: 1.9423\nEpoch [24/30]                        Batch 100/1563 Discriminator Loss: 0.5867 Generator Loss: 1.8228\nEpoch [24/30]                        Batch 200/1563 Discriminator Loss: 0.6380 Generator Loss: 0.7886\nEpoch [24/30]                        Batch 300/1563 Discriminator Loss: 1.0712 Generator Loss: 1.7067\nEpoch [24/30]                        Batch 400/1563 Discriminator Loss: 0.3691 Generator Loss: 1.6998\nEpoch [24/30]                        Batch 500/1563 Discriminator Loss: 0.7596 Generator Loss: 0.6284\nEpoch [24/30]                        Batch 600/1563 Discriminator Loss: 0.8459 Generator Loss: 1.5625\nEpoch [24/30]                        Batch 700/1563 Discriminator Loss: 0.4499 Generator Loss: 1.0025\nEpoch [24/30]                        Batch 800/1563 Discriminator Loss: 0.3667 Generator Loss: 1.3655\nEpoch [24/30]                        Batch 900/1563 Discriminator Loss: 0.8259 Generator Loss: 1.6259\nEpoch [24/30]                        Batch 1000/1563 Discriminator Loss: 0.8847 Generator Loss: 1.0071\nEpoch [24/30]                        Batch 1100/1563 Discriminator Loss: 0.2908 Generator Loss: 2.2473\nEpoch [24/30]                        Batch 1200/1563 Discriminator Loss: 0.8809 Generator Loss: 0.8121\nEpoch [24/30]                        Batch 1300/1563 Discriminator Loss: 0.5599 Generator Loss: 0.8316\nEpoch [24/30]                        Batch 1400/1563 Discriminator Loss: 0.5931 Generator Loss: 1.6818\nEpoch [24/30]                        Batch 1500/1563 Discriminator Loss: 0.4780 Generator Loss: 1.4393\nEpoch [25/30]                        Batch 100/1563 Discriminator Loss: 0.4862 Generator Loss: 1.0551\nEpoch [25/30]                        Batch 200/1563 Discriminator Loss: 0.7419 Generator Loss: 0.8418\nEpoch [25/30]                        Batch 300/1563 Discriminator Loss: 0.3005 Generator Loss: 1.4868\nEpoch [25/30]                        Batch 400/1563 Discriminator Loss: 0.4458 Generator Loss: 0.8691\nEpoch [25/30]                        Batch 500/1563 Discriminator Loss: 0.4591 Generator Loss: 1.3558\nEpoch [25/30]                        Batch 600/1563 Discriminator Loss: 0.2917 Generator Loss: 1.1712\nEpoch [25/30]                        Batch 700/1563 Discriminator Loss: 0.6088 Generator Loss: 1.1128\nEpoch [25/30]                        Batch 800/1563 Discriminator Loss: 0.8086 Generator Loss: 1.5543\nEpoch [25/30]                        Batch 900/1563 Discriminator Loss: 0.5244 Generator Loss: 1.4409\nEpoch [25/30]                        Batch 1000/1563 Discriminator Loss: 0.6720 Generator Loss: 1.2794\nEpoch [25/30]                        Batch 1100/1563 Discriminator Loss: 0.5955 Generator Loss: 1.0705\nEpoch [25/30]                        Batch 1200/1563 Discriminator Loss: 0.6215 Generator Loss: 1.2729\nEpoch [25/30]                        Batch 1300/1563 Discriminator Loss: 1.2661 Generator Loss: 1.4346\nEpoch [25/30]                        Batch 1400/1563 Discriminator Loss: 0.6111 Generator Loss: 1.4205\nEpoch [25/30]                        Batch 1500/1563 Discriminator Loss: 0.5564 Generator Loss: 1.5329\nEpoch [26/30]                        Batch 100/1563 Discriminator Loss: 0.7150 Generator Loss: 1.1040\nEpoch [26/30]                        Batch 200/1563 Discriminator Loss: 0.5486 Generator Loss: 0.8081\nEpoch [26/30]                        Batch 300/1563 Discriminator Loss: 0.4655 Generator Loss: 1.0543\nEpoch [26/30]                        Batch 400/1563 Discriminator Loss: 0.6280 Generator Loss: 1.3089\nEpoch [26/30]                        Batch 500/1563 Discriminator Loss: 0.6435 Generator Loss: 1.2938\nEpoch [26/30]                        Batch 600/1563 Discriminator Loss: 0.5681 Generator Loss: 0.6704\nEpoch [26/30]                        Batch 700/1563 Discriminator Loss: 0.5975 Generator Loss: 1.6594\nEpoch [26/30]                        Batch 800/1563 Discriminator Loss: 0.5214 Generator Loss: 1.5375\nEpoch [26/30]                        Batch 900/1563 Discriminator Loss: 0.7233 Generator Loss: 2.1745\nEpoch [26/30]                        Batch 1000/1563 Discriminator Loss: 0.7600 Generator Loss: 1.9514\nEpoch [26/30]                        Batch 1100/1563 Discriminator Loss: 0.4599 Generator Loss: 1.6351\nEpoch [26/30]                        Batch 1200/1563 Discriminator Loss: 0.4130 Generator Loss: 0.6645\nEpoch [26/30]                        Batch 1300/1563 Discriminator Loss: 0.7173 Generator Loss: 1.5409\nEpoch [26/30]                        Batch 1400/1563 Discriminator Loss: 0.6444 Generator Loss: 0.4363\nEpoch [26/30]                        Batch 1500/1563 Discriminator Loss: 0.8130 Generator Loss: 0.5472\nEpoch [27/30]                        Batch 100/1563 Discriminator Loss: 0.3806 Generator Loss: 2.5330\nEpoch [27/30]                        Batch 200/1563 Discriminator Loss: 0.6471 Generator Loss: 1.4548\nEpoch [27/30]                        Batch 300/1563 Discriminator Loss: 0.4945 Generator Loss: 2.0417\nEpoch [27/30]                        Batch 400/1563 Discriminator Loss: 0.8287 Generator Loss: 0.8313\nEpoch [27/30]                        Batch 500/1563 Discriminator Loss: 0.2590 Generator Loss: 1.3648\nEpoch [27/30]                        Batch 600/1563 Discriminator Loss: 0.7229 Generator Loss: 1.0506\nEpoch [27/30]                        Batch 700/1563 Discriminator Loss: 0.5257 Generator Loss: 0.6630\nEpoch [27/30]                        Batch 800/1563 Discriminator Loss: 0.7077 Generator Loss: 1.5787\nEpoch [27/30]                        Batch 900/1563 Discriminator Loss: 0.2241 Generator Loss: 2.4144\nEpoch [27/30]                        Batch 1000/1563 Discriminator Loss: 0.9249 Generator Loss: 1.0945\nEpoch [27/30]                        Batch 1100/1563 Discriminator Loss: 0.5512 Generator Loss: 1.3305\nEpoch [27/30]                        Batch 1200/1563 Discriminator Loss: 0.6963 Generator Loss: 0.6857\nEpoch [27/30]                        Batch 1300/1563 Discriminator Loss: 0.6866 Generator Loss: 1.0320\nEpoch [27/30]                        Batch 1400/1563 Discriminator Loss: 0.8760 Generator Loss: 0.5938\nEpoch [27/30]                        Batch 1500/1563 Discriminator Loss: 0.6564 Generator Loss: 1.1705\nEpoch [28/30]                        Batch 100/1563 Discriminator Loss: 0.6794 Generator Loss: 0.6070\nEpoch [28/30]                        Batch 200/1563 Discriminator Loss: 0.5660 Generator Loss: 1.3422\nEpoch [28/30]                        Batch 300/1563 Discriminator Loss: 0.4716 Generator Loss: 1.3143\nEpoch [28/30]                        Batch 400/1563 Discriminator Loss: 0.5311 Generator Loss: 1.8503\nEpoch [28/30]                        Batch 500/1563 Discriminator Loss: 0.5435 Generator Loss: 1.4696\nEpoch [28/30]                        Batch 600/1563 Discriminator Loss: 0.4778 Generator Loss: 1.1126\nEpoch [28/30]                        Batch 700/1563 Discriminator Loss: 0.4951 Generator Loss: 0.9386\nEpoch [28/30]                        Batch 800/1563 Discriminator Loss: 0.5161 Generator Loss: 1.7677\nEpoch [28/30]                        Batch 900/1563 Discriminator Loss: 0.5352 Generator Loss: 1.3093\nEpoch [28/30]                        Batch 1000/1563 Discriminator Loss: 0.4307 Generator Loss: 1.0498\nEpoch [28/30]                        Batch 1100/1563 Discriminator Loss: 0.8755 Generator Loss: 1.3385\nEpoch [28/30]                        Batch 1200/1563 Discriminator Loss: 0.5321 Generator Loss: 1.0425\nEpoch [28/30]                        Batch 1300/1563 Discriminator Loss: 0.6281 Generator Loss: 0.8529\nEpoch [28/30]                        Batch 1400/1563 Discriminator Loss: 0.5690 Generator Loss: 1.3257\nEpoch [28/30]                        Batch 1500/1563 Discriminator Loss: 1.0743 Generator Loss: 1.1915\nEpoch [29/30]                        Batch 100/1563 Discriminator Loss: 0.6799 Generator Loss: 1.0841\nEpoch [29/30]                        Batch 200/1563 Discriminator Loss: 0.6932 Generator Loss: 1.3634\nEpoch [29/30]                        Batch 300/1563 Discriminator Loss: 0.4663 Generator Loss: 1.1661\nEpoch [29/30]                        Batch 400/1563 Discriminator Loss: 0.7206 Generator Loss: 0.9654\nEpoch [29/30]                        Batch 500/1563 Discriminator Loss: 0.7813 Generator Loss: 1.4285\nEpoch [29/30]                        Batch 600/1563 Discriminator Loss: 0.8215 Generator Loss: 1.1129\nEpoch [29/30]                        Batch 700/1563 Discriminator Loss: 0.5939 Generator Loss: 1.5154\nEpoch [29/30]                        Batch 800/1563 Discriminator Loss: 0.6840 Generator Loss: 1.2734\nEpoch [29/30]                        Batch 900/1563 Discriminator Loss: 0.4068 Generator Loss: 1.2772\nEpoch [29/30]                        Batch 1000/1563 Discriminator Loss: 0.6416 Generator Loss: 0.4559\nEpoch [29/30]                        Batch 1100/1563 Discriminator Loss: 0.4972 Generator Loss: 1.6791\nEpoch [29/30]                        Batch 1200/1563 Discriminator Loss: 0.8609 Generator Loss: 1.1342\nEpoch [29/30]                        Batch 1300/1563 Discriminator Loss: 0.6266 Generator Loss: 0.8422\nEpoch [29/30]                        Batch 1400/1563 Discriminator Loss: 0.5542 Generator Loss: 1.1366\nEpoch [29/30]                        Batch 1500/1563 Discriminator Loss: 0.5414 Generator Loss: 1.0412\nEpoch [30/30]                        Batch 100/1563 Discriminator Loss: 1.0110 Generator Loss: 0.9997\nEpoch [30/30]                        Batch 200/1563 Discriminator Loss: 0.4734 Generator Loss: 1.2250\nEpoch [30/30]                        Batch 300/1563 Discriminator Loss: 0.3835 Generator Loss: 1.3170\nEpoch [30/30]                        Batch 400/1563 Discriminator Loss: 0.3179 Generator Loss: 2.2588\nEpoch [30/30]                        Batch 500/1563 Discriminator Loss: 0.3329 Generator Loss: 1.5280\nEpoch [30/30]                        Batch 600/1563 Discriminator Loss: 0.3452 Generator Loss: 0.8436\nEpoch [30/30]                        Batch 700/1563 Discriminator Loss: 0.8666 Generator Loss: 0.6735\nEpoch [30/30]                        Batch 800/1563 Discriminator Loss: 0.6920 Generator Loss: 1.8045\nEpoch [30/30]                        Batch 900/1563 Discriminator Loss: 0.6625 Generator Loss: 0.8388\nEpoch [30/30]                        Batch 1000/1563 Discriminator Loss: 0.7458 Generator Loss: 2.1390\nEpoch [30/30]                        Batch 1100/1563 Discriminator Loss: 1.4766 Generator Loss: 1.7256\nEpoch [30/30]                        Batch 1200/1563 Discriminator Loss: 0.4399 Generator Loss: 1.4570\nEpoch [30/30]                        Batch 1300/1563 Discriminator Loss: 0.4310 Generator Loss: 1.7740\nEpoch [30/30]                        Batch 1400/1563 Discriminator Loss: 0.4642 Generator Loss: 1.1918\nEpoch [30/30]                        Batch 1500/1563 Discriminator Loss: 0.5108 Generator Loss: 1.7462\n</code></pre> <p></p> Architecture Visualization GeneratorDiscriminator <p></p> <p></p>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/", "title": "Gan example cifar 10", "text": "<p>Example extracted from https://www.geeksforgeeks.org/deep-learning/generative-adversarial-network-gan/</p>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#1-importing-libraries", "title": "1. Importing Libraries", "text": "<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#2-defining-image-transformations", "title": "2. Defining Image Transformations", "text": "<pre><code>transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#3-loaging-the-cifar-10-dataset", "title": "3. Loaging the CIFAR-10 Dataset", "text": "<pre><code>train_dataset = datasets.CIFAR10(root='./data',\\\n              train=True, download=True, transform=transform)\ndataloader = torch.utils.data.DataLoader(train_dataset, \\\n                                batch_size=32, shuffle=True)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:16&lt;00:00, 10.5MB/s]\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#4-defining-gan-hyperparameters", "title": "4. Defining GAN Hyperparameters", "text": "<p>Set important training parameters:</p> <ul> <li>latent_dim: Dimensionality of the noise vector.</li> <li>lr: Learning rate of the optimizer.</li> <li>beta1, beta2: Beta parameters for Adam optimizer (e.g 0.5, 0.999)</li> <li>num_epochs: Number of times the entire dataset will be processed (e.g 10)</li> </ul> <pre><code>latent_dim = 100\nlr = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\nnum_epochs = 30\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#5-building-the-generator", "title": "5. Building the Generator", "text": "<p>Create a neural network that converts random noise into images. Use transpose convolutional layers, batch normalization and ReLU activations. The final layer uses Tanh activation to scale outputs to the range [-1, 1].</p> <ul> <li>nn.Linear(latent_dim, 128 * 8 * 8): Defines a fully connected layer that projects the noise vector into a higher dimensional feature space.</li> <li>nn.Upsample(scale_factor=2): Doubles the spatial resolution of the feature maps by upsampling.</li> <li>nn.Conv2d(128, 128, kernel_size=3, padding=1): Applies a convolutional layer keeping the number of channels the same to refine features.</li> </ul> <pre><code>class Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 128 * 8 * 8),\n            nn.ReLU(),\n            nn.Unflatten(1, (128, 8, 8)),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128, momentum=0.78),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64, momentum=0.78),\n            nn.ReLU(),\n            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        return img\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#6-building-the-discriminator", "title": "6. Building the Discriminator", "text": "<p>Create a binary classifier network that distinguishes real from fake images. Use convolutional layers, batch normalization, dropout, LeakyReLU activation and a Sigmoid output layer to give a probability between 0 and 1.</p> <ul> <li>nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1): Second convolutional layer increasing channels to 64, downsampling further.</li> <li>nn.BatchNorm2d(256, momentum=0.8): Batch normalization for 256 feature maps with momentum 0.8.</li> </ul> <pre><code>class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n        nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n        nn.LeakyReLU(0.2),\n        nn.Dropout(0.25),\n        nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n        nn.ZeroPad2d((0, 1, 0, 1)),\n        nn.BatchNorm2d(64, momentum=0.82),\n        nn.LeakyReLU(0.25),\n        nn.Dropout(0.25),\n        nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n        nn.BatchNorm2d(128, momentum=0.82),\n        nn.LeakyReLU(0.2),\n        nn.Dropout(0.25),\n        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(256, momentum=0.8),\n        nn.LeakyReLU(0.25),\n        nn.Dropout(0.25),\n        nn.Flatten(),\n        nn.Linear(256 * 5 * 5, 1),\n        nn.Sigmoid()\n    )\n\n    def forward(self, img):\n        validity = self.model(img)\n        return validity\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#7-initializing", "title": "7. Initializing", "text": "<ul> <li>Generator and Discriminator are initialized on the available device (GPU or CPU).</li> <li>Binary Cross-Entropy (BCE) Loss is chosen as the loss function.</li> <li>Adam optimizers are defined separately for the generator and discriminator with specified learning rates and betas.</li> </ul> <pre><code>generator = Generator(latent_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\nadversarial_loss = nn.BCELoss()\n\noptimizer_G = optim.Adam(generator.parameters()\\\n                         , lr=lr, betas=(beta1, beta2))\noptimizer_D = optim.Adam(discriminator.parameters()\\\n                         , lr=lr, betas=(beta1, beta2))\n</code></pre>"}, {"location": "classes/generative-adversarial-networks/gan_example_cifar_10/#8-training", "title": "8. Training", "text": "<p>Train the discriminator on real and fake images, then update the generator to improve its fake image quality. Track losses and visualize generated images after each epoch.</p> <ul> <li>valid = torch.ones(real_images.size(0), 1, device=device): Create a tensor of ones representing real labels for the discriminator.</li> <li>fake = torch.zeros(real_images.size(0), 1, device=device): Create a tensor of zeros representing fake labels for the discriminator.</li> <li>z = torch.randn(real_images.size(0), latent_dim, device=device): Generate random noise vectors as input for the generator.</li> <li>g_loss = adversarial_loss(discriminator(gen_images), valid): Calculate generator loss based on the discriminator classifying fake images as real.</li> <li>grid = torchvision.utils.make_grid(generated, nrow=4, normalize=True): Arrange generated images into a grid for display, normalizing pixel values.</li> </ul> <pre><code>for epoch in range(num_epochs):\n    for i, batch in enumerate(dataloader):\n\n        real_images = batch[0].to(device)\n\n        # Create a tensor of ones representing real labels for the discriminator.\n        valid = torch.ones(real_images.size(0), 1, device=device)\n        # Create a tensor of zeros representing fake labels for the discriminator.\n        fake = torch.zeros(real_images.size(0), 1, device=device)\n\n        real_images = real_images.to(device)\n\n        optimizer_D.zero_grad()\n\n        # Generate random noise vectors as input for the generator.\n        z = torch.randn(real_images.size(0), latent_dim, device=device)\n\n        fake_images = generator(z)\n\n        real_loss = adversarial_loss(discriminator\\\n                                     (real_images), valid)\n        fake_loss = adversarial_loss(discriminator\\\n                                     (fake_images.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        optimizer_G.zero_grad()\n\n        gen_images = generator(z)\n\n        # Calculate generator loss based on the discriminator classifying fake images as real.\n        g_loss = adversarial_loss(discriminator(gen_images), valid)\n        g_loss.backward()\n        optimizer_G.step()\n\n        if (i + 1) % 100 == 0:\n            print(\n                f\"Epoch [{epoch+1}/{num_epochs}]\\\n                        Batch {i+1}/{len(dataloader)} \"\n                f\"Discriminator Loss: {d_loss.item():.4f} \"\n                f\"Generator Loss: {g_loss.item():.4f}\"\n            )\n    if (epoch + 1) % 10 == 0:\n        with torch.no_grad():\n            z = torch.randn(16, latent_dim, device=device)\n            generated = generator(z).detach().cpu()\n            # Arrange generated images into a grid for display, normalizing pixel values.\n            grid = torchvision.utils.make_grid(generated,\\\n                                        nrow=4, normalize=True)\n            plt.imshow(np.transpose(grid, (1, 2, 0)))\n            plt.axis(\"off\")\n            plt.show()\n\nplt.close()\n</code></pre> <pre><code>Epoch [1/30]                        Batch 100/1563 Discriminator Loss: 0.6310 Generator Loss: 1.0856\nEpoch [1/30]                        Batch 200/1563 Discriminator Loss: 0.7243 Generator Loss: 1.0722\nEpoch [1/30]                        Batch 300/1563 Discriminator Loss: 0.7337 Generator Loss: 1.1005\nEpoch [1/30]                        Batch 400/1563 Discriminator Loss: 0.6917 Generator Loss: 1.1717\nEpoch [1/30]                        Batch 500/1563 Discriminator Loss: 0.5310 Generator Loss: 1.1474\nEpoch [1/30]                        Batch 600/1563 Discriminator Loss: 0.7355 Generator Loss: 0.8708\nEpoch [1/30]                        Batch 700/1563 Discriminator Loss: 0.6745 Generator Loss: 1.2315\nEpoch [1/30]                        Batch 800/1563 Discriminator Loss: 0.7858 Generator Loss: 0.8245\nEpoch [1/30]                        Batch 900/1563 Discriminator Loss: 0.6330 Generator Loss: 0.9612\nEpoch [1/30]                        Batch 1000/1563 Discriminator Loss: 0.6853 Generator Loss: 1.1186\nEpoch [1/30]                        Batch 1100/1563 Discriminator Loss: 0.5872 Generator Loss: 1.3537\nEpoch [1/30]                        Batch 1200/1563 Discriminator Loss: 0.6135 Generator Loss: 1.0075\nEpoch [1/30]                        Batch 1300/1563 Discriminator Loss: 0.5315 Generator Loss: 1.4053\nEpoch [1/30]                        Batch 1400/1563 Discriminator Loss: 0.7756 Generator Loss: 0.8813\nEpoch [1/30]                        Batch 1500/1563 Discriminator Loss: 0.6498 Generator Loss: 0.8439\nEpoch [2/30]                        Batch 100/1563 Discriminator Loss: 0.6724 Generator Loss: 0.7996\nEpoch [2/30]                        Batch 200/1563 Discriminator Loss: 0.8157 Generator Loss: 0.7308\nEpoch [2/30]                        Batch 300/1563 Discriminator Loss: 0.5542 Generator Loss: 1.0256\nEpoch [2/30]                        Batch 400/1563 Discriminator Loss: 0.5977 Generator Loss: 1.1029\nEpoch [2/30]                        Batch 500/1563 Discriminator Loss: 0.6931 Generator Loss: 0.8717\nEpoch [2/30]                        Batch 600/1563 Discriminator Loss: 0.5693 Generator Loss: 1.3365\nEpoch [2/30]                        Batch 700/1563 Discriminator Loss: 0.6488 Generator Loss: 0.9503\nEpoch [2/30]                        Batch 800/1563 Discriminator Loss: 0.6761 Generator Loss: 0.8909\nEpoch [2/30]                        Batch 900/1563 Discriminator Loss: 0.5708 Generator Loss: 0.9972\nEpoch [2/30]                        Batch 1000/1563 Discriminator Loss: 0.6001 Generator Loss: 1.1221\nEpoch [2/30]                        Batch 1100/1563 Discriminator Loss: 0.6978 Generator Loss: 0.8663\nEpoch [2/30]                        Batch 1200/1563 Discriminator Loss: 0.7402 Generator Loss: 1.1249\nEpoch [2/30]                        Batch 1300/1563 Discriminator Loss: 0.6372 Generator Loss: 1.0888\nEpoch [2/30]                        Batch 1400/1563 Discriminator Loss: 0.6317 Generator Loss: 0.9584\nEpoch [2/30]                        Batch 1500/1563 Discriminator Loss: 0.5677 Generator Loss: 1.0163\nEpoch [3/30]                        Batch 100/1563 Discriminator Loss: 0.6370 Generator Loss: 1.3195\nEpoch [3/30]                        Batch 200/1563 Discriminator Loss: 0.5646 Generator Loss: 1.3755\nEpoch [3/30]                        Batch 300/1563 Discriminator Loss: 0.7698 Generator Loss: 0.8023\nEpoch [3/30]                        Batch 400/1563 Discriminator Loss: 0.5839 Generator Loss: 1.0319\nEpoch [3/30]                        Batch 500/1563 Discriminator Loss: 0.5613 Generator Loss: 1.2391\nEpoch [3/30]                        Batch 600/1563 Discriminator Loss: 0.5798 Generator Loss: 1.1867\nEpoch [3/30]                        Batch 700/1563 Discriminator Loss: 0.6622 Generator Loss: 1.4665\nEpoch [3/30]                        Batch 800/1563 Discriminator Loss: 0.7389 Generator Loss: 1.3259\nEpoch [3/30]                        Batch 900/1563 Discriminator Loss: 0.6855 Generator Loss: 1.4243\nEpoch [3/30]                        Batch 1000/1563 Discriminator Loss: 0.5348 Generator Loss: 1.5141\nEpoch [3/30]                        Batch 1100/1563 Discriminator Loss: 0.4696 Generator Loss: 1.3948\nEpoch [3/30]                        Batch 1200/1563 Discriminator Loss: 0.5467 Generator Loss: 0.6435\nEpoch [3/30]                        Batch 1300/1563 Discriminator Loss: 0.6741 Generator Loss: 0.9211\nEpoch [3/30]                        Batch 1400/1563 Discriminator Loss: 0.4664 Generator Loss: 1.0165\nEpoch [3/30]                        Batch 1500/1563 Discriminator Loss: 0.4799 Generator Loss: 0.9388\nEpoch [4/30]                        Batch 100/1563 Discriminator Loss: 0.6887 Generator Loss: 0.9108\nEpoch [4/30]                        Batch 200/1563 Discriminator Loss: 0.7852 Generator Loss: 0.8538\nEpoch [4/30]                        Batch 300/1563 Discriminator Loss: 0.5036 Generator Loss: 0.7702\nEpoch [4/30]                        Batch 400/1563 Discriminator Loss: 0.4476 Generator Loss: 1.3409\nEpoch [4/30]                        Batch 500/1563 Discriminator Loss: 0.6550 Generator Loss: 1.2383\nEpoch [4/30]                        Batch 600/1563 Discriminator Loss: 0.7891 Generator Loss: 1.6518\nEpoch [4/30]                        Batch 700/1563 Discriminator Loss: 0.5458 Generator Loss: 1.3395\nEpoch [4/30]                        Batch 800/1563 Discriminator Loss: 0.6637 Generator Loss: 0.9561\nEpoch [4/30]                        Batch 900/1563 Discriminator Loss: 0.6408 Generator Loss: 1.4673\nEpoch [4/30]                        Batch 1000/1563 Discriminator Loss: 0.5879 Generator Loss: 0.8866\nEpoch [4/30]                        Batch 1100/1563 Discriminator Loss: 0.5762 Generator Loss: 1.2028\nEpoch [4/30]                        Batch 1200/1563 Discriminator Loss: 0.4307 Generator Loss: 1.8184\nEpoch [4/30]                        Batch 1300/1563 Discriminator Loss: 0.8079 Generator Loss: 0.7371\nEpoch [4/30]                        Batch 1400/1563 Discriminator Loss: 0.5739 Generator Loss: 0.9877\nEpoch [4/30]                        Batch 1500/1563 Discriminator Loss: 0.5317 Generator Loss: 1.7946\nEpoch [5/30]                        Batch 100/1563 Discriminator Loss: 0.4036 Generator Loss: 1.1946\nEpoch [5/30]                        Batch 200/1563 Discriminator Loss: 0.4312 Generator Loss: 1.4645\nEpoch [5/30]                        Batch 300/1563 Discriminator Loss: 0.4584 Generator Loss: 1.0263\nEpoch [5/30]                        Batch 400/1563 Discriminator Loss: 0.5096 Generator Loss: 1.4634\nEpoch [5/30]                        Batch 500/1563 Discriminator Loss: 0.6048 Generator Loss: 1.7682\nEpoch [5/30]                        Batch 600/1563 Discriminator Loss: 0.7375 Generator Loss: 1.0852\nEpoch [5/30]                        Batch 700/1563 Discriminator Loss: 0.6004 Generator Loss: 1.3448\nEpoch [5/30]                        Batch 800/1563 Discriminator Loss: 0.5066 Generator Loss: 1.0084\nEpoch [5/30]                        Batch 900/1563 Discriminator Loss: 0.6089 Generator Loss: 1.0463\nEpoch [5/30]                        Batch 1000/1563 Discriminator Loss: 0.4824 Generator Loss: 1.2175\nEpoch [5/30]                        Batch 1100/1563 Discriminator Loss: 0.7544 Generator Loss: 0.8785\nEpoch [5/30]                        Batch 1200/1563 Discriminator Loss: 0.4876 Generator Loss: 1.2693\nEpoch [5/30]                        Batch 1300/1563 Discriminator Loss: 0.4419 Generator Loss: 0.9601\nEpoch [5/30]                        Batch 1400/1563 Discriminator Loss: 0.5805 Generator Loss: 1.0689\nEpoch [5/30]                        Batch 1500/1563 Discriminator Loss: 0.5700 Generator Loss: 1.5043\nEpoch [6/30]                        Batch 100/1563 Discriminator Loss: 0.3690 Generator Loss: 1.8759\nEpoch [6/30]                        Batch 200/1563 Discriminator Loss: 0.4619 Generator Loss: 1.5484\nEpoch [6/30]                        Batch 300/1563 Discriminator Loss: 0.5607 Generator Loss: 1.1555\nEpoch [6/30]                        Batch 400/1563 Discriminator Loss: 0.3786 Generator Loss: 1.5910\nEpoch [6/30]                        Batch 500/1563 Discriminator Loss: 0.6497 Generator Loss: 0.9485\nEpoch [6/30]                        Batch 600/1563 Discriminator Loss: 0.4534 Generator Loss: 1.4235\nEpoch [6/30]                        Batch 700/1563 Discriminator Loss: 0.5807 Generator Loss: 0.6237\nEpoch [6/30]                        Batch 800/1563 Discriminator Loss: 0.5381 Generator Loss: 1.4090\nEpoch [6/30]                        Batch 900/1563 Discriminator Loss: 0.4686 Generator Loss: 1.3971\nEpoch [6/30]                        Batch 1000/1563 Discriminator Loss: 0.4787 Generator Loss: 1.4606\nEpoch [6/30]                        Batch 1100/1563 Discriminator Loss: 0.6769 Generator Loss: 1.3275\nEpoch [6/30]                        Batch 1200/1563 Discriminator Loss: 0.4402 Generator Loss: 1.3996\nEpoch [6/30]                        Batch 1300/1563 Discriminator Loss: 0.6449 Generator Loss: 1.2970\nEpoch [6/30]                        Batch 1400/1563 Discriminator Loss: 0.7675 Generator Loss: 1.1988\nEpoch [6/30]                        Batch 1500/1563 Discriminator Loss: 0.6860 Generator Loss: 1.0752\nEpoch [7/30]                        Batch 100/1563 Discriminator Loss: 0.5656 Generator Loss: 0.5764\nEpoch [7/30]                        Batch 200/1563 Discriminator Loss: 0.4979 Generator Loss: 1.2684\nEpoch [7/30]                        Batch 300/1563 Discriminator Loss: 0.6714 Generator Loss: 0.9369\nEpoch [7/30]                        Batch 400/1563 Discriminator Loss: 0.6402 Generator Loss: 1.0803\nEpoch [7/30]                        Batch 500/1563 Discriminator Loss: 0.5717 Generator Loss: 1.3460\nEpoch [7/30]                        Batch 600/1563 Discriminator Loss: 0.4720 Generator Loss: 1.2296\nEpoch [7/30]                        Batch 700/1563 Discriminator Loss: 0.5592 Generator Loss: 1.6116\nEpoch [7/30]                        Batch 800/1563 Discriminator Loss: 0.7146 Generator Loss: 1.3487\nEpoch [7/30]                        Batch 900/1563 Discriminator Loss: 0.4830 Generator Loss: 1.3931\nEpoch [7/30]                        Batch 1000/1563 Discriminator Loss: 0.4894 Generator Loss: 1.7213\nEpoch [7/30]                        Batch 1100/1563 Discriminator Loss: 0.8574 Generator Loss: 1.3492\nEpoch [7/30]                        Batch 1200/1563 Discriminator Loss: 0.8195 Generator Loss: 0.9998\nEpoch [7/30]                        Batch 1300/1563 Discriminator Loss: 0.7399 Generator Loss: 1.3046\nEpoch [7/30]                        Batch 1400/1563 Discriminator Loss: 0.5688 Generator Loss: 0.9374\nEpoch [7/30]                        Batch 1500/1563 Discriminator Loss: 0.4341 Generator Loss: 1.2548\nEpoch [8/30]                        Batch 100/1563 Discriminator Loss: 0.6533 Generator Loss: 1.1707\nEpoch [8/30]                        Batch 200/1563 Discriminator Loss: 0.4287 Generator Loss: 1.1481\nEpoch [8/30]                        Batch 300/1563 Discriminator Loss: 0.5281 Generator Loss: 1.7373\nEpoch [8/30]                        Batch 400/1563 Discriminator Loss: 0.5967 Generator Loss: 0.9080\nEpoch [8/30]                        Batch 500/1563 Discriminator Loss: 0.6105 Generator Loss: 1.0256\nEpoch [8/30]                        Batch 600/1563 Discriminator Loss: 0.5247 Generator Loss: 1.5951\nEpoch [8/30]                        Batch 700/1563 Discriminator Loss: 0.7669 Generator Loss: 1.3100\nEpoch [8/30]                        Batch 800/1563 Discriminator Loss: 0.5892 Generator Loss: 1.1584\nEpoch [8/30]                        Batch 900/1563 Discriminator Loss: 0.8667 Generator Loss: 0.8409\nEpoch [8/30]                        Batch 1000/1563 Discriminator Loss: 0.8007 Generator Loss: 0.6958\nEpoch [8/30]                        Batch 1100/1563 Discriminator Loss: 0.6248 Generator Loss: 1.0027\nEpoch [8/30]                        Batch 1200/1563 Discriminator Loss: 0.9791 Generator Loss: 1.2779\nEpoch [8/30]                        Batch 1300/1563 Discriminator Loss: 0.6273 Generator Loss: 1.3308\nEpoch [8/30]                        Batch 1400/1563 Discriminator Loss: 0.4397 Generator Loss: 2.0570\nEpoch [8/30]                        Batch 1500/1563 Discriminator Loss: 0.5385 Generator Loss: 1.1689\nEpoch [9/30]                        Batch 100/1563 Discriminator Loss: 0.8302 Generator Loss: 1.3391\nEpoch [9/30]                        Batch 200/1563 Discriminator Loss: 0.4323 Generator Loss: 1.5036\nEpoch [9/30]                        Batch 300/1563 Discriminator Loss: 0.6001 Generator Loss: 2.3062\nEpoch [9/30]                        Batch 400/1563 Discriminator Loss: 0.5612 Generator Loss: 1.0222\nEpoch [9/30]                        Batch 500/1563 Discriminator Loss: 0.5333 Generator Loss: 1.5129\nEpoch [9/30]                        Batch 600/1563 Discriminator Loss: 0.4739 Generator Loss: 0.9228\nEpoch [9/30]                        Batch 700/1563 Discriminator Loss: 0.6237 Generator Loss: 1.3650\nEpoch [9/30]                        Batch 800/1563 Discriminator Loss: 0.6755 Generator Loss: 0.7273\nEpoch [9/30]                        Batch 900/1563 Discriminator Loss: 0.5411 Generator Loss: 1.7899\nEpoch [9/30]                        Batch 1000/1563 Discriminator Loss: 0.5930 Generator Loss: 1.0269\nEpoch [9/30]                        Batch 1100/1563 Discriminator Loss: 0.5476 Generator Loss: 1.8822\nEpoch [9/30]                        Batch 1200/1563 Discriminator Loss: 0.7660 Generator Loss: 1.4145\nEpoch [9/30]                        Batch 1300/1563 Discriminator Loss: 0.5074 Generator Loss: 1.5135\nEpoch [9/30]                        Batch 1400/1563 Discriminator Loss: 0.5582 Generator Loss: 1.1927\nEpoch [9/30]                        Batch 1500/1563 Discriminator Loss: 0.8168 Generator Loss: 1.2279\nEpoch [10/30]                        Batch 100/1563 Discriminator Loss: 0.2423 Generator Loss: 1.7337\nEpoch [10/30]                        Batch 200/1563 Discriminator Loss: 0.9663 Generator Loss: 1.0028\nEpoch [10/30]                        Batch 300/1563 Discriminator Loss: 0.5886 Generator Loss: 0.9407\nEpoch [10/30]                        Batch 400/1563 Discriminator Loss: 0.7274 Generator Loss: 1.0736\nEpoch [10/30]                        Batch 500/1563 Discriminator Loss: 0.6266 Generator Loss: 0.8596\nEpoch [10/30]                        Batch 600/1563 Discriminator Loss: 0.5225 Generator Loss: 2.0611\nEpoch [10/30]                        Batch 700/1563 Discriminator Loss: 0.4990 Generator Loss: 1.6639\nEpoch [10/30]                        Batch 800/1563 Discriminator Loss: 0.8305 Generator Loss: 0.8100\nEpoch [10/30]                        Batch 900/1563 Discriminator Loss: 0.5522 Generator Loss: 1.2785\nEpoch [10/30]                        Batch 1000/1563 Discriminator Loss: 0.8009 Generator Loss: 1.3512\nEpoch [10/30]                        Batch 1100/1563 Discriminator Loss: 0.6749 Generator Loss: 1.1727\nEpoch [10/30]                        Batch 1200/1563 Discriminator Loss: 0.8850 Generator Loss: 1.1001\nEpoch [10/30]                        Batch 1300/1563 Discriminator Loss: 0.6387 Generator Loss: 1.4558\nEpoch [10/30]                        Batch 1400/1563 Discriminator Loss: 0.6613 Generator Loss: 2.5028\nEpoch [10/30]                        Batch 1500/1563 Discriminator Loss: 0.5191 Generator Loss: 0.9412\n</code></pre> <p></p> <pre><code>Epoch [11/30]                        Batch 100/1563 Discriminator Loss: 0.6146 Generator Loss: 1.1872\nEpoch [11/30]                        Batch 200/1563 Discriminator Loss: 0.7205 Generator Loss: 1.0533\nEpoch [11/30]                        Batch 300/1563 Discriminator Loss: 0.6223 Generator Loss: 1.1095\nEpoch [11/30]                        Batch 400/1563 Discriminator Loss: 0.5828 Generator Loss: 0.7897\nEpoch [11/30]                        Batch 500/1563 Discriminator Loss: 0.5107 Generator Loss: 1.1712\nEpoch [11/30]                        Batch 600/1563 Discriminator Loss: 0.5452 Generator Loss: 1.1918\nEpoch [11/30]                        Batch 700/1563 Discriminator Loss: 0.4733 Generator Loss: 1.6043\nEpoch [11/30]                        Batch 800/1563 Discriminator Loss: 0.6569 Generator Loss: 1.1823\nEpoch [11/30]                        Batch 900/1563 Discriminator Loss: 0.7731 Generator Loss: 0.9131\nEpoch [11/30]                        Batch 1000/1563 Discriminator Loss: 0.7283 Generator Loss: 0.9966\nEpoch [11/30]                        Batch 1100/1563 Discriminator Loss: 0.6360 Generator Loss: 1.4022\nEpoch [11/30]                        Batch 1200/1563 Discriminator Loss: 0.6779 Generator Loss: 1.6380\nEpoch [11/30]                        Batch 1300/1563 Discriminator Loss: 0.5721 Generator Loss: 1.0732\nEpoch [11/30]                        Batch 1400/1563 Discriminator Loss: 0.5500 Generator Loss: 1.6293\nEpoch [11/30]                        Batch 1500/1563 Discriminator Loss: 0.6260 Generator Loss: 1.6629\nEpoch [12/30]                        Batch 100/1563 Discriminator Loss: 0.2396 Generator Loss: 1.8887\nEpoch [12/30]                        Batch 200/1563 Discriminator Loss: 0.8208 Generator Loss: 0.7347\nEpoch [12/30]                        Batch 300/1563 Discriminator Loss: 0.7865 Generator Loss: 1.2288\nEpoch [12/30]                        Batch 400/1563 Discriminator Loss: 0.8839 Generator Loss: 1.5515\nEpoch [12/30]                        Batch 500/1563 Discriminator Loss: 0.4100 Generator Loss: 1.6313\nEpoch [12/30]                        Batch 600/1563 Discriminator Loss: 0.5070 Generator Loss: 0.7579\nEpoch [12/30]                        Batch 700/1563 Discriminator Loss: 0.4470 Generator Loss: 1.4843\nEpoch [12/30]                        Batch 800/1563 Discriminator Loss: 0.3804 Generator Loss: 1.9155\nEpoch [12/30]                        Batch 900/1563 Discriminator Loss: 0.8740 Generator Loss: 0.9627\nEpoch [12/30]                        Batch 1000/1563 Discriminator Loss: 0.6253 Generator Loss: 1.2338\nEpoch [12/30]                        Batch 1100/1563 Discriminator Loss: 0.5772 Generator Loss: 0.7276\nEpoch [12/30]                        Batch 1200/1563 Discriminator Loss: 0.6984 Generator Loss: 1.2770\nEpoch [12/30]                        Batch 1300/1563 Discriminator Loss: 0.5281 Generator Loss: 1.6734\nEpoch [12/30]                        Batch 1400/1563 Discriminator Loss: 0.7011 Generator Loss: 1.5269\nEpoch [12/30]                        Batch 1500/1563 Discriminator Loss: 0.6542 Generator Loss: 1.1141\nEpoch [13/30]                        Batch 100/1563 Discriminator Loss: 0.3590 Generator Loss: 1.6407\nEpoch [13/30]                        Batch 200/1563 Discriminator Loss: 0.6046 Generator Loss: 0.6858\nEpoch [13/30]                        Batch 300/1563 Discriminator Loss: 0.5175 Generator Loss: 2.0719\nEpoch [13/30]                        Batch 400/1563 Discriminator Loss: 0.6386 Generator Loss: 1.1742\nEpoch [13/30]                        Batch 500/1563 Discriminator Loss: 0.5023 Generator Loss: 1.7407\nEpoch [13/30]                        Batch 600/1563 Discriminator Loss: 0.4565 Generator Loss: 1.8108\nEpoch [13/30]                        Batch 700/1563 Discriminator Loss: 0.3615 Generator Loss: 0.7747\nEpoch [13/30]                        Batch 800/1563 Discriminator Loss: 0.5649 Generator Loss: 1.1583\nEpoch [13/30]                        Batch 900/1563 Discriminator Loss: 0.5132 Generator Loss: 1.2286\nEpoch [13/30]                        Batch 1000/1563 Discriminator Loss: 0.6191 Generator Loss: 1.4292\nEpoch [13/30]                        Batch 1100/1563 Discriminator Loss: 0.4916 Generator Loss: 1.4518\nEpoch [13/30]                        Batch 1200/1563 Discriminator Loss: 0.5353 Generator Loss: 1.0762\nEpoch [13/30]                        Batch 1300/1563 Discriminator Loss: 0.6027 Generator Loss: 0.7657\nEpoch [13/30]                        Batch 1400/1563 Discriminator Loss: 0.6896 Generator Loss: 1.5476\nEpoch [13/30]                        Batch 1500/1563 Discriminator Loss: 0.5865 Generator Loss: 0.7674\nEpoch [14/30]                        Batch 100/1563 Discriminator Loss: 0.3891 Generator Loss: 1.4870\nEpoch [14/30]                        Batch 200/1563 Discriminator Loss: 0.4392 Generator Loss: 0.7682\nEpoch [14/30]                        Batch 300/1563 Discriminator Loss: 0.9042 Generator Loss: 0.7021\nEpoch [14/30]                        Batch 400/1563 Discriminator Loss: 0.5336 Generator Loss: 1.6118\nEpoch [14/30]                        Batch 500/1563 Discriminator Loss: 0.6186 Generator Loss: 1.0359\nEpoch [14/30]                        Batch 600/1563 Discriminator Loss: 0.7181 Generator Loss: 0.8545\nEpoch [14/30]                        Batch 700/1563 Discriminator Loss: 0.8549 Generator Loss: 0.8246\nEpoch [14/30]                        Batch 800/1563 Discriminator Loss: 0.8046 Generator Loss: 1.3709\nEpoch [14/30]                        Batch 900/1563 Discriminator Loss: 0.4450 Generator Loss: 1.2583\nEpoch [14/30]                        Batch 1000/1563 Discriminator Loss: 0.4912 Generator Loss: 1.8047\nEpoch [14/30]                        Batch 1100/1563 Discriminator Loss: 0.4890 Generator Loss: 1.1271\nEpoch [14/30]                        Batch 1200/1563 Discriminator Loss: 0.7053 Generator Loss: 1.3877\nEpoch [14/30]                        Batch 1300/1563 Discriminator Loss: 0.8480 Generator Loss: 0.9890\nEpoch [14/30]                        Batch 1400/1563 Discriminator Loss: 0.3800 Generator Loss: 1.6878\nEpoch [14/30]                        Batch 1500/1563 Discriminator Loss: 0.4641 Generator Loss: 1.5752\nEpoch [15/30]                        Batch 100/1563 Discriminator Loss: 0.6491 Generator Loss: 1.2644\nEpoch [15/30]                        Batch 200/1563 Discriminator Loss: 0.5590 Generator Loss: 0.9048\nEpoch [15/30]                        Batch 300/1563 Discriminator Loss: 0.7712 Generator Loss: 1.0408\nEpoch [15/30]                        Batch 400/1563 Discriminator Loss: 0.6446 Generator Loss: 1.2220\nEpoch [15/30]                        Batch 500/1563 Discriminator Loss: 0.5891 Generator Loss: 0.8177\nEpoch [15/30]                        Batch 600/1563 Discriminator Loss: 0.4539 Generator Loss: 1.5968\nEpoch [15/30]                        Batch 700/1563 Discriminator Loss: 0.9036 Generator Loss: 1.2203\nEpoch [15/30]                        Batch 800/1563 Discriminator Loss: 0.6573 Generator Loss: 1.2048\nEpoch [15/30]                        Batch 900/1563 Discriminator Loss: 0.5110 Generator Loss: 0.8048\nEpoch [15/30]                        Batch 1000/1563 Discriminator Loss: 0.6662 Generator Loss: 1.6221\nEpoch [15/30]                        Batch 1100/1563 Discriminator Loss: 0.4542 Generator Loss: 1.4416\nEpoch [15/30]                        Batch 1200/1563 Discriminator Loss: 0.5644 Generator Loss: 1.5006\nEpoch [15/30]                        Batch 1300/1563 Discriminator Loss: 0.5741 Generator Loss: 1.5613\nEpoch [15/30]                        Batch 1400/1563 Discriminator Loss: 0.5394 Generator Loss: 1.6310\nEpoch [15/30]                        Batch 1500/1563 Discriminator Loss: 0.2746 Generator Loss: 1.2002\nEpoch [16/30]                        Batch 100/1563 Discriminator Loss: 0.9168 Generator Loss: 1.0404\nEpoch [16/30]                        Batch 200/1563 Discriminator Loss: 0.3522 Generator Loss: 2.0820\nEpoch [16/30]                        Batch 300/1563 Discriminator Loss: 1.0174 Generator Loss: 1.2016\nEpoch [16/30]                        Batch 400/1563 Discriminator Loss: 0.9039 Generator Loss: 0.9897\nEpoch [16/30]                        Batch 500/1563 Discriminator Loss: 0.7441 Generator Loss: 1.5617\nEpoch [16/30]                        Batch 600/1563 Discriminator Loss: 0.6339 Generator Loss: 0.7029\nEpoch [16/30]                        Batch 700/1563 Discriminator Loss: 0.6705 Generator Loss: 1.2205\nEpoch [16/30]                        Batch 800/1563 Discriminator Loss: 0.4608 Generator Loss: 1.9291\nEpoch [16/30]                        Batch 900/1563 Discriminator Loss: 0.3003 Generator Loss: 2.2707\nEpoch [16/30]                        Batch 1000/1563 Discriminator Loss: 0.4935 Generator Loss: 0.9453\nEpoch [16/30]                        Batch 1100/1563 Discriminator Loss: 0.6426 Generator Loss: 2.0306\nEpoch [16/30]                        Batch 1200/1563 Discriminator Loss: 0.3256 Generator Loss: 1.9450\nEpoch [16/30]                        Batch 1300/1563 Discriminator Loss: 0.4460 Generator Loss: 1.3895\nEpoch [16/30]                        Batch 1400/1563 Discriminator Loss: 0.5697 Generator Loss: 1.8839\nEpoch [16/30]                        Batch 1500/1563 Discriminator Loss: 0.3510 Generator Loss: 1.2868\nEpoch [17/30]                        Batch 100/1563 Discriminator Loss: 0.4061 Generator Loss: 1.5765\nEpoch [17/30]                        Batch 200/1563 Discriminator Loss: 0.6059 Generator Loss: 1.4607\nEpoch [17/30]                        Batch 300/1563 Discriminator Loss: 0.3888 Generator Loss: 2.0352\nEpoch [17/30]                        Batch 400/1563 Discriminator Loss: 0.9055 Generator Loss: 1.5344\nEpoch [17/30]                        Batch 500/1563 Discriminator Loss: 0.4919 Generator Loss: 2.0243\nEpoch [17/30]                        Batch 600/1563 Discriminator Loss: 0.5129 Generator Loss: 1.3443\nEpoch [17/30]                        Batch 700/1563 Discriminator Loss: 0.5908 Generator Loss: 1.3944\nEpoch [17/30]                        Batch 800/1563 Discriminator Loss: 0.6198 Generator Loss: 1.9401\nEpoch [17/30]                        Batch 900/1563 Discriminator Loss: 0.4722 Generator Loss: 1.3389\nEpoch [17/30]                        Batch 1000/1563 Discriminator Loss: 0.6337 Generator Loss: 1.2892\nEpoch [17/30]                        Batch 1100/1563 Discriminator Loss: 0.4962 Generator Loss: 1.3870\nEpoch [17/30]                        Batch 1200/1563 Discriminator Loss: 0.4610 Generator Loss: 0.8908\nEpoch [17/30]                        Batch 1300/1563 Discriminator Loss: 0.7297 Generator Loss: 1.1273\nEpoch [17/30]                        Batch 1400/1563 Discriminator Loss: 0.7066 Generator Loss: 0.8907\nEpoch [17/30]                        Batch 1500/1563 Discriminator Loss: 0.4157 Generator Loss: 1.4886\nEpoch [18/30]                        Batch 100/1563 Discriminator Loss: 0.4349 Generator Loss: 2.3884\nEpoch [18/30]                        Batch 200/1563 Discriminator Loss: 0.6107 Generator Loss: 0.9189\nEpoch [18/30]                        Batch 300/1563 Discriminator Loss: 0.6645 Generator Loss: 0.9639\nEpoch [18/30]                        Batch 400/1563 Discriminator Loss: 0.6846 Generator Loss: 1.6454\nEpoch [18/30]                        Batch 500/1563 Discriminator Loss: 0.6662 Generator Loss: 1.3799\nEpoch [18/30]                        Batch 600/1563 Discriminator Loss: 0.3959 Generator Loss: 1.6886\nEpoch [18/30]                        Batch 700/1563 Discriminator Loss: 0.7645 Generator Loss: 0.4036\nEpoch [18/30]                        Batch 800/1563 Discriminator Loss: 0.3437 Generator Loss: 1.5305\nEpoch [18/30]                        Batch 900/1563 Discriminator Loss: 0.7294 Generator Loss: 1.9114\nEpoch [18/30]                        Batch 1000/1563 Discriminator Loss: 0.3701 Generator Loss: 2.1974\nEpoch [18/30]                        Batch 1100/1563 Discriminator Loss: 0.3689 Generator Loss: 1.6446\nEpoch [18/30]                        Batch 1200/1563 Discriminator Loss: 0.4035 Generator Loss: 1.3158\nEpoch [18/30]                        Batch 1300/1563 Discriminator Loss: 0.6356 Generator Loss: 1.4208\nEpoch [18/30]                        Batch 1400/1563 Discriminator Loss: 0.6068 Generator Loss: 1.0243\nEpoch [18/30]                        Batch 1500/1563 Discriminator Loss: 0.7674 Generator Loss: 1.6801\nEpoch [19/30]                        Batch 100/1563 Discriminator Loss: 0.4846 Generator Loss: 1.2064\nEpoch [19/30]                        Batch 200/1563 Discriminator Loss: 0.6070 Generator Loss: 1.7031\nEpoch [19/30]                        Batch 300/1563 Discriminator Loss: 0.7450 Generator Loss: 1.0996\nEpoch [19/30]                        Batch 400/1563 Discriminator Loss: 0.6458 Generator Loss: 1.0814\nEpoch [19/30]                        Batch 500/1563 Discriminator Loss: 0.4784 Generator Loss: 1.4959\nEpoch [19/30]                        Batch 600/1563 Discriminator Loss: 0.3522 Generator Loss: 1.8114\nEpoch [19/30]                        Batch 700/1563 Discriminator Loss: 0.6813 Generator Loss: 1.4926\nEpoch [19/30]                        Batch 800/1563 Discriminator Loss: 0.3082 Generator Loss: 1.2779\nEpoch [19/30]                        Batch 900/1563 Discriminator Loss: 0.4817 Generator Loss: 1.6282\nEpoch [19/30]                        Batch 1000/1563 Discriminator Loss: 0.6549 Generator Loss: 1.8198\nEpoch [19/30]                        Batch 1100/1563 Discriminator Loss: 0.5999 Generator Loss: 2.1225\nEpoch [19/30]                        Batch 1200/1563 Discriminator Loss: 0.2709 Generator Loss: 1.7114\nEpoch [19/30]                        Batch 1300/1563 Discriminator Loss: 0.5228 Generator Loss: 1.3422\nEpoch [19/30]                        Batch 1400/1563 Discriminator Loss: 0.6171 Generator Loss: 1.0001\nEpoch [19/30]                        Batch 1500/1563 Discriminator Loss: 0.5075 Generator Loss: 1.5396\nEpoch [20/30]                        Batch 100/1563 Discriminator Loss: 0.7957 Generator Loss: 1.0529\nEpoch [20/30]                        Batch 200/1563 Discriminator Loss: 0.4388 Generator Loss: 0.9466\nEpoch [20/30]                        Batch 300/1563 Discriminator Loss: 0.3565 Generator Loss: 1.0153\nEpoch [20/30]                        Batch 400/1563 Discriminator Loss: 0.5776 Generator Loss: 1.3406\nEpoch [20/30]                        Batch 500/1563 Discriminator Loss: 0.5628 Generator Loss: 1.7318\nEpoch [20/30]                        Batch 600/1563 Discriminator Loss: 0.6587 Generator Loss: 1.0502\nEpoch [20/30]                        Batch 700/1563 Discriminator Loss: 0.3938 Generator Loss: 1.5252\nEpoch [20/30]                        Batch 800/1563 Discriminator Loss: 0.5031 Generator Loss: 1.4253\nEpoch [20/30]                        Batch 900/1563 Discriminator Loss: 0.8166 Generator Loss: 0.8745\nEpoch [20/30]                        Batch 1000/1563 Discriminator Loss: 0.8261 Generator Loss: 0.7913\nEpoch [20/30]                        Batch 1100/1563 Discriminator Loss: 0.3707 Generator Loss: 1.8016\nEpoch [20/30]                        Batch 1200/1563 Discriminator Loss: 0.5905 Generator Loss: 1.0425\nEpoch [20/30]                        Batch 1300/1563 Discriminator Loss: 0.4316 Generator Loss: 1.2454\nEpoch [20/30]                        Batch 1400/1563 Discriminator Loss: 0.6296 Generator Loss: 1.1763\nEpoch [20/30]                        Batch 1500/1563 Discriminator Loss: 0.3132 Generator Loss: 1.5287\n</code></pre> <p></p> <pre><code>Epoch [21/30]                        Batch 100/1563 Discriminator Loss: 0.6704 Generator Loss: 0.9994\nEpoch [21/30]                        Batch 200/1563 Discriminator Loss: 0.4294 Generator Loss: 1.6680\nEpoch [21/30]                        Batch 300/1563 Discriminator Loss: 0.3051 Generator Loss: 1.7558\nEpoch [21/30]                        Batch 400/1563 Discriminator Loss: 0.5725 Generator Loss: 1.0703\nEpoch [21/30]                        Batch 500/1563 Discriminator Loss: 0.5754 Generator Loss: 0.8874\nEpoch [21/30]                        Batch 600/1563 Discriminator Loss: 0.5921 Generator Loss: 0.9687\nEpoch [21/30]                        Batch 700/1563 Discriminator Loss: 0.3578 Generator Loss: 2.0567\nEpoch [21/30]                        Batch 800/1563 Discriminator Loss: 0.3976 Generator Loss: 1.6124\nEpoch [21/30]                        Batch 900/1563 Discriminator Loss: 0.5874 Generator Loss: 1.4437\nEpoch [21/30]                        Batch 1000/1563 Discriminator Loss: 0.3392 Generator Loss: 1.8707\nEpoch [21/30]                        Batch 1100/1563 Discriminator Loss: 0.6126 Generator Loss: 0.8583\nEpoch [21/30]                        Batch 1200/1563 Discriminator Loss: 0.6787 Generator Loss: 1.2624\nEpoch [21/30]                        Batch 1300/1563 Discriminator Loss: 0.4979 Generator Loss: 1.1952\nEpoch [21/30]                        Batch 1400/1563 Discriminator Loss: 0.4609 Generator Loss: 1.2393\nEpoch [21/30]                        Batch 1500/1563 Discriminator Loss: 0.4389 Generator Loss: 1.0301\nEpoch [22/30]                        Batch 100/1563 Discriminator Loss: 0.5087 Generator Loss: 0.8303\nEpoch [22/30]                        Batch 200/1563 Discriminator Loss: 0.5395 Generator Loss: 1.7216\nEpoch [22/30]                        Batch 300/1563 Discriminator Loss: 0.7014 Generator Loss: 1.3354\nEpoch [22/30]                        Batch 400/1563 Discriminator Loss: 0.4920 Generator Loss: 1.2890\nEpoch [22/30]                        Batch 500/1563 Discriminator Loss: 0.6030 Generator Loss: 0.8941\nEpoch [22/30]                        Batch 600/1563 Discriminator Loss: 0.5582 Generator Loss: 0.8958\nEpoch [22/30]                        Batch 700/1563 Discriminator Loss: 0.5013 Generator Loss: 2.0577\nEpoch [22/30]                        Batch 800/1563 Discriminator Loss: 0.6041 Generator Loss: 1.3337\nEpoch [22/30]                        Batch 900/1563 Discriminator Loss: 0.5399 Generator Loss: 0.9913\nEpoch [22/30]                        Batch 1000/1563 Discriminator Loss: 0.6801 Generator Loss: 0.9158\nEpoch [22/30]                        Batch 1100/1563 Discriminator Loss: 0.4355 Generator Loss: 1.8976\nEpoch [22/30]                        Batch 1200/1563 Discriminator Loss: 0.6911 Generator Loss: 1.3129\nEpoch [22/30]                        Batch 1300/1563 Discriminator Loss: 0.5185 Generator Loss: 1.0487\nEpoch [22/30]                        Batch 1400/1563 Discriminator Loss: 0.7541 Generator Loss: 1.2654\nEpoch [22/30]                        Batch 1500/1563 Discriminator Loss: 0.4369 Generator Loss: 1.2044\nEpoch [23/30]                        Batch 100/1563 Discriminator Loss: 0.5012 Generator Loss: 0.8639\nEpoch [23/30]                        Batch 200/1563 Discriminator Loss: 0.7417 Generator Loss: 1.0816\nEpoch [23/30]                        Batch 300/1563 Discriminator Loss: 0.5031 Generator Loss: 1.6705\nEpoch [23/30]                        Batch 400/1563 Discriminator Loss: 0.7800 Generator Loss: 1.1085\nEpoch [23/30]                        Batch 500/1563 Discriminator Loss: 0.6581 Generator Loss: 0.7194\nEpoch [23/30]                        Batch 600/1563 Discriminator Loss: 0.6880 Generator Loss: 1.2136\nEpoch [23/30]                        Batch 700/1563 Discriminator Loss: 0.8604 Generator Loss: 1.2978\nEpoch [23/30]                        Batch 800/1563 Discriminator Loss: 0.4069 Generator Loss: 1.5116\nEpoch [23/30]                        Batch 900/1563 Discriminator Loss: 0.6084 Generator Loss: 1.3218\nEpoch [23/30]                        Batch 1000/1563 Discriminator Loss: 0.3370 Generator Loss: 2.3237\nEpoch [23/30]                        Batch 1100/1563 Discriminator Loss: 0.5806 Generator Loss: 1.6243\nEpoch [23/30]                        Batch 1200/1563 Discriminator Loss: 0.5700 Generator Loss: 1.3030\nEpoch [23/30]                        Batch 1300/1563 Discriminator Loss: 0.4817 Generator Loss: 1.0187\nEpoch [23/30]                        Batch 1400/1563 Discriminator Loss: 0.5350 Generator Loss: 0.9343\nEpoch [23/30]                        Batch 1500/1563 Discriminator Loss: 0.3988 Generator Loss: 1.9423\nEpoch [24/30]                        Batch 100/1563 Discriminator Loss: 0.5867 Generator Loss: 1.8228\nEpoch [24/30]                        Batch 200/1563 Discriminator Loss: 0.6380 Generator Loss: 0.7886\nEpoch [24/30]                        Batch 300/1563 Discriminator Loss: 1.0712 Generator Loss: 1.7067\nEpoch [24/30]                        Batch 400/1563 Discriminator Loss: 0.3691 Generator Loss: 1.6998\nEpoch [24/30]                        Batch 500/1563 Discriminator Loss: 0.7596 Generator Loss: 0.6284\nEpoch [24/30]                        Batch 600/1563 Discriminator Loss: 0.8459 Generator Loss: 1.5625\nEpoch [24/30]                        Batch 700/1563 Discriminator Loss: 0.4499 Generator Loss: 1.0025\nEpoch [24/30]                        Batch 800/1563 Discriminator Loss: 0.3667 Generator Loss: 1.3655\nEpoch [24/30]                        Batch 900/1563 Discriminator Loss: 0.8259 Generator Loss: 1.6259\nEpoch [24/30]                        Batch 1000/1563 Discriminator Loss: 0.8847 Generator Loss: 1.0071\nEpoch [24/30]                        Batch 1100/1563 Discriminator Loss: 0.2908 Generator Loss: 2.2473\nEpoch [24/30]                        Batch 1200/1563 Discriminator Loss: 0.8809 Generator Loss: 0.8121\nEpoch [24/30]                        Batch 1300/1563 Discriminator Loss: 0.5599 Generator Loss: 0.8316\nEpoch [24/30]                        Batch 1400/1563 Discriminator Loss: 0.5931 Generator Loss: 1.6818\nEpoch [24/30]                        Batch 1500/1563 Discriminator Loss: 0.4780 Generator Loss: 1.4393\nEpoch [25/30]                        Batch 100/1563 Discriminator Loss: 0.4862 Generator Loss: 1.0551\nEpoch [25/30]                        Batch 200/1563 Discriminator Loss: 0.7419 Generator Loss: 0.8418\nEpoch [25/30]                        Batch 300/1563 Discriminator Loss: 0.3005 Generator Loss: 1.4868\nEpoch [25/30]                        Batch 400/1563 Discriminator Loss: 0.4458 Generator Loss: 0.8691\nEpoch [25/30]                        Batch 500/1563 Discriminator Loss: 0.4591 Generator Loss: 1.3558\nEpoch [25/30]                        Batch 600/1563 Discriminator Loss: 0.2917 Generator Loss: 1.1712\nEpoch [25/30]                        Batch 700/1563 Discriminator Loss: 0.6088 Generator Loss: 1.1128\nEpoch [25/30]                        Batch 800/1563 Discriminator Loss: 0.8086 Generator Loss: 1.5543\nEpoch [25/30]                        Batch 900/1563 Discriminator Loss: 0.5244 Generator Loss: 1.4409\nEpoch [25/30]                        Batch 1000/1563 Discriminator Loss: 0.6720 Generator Loss: 1.2794\nEpoch [25/30]                        Batch 1100/1563 Discriminator Loss: 0.5955 Generator Loss: 1.0705\nEpoch [25/30]                        Batch 1200/1563 Discriminator Loss: 0.6215 Generator Loss: 1.2729\nEpoch [25/30]                        Batch 1300/1563 Discriminator Loss: 1.2661 Generator Loss: 1.4346\nEpoch [25/30]                        Batch 1400/1563 Discriminator Loss: 0.6111 Generator Loss: 1.4205\nEpoch [25/30]                        Batch 1500/1563 Discriminator Loss: 0.5564 Generator Loss: 1.5329\nEpoch [26/30]                        Batch 100/1563 Discriminator Loss: 0.7150 Generator Loss: 1.1040\nEpoch [26/30]                        Batch 200/1563 Discriminator Loss: 0.5486 Generator Loss: 0.8081\nEpoch [26/30]                        Batch 300/1563 Discriminator Loss: 0.4655 Generator Loss: 1.0543\nEpoch [26/30]                        Batch 400/1563 Discriminator Loss: 0.6280 Generator Loss: 1.3089\nEpoch [26/30]                        Batch 500/1563 Discriminator Loss: 0.6435 Generator Loss: 1.2938\nEpoch [26/30]                        Batch 600/1563 Discriminator Loss: 0.5681 Generator Loss: 0.6704\nEpoch [26/30]                        Batch 700/1563 Discriminator Loss: 0.5975 Generator Loss: 1.6594\nEpoch [26/30]                        Batch 800/1563 Discriminator Loss: 0.5214 Generator Loss: 1.5375\nEpoch [26/30]                        Batch 900/1563 Discriminator Loss: 0.7233 Generator Loss: 2.1745\nEpoch [26/30]                        Batch 1000/1563 Discriminator Loss: 0.7600 Generator Loss: 1.9514\nEpoch [26/30]                        Batch 1100/1563 Discriminator Loss: 0.4599 Generator Loss: 1.6351\nEpoch [26/30]                        Batch 1200/1563 Discriminator Loss: 0.4130 Generator Loss: 0.6645\nEpoch [26/30]                        Batch 1300/1563 Discriminator Loss: 0.7173 Generator Loss: 1.5409\nEpoch [26/30]                        Batch 1400/1563 Discriminator Loss: 0.6444 Generator Loss: 0.4363\nEpoch [26/30]                        Batch 1500/1563 Discriminator Loss: 0.8130 Generator Loss: 0.5472\nEpoch [27/30]                        Batch 100/1563 Discriminator Loss: 0.3806 Generator Loss: 2.5330\nEpoch [27/30]                        Batch 200/1563 Discriminator Loss: 0.6471 Generator Loss: 1.4548\nEpoch [27/30]                        Batch 300/1563 Discriminator Loss: 0.4945 Generator Loss: 2.0417\nEpoch [27/30]                        Batch 400/1563 Discriminator Loss: 0.8287 Generator Loss: 0.8313\nEpoch [27/30]                        Batch 500/1563 Discriminator Loss: 0.2590 Generator Loss: 1.3648\nEpoch [27/30]                        Batch 600/1563 Discriminator Loss: 0.7229 Generator Loss: 1.0506\nEpoch [27/30]                        Batch 700/1563 Discriminator Loss: 0.5257 Generator Loss: 0.6630\nEpoch [27/30]                        Batch 800/1563 Discriminator Loss: 0.7077 Generator Loss: 1.5787\nEpoch [27/30]                        Batch 900/1563 Discriminator Loss: 0.2241 Generator Loss: 2.4144\nEpoch [27/30]                        Batch 1000/1563 Discriminator Loss: 0.9249 Generator Loss: 1.0945\nEpoch [27/30]                        Batch 1100/1563 Discriminator Loss: 0.5512 Generator Loss: 1.3305\nEpoch [27/30]                        Batch 1200/1563 Discriminator Loss: 0.6963 Generator Loss: 0.6857\nEpoch [27/30]                        Batch 1300/1563 Discriminator Loss: 0.6866 Generator Loss: 1.0320\nEpoch [27/30]                        Batch 1400/1563 Discriminator Loss: 0.8760 Generator Loss: 0.5938\nEpoch [27/30]                        Batch 1500/1563 Discriminator Loss: 0.6564 Generator Loss: 1.1705\nEpoch [28/30]                        Batch 100/1563 Discriminator Loss: 0.6794 Generator Loss: 0.6070\nEpoch [28/30]                        Batch 200/1563 Discriminator Loss: 0.5660 Generator Loss: 1.3422\nEpoch [28/30]                        Batch 300/1563 Discriminator Loss: 0.4716 Generator Loss: 1.3143\nEpoch [28/30]                        Batch 400/1563 Discriminator Loss: 0.5311 Generator Loss: 1.8503\nEpoch [28/30]                        Batch 500/1563 Discriminator Loss: 0.5435 Generator Loss: 1.4696\nEpoch [28/30]                        Batch 600/1563 Discriminator Loss: 0.4778 Generator Loss: 1.1126\nEpoch [28/30]                        Batch 700/1563 Discriminator Loss: 0.4951 Generator Loss: 0.9386\nEpoch [28/30]                        Batch 800/1563 Discriminator Loss: 0.5161 Generator Loss: 1.7677\nEpoch [28/30]                        Batch 900/1563 Discriminator Loss: 0.5352 Generator Loss: 1.3093\nEpoch [28/30]                        Batch 1000/1563 Discriminator Loss: 0.4307 Generator Loss: 1.0498\nEpoch [28/30]                        Batch 1100/1563 Discriminator Loss: 0.8755 Generator Loss: 1.3385\nEpoch [28/30]                        Batch 1200/1563 Discriminator Loss: 0.5321 Generator Loss: 1.0425\nEpoch [28/30]                        Batch 1300/1563 Discriminator Loss: 0.6281 Generator Loss: 0.8529\nEpoch [28/30]                        Batch 1400/1563 Discriminator Loss: 0.5690 Generator Loss: 1.3257\nEpoch [28/30]                        Batch 1500/1563 Discriminator Loss: 1.0743 Generator Loss: 1.1915\nEpoch [29/30]                        Batch 100/1563 Discriminator Loss: 0.6799 Generator Loss: 1.0841\nEpoch [29/30]                        Batch 200/1563 Discriminator Loss: 0.6932 Generator Loss: 1.3634\nEpoch [29/30]                        Batch 300/1563 Discriminator Loss: 0.4663 Generator Loss: 1.1661\nEpoch [29/30]                        Batch 400/1563 Discriminator Loss: 0.7206 Generator Loss: 0.9654\nEpoch [29/30]                        Batch 500/1563 Discriminator Loss: 0.7813 Generator Loss: 1.4285\nEpoch [29/30]                        Batch 600/1563 Discriminator Loss: 0.8215 Generator Loss: 1.1129\nEpoch [29/30]                        Batch 700/1563 Discriminator Loss: 0.5939 Generator Loss: 1.5154\nEpoch [29/30]                        Batch 800/1563 Discriminator Loss: 0.6840 Generator Loss: 1.2734\nEpoch [29/30]                        Batch 900/1563 Discriminator Loss: 0.4068 Generator Loss: 1.2772\nEpoch [29/30]                        Batch 1000/1563 Discriminator Loss: 0.6416 Generator Loss: 0.4559\nEpoch [29/30]                        Batch 1100/1563 Discriminator Loss: 0.4972 Generator Loss: 1.6791\nEpoch [29/30]                        Batch 1200/1563 Discriminator Loss: 0.8609 Generator Loss: 1.1342\nEpoch [29/30]                        Batch 1300/1563 Discriminator Loss: 0.6266 Generator Loss: 0.8422\nEpoch [29/30]                        Batch 1400/1563 Discriminator Loss: 0.5542 Generator Loss: 1.1366\nEpoch [29/30]                        Batch 1500/1563 Discriminator Loss: 0.5414 Generator Loss: 1.0412\nEpoch [30/30]                        Batch 100/1563 Discriminator Loss: 1.0110 Generator Loss: 0.9997\nEpoch [30/30]                        Batch 200/1563 Discriminator Loss: 0.4734 Generator Loss: 1.2250\nEpoch [30/30]                        Batch 300/1563 Discriminator Loss: 0.3835 Generator Loss: 1.3170\nEpoch [30/30]                        Batch 400/1563 Discriminator Loss: 0.3179 Generator Loss: 2.2588\nEpoch [30/30]                        Batch 500/1563 Discriminator Loss: 0.3329 Generator Loss: 1.5280\nEpoch [30/30]                        Batch 600/1563 Discriminator Loss: 0.3452 Generator Loss: 0.8436\nEpoch [30/30]                        Batch 700/1563 Discriminator Loss: 0.8666 Generator Loss: 0.6735\nEpoch [30/30]                        Batch 800/1563 Discriminator Loss: 0.6920 Generator Loss: 1.8045\nEpoch [30/30]                        Batch 900/1563 Discriminator Loss: 0.6625 Generator Loss: 0.8388\nEpoch [30/30]                        Batch 1000/1563 Discriminator Loss: 0.7458 Generator Loss: 2.1390\nEpoch [30/30]                        Batch 1100/1563 Discriminator Loss: 1.4766 Generator Loss: 1.7256\nEpoch [30/30]                        Batch 1200/1563 Discriminator Loss: 0.4399 Generator Loss: 1.4570\nEpoch [30/30]                        Batch 1300/1563 Discriminator Loss: 0.4310 Generator Loss: 1.7740\nEpoch [30/30]                        Batch 1400/1563 Discriminator Loss: 0.4642 Generator Loss: 1.1918\nEpoch [30/30]                        Batch 1500/1563 Discriminator Loss: 0.5108 Generator Loss: 1.7462\n</code></pre> <p></p>"}, {"location": "classes/generative-models/", "title": "12. Generative Models", "text": "<p>Generative AI, often abbreviated as GenAI, is a subfield of artificial intelligence that employs generative models to create new content, such as text, images, videos, audio, software code, or other data forms, by learning patterns and structures from vast training datasets. These models typically respond to user prompts (e.g., natural language inputs) by producing original outputs that mimic the style or characteristics of the learned data, distinguishing them from traditional AI systems that primarily analyze or predict existing information. Common examples include tools like ChatGPT for text generation and DALL-E for image creation, powered by techniques such as large language models (LLMs) or generative adversarial networks (GANs).</p> <p>Generative AI's roots trace back to early probability models, evolving through rule-based systems to deep learning. We'll cover chronologically, with detailed highlights on transformative events.</p> Era/Year Key Development Details/Highlights Impact 1950s: Probabilistic Foundations Markov Chains (1953, Claude Shannon) Shannon's work on information theory introduced Markov models for text generation (e.g., predicting next letters). Highlight: First \"AI poem\" generated via chains\u2014crude but proved machines could mimic patterns. Laid groundwork for sequence generation; influenced NLP. 1980s: Early Neural Nets Boltzmann Machines (1986, Geoffrey Hinton et al.) Restricted Boltzmann Machines (RBMs) used energy-based models to learn data distributions. Highlight: Hinton's \"wake-sleep\" algorithm (1995 precursor) trained unsupervised nets on images\u2014first glimpses of generative \"dreaming.\" Bridge to deep learning; used in early recommender systems (e.g., Netflix Prize roots). 1990s: Variational Inference Variational Autoencoders (VAEs) precursors (1990s, Dayan et al.) Bayesian methods for latent variable models. Highlight: Jordan &amp; Weiss's variational EM (1998)\u2014key paper enabling tractable posterior approximations. Enabled scalable generative modeling; foundation for modern VAEs. 2010s: Deep Learning Boom Deep Belief Nets (2006, Hinton) \u2192 VAEs (2013, Kingma &amp; Welling) Stacked RBMs pre-trained deep nets. Highlight: VAEs paper (ICLR 2014) introduced reparameterization trick for backprop through stochastic nodes\u2014generated blurry MNIST digits, but scalable.  Diffusion models' roots in score-based generative modeling (Sohl-Dickstein et al., 2015). Democratized unsupervised learning; VAEs in drug discovery (e.g., AlphaFold precursors). 2014-Present: Adversarial Era GANs (2014, Goodfellow et al.) Subsequent: StyleGAN (2018, NVIDIA) for photorealistic faces. Highlight: GANs' NIPS 2014 debut generated realistic bedrooms\u2014shocked community, sparking \"adversarial training\" paradigm. AlphaGo (2016) used generative rollouts. GPT-1 (2018) for text; Exploded applications (e.g., DeepFakes 2017); ethical debates (e.g., 2018 EU AI ethics guidelines). 2020s: Scaling &amp; Multimodal Diffusion Models (2020, Ho et al.); GPT-3 (2020, OpenAI) Denoising diffusion probabilistic models (DDPMs). Highlight: DALL\u00b7E (2021) combined diffusion + transformers for text-to-image; Stable Diffusion (2022) open-sourced, generating 1B+ images/month. Multimodal gen AI (e.g., Sora video gen, 2024); concerns over energy use (training GPT-4 ~1GWh). <ol> <li> <p>Geeks for Geeks - What is Generative AI? \u21a9</p> </li> <li> <p>A Brief History of Generative Models \u21a9</p> </li> </ol>"}, {"location": "classes/metrics/classification/", "title": "9.1. Classification", "text": "<p>Below is a detailed list of metrics commonly used to evaluate the accuracy and performance of classification and regression models in machine learning, including neural networks. The metrics are categorized based on their applicability to classification or regression tasks, with explanations of their purpose and mathematical formulations where relevant.</p>"}, {"location": "classes/metrics/classification/#classification-metrics", "title": "Classification Metrics", "text": "<p>Classification tasks involve predicting discrete class labels. The following metrics assess the accuracy and effectiveness of such models:</p> Metric Purpose Use Case Accuracy \\( \\displaystyle \\frac{TP + TN}{TP + TN + FP + FN} \\) Measures the proportion of correct predictions across all classes Suitable for balanced datasets but misleading for imbalanced ones Precision \\( \\displaystyle \\frac{TP}{TP + FP} \\) Evaluates the proportion of positive predictions that are actually correct Important when false positives are costly (e.g., spam detection) Recall (Sensitivity) \\( \\displaystyle \\frac{TP}{TP + FN} \\) Assesses the proportion of actual positives correctly identified Critical when false negatives are costly (e.g., disease detection) F1-Score \\( \\displaystyle 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\) Harmonic mean of precision and recall, balancing both metrics Useful for imbalanced datasets where both precision and recall matter AUC-ROC  Area under the curve plotting True Positive Rate (Recall) vs. False Positive Rate \\( \\displaystyle \\left( \\frac{FP}{FP + TN} \\right) \\) Measures the model\u2019s ability to distinguish between classes across all thresholds Effective for binary classification and assessing model robustness AUC-PR  Area under the curve plotting Precision vs. Recall Focuses on precision and recall trade-off, especially for imbalanced datasets Preferred when positive class is rare (e.g., fraud detection) Confusion Matrix<sup>1</sup> Provides a tabular summary of prediction outcomes (TP, TN, FP, FN) Offers detailed insights into class-specific performance, especially for multi-class problems Hamming Loss \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{L} \\sum_{j=1}^L \\mathbf{1}(y_{ij} \\neq \\hat{y}_{ij}) \\) Calculates the fraction of incorrect labels to the total number of labels Suitable for multi-label classification tasks Balanced Accuracy \\( \\displaystyle \\frac{1}{C} \\sum_{i=1}^C \\frac{TP_i}{TP_i + FN_i} \\) Average of recall obtained on each class, useful for imbalanced datasets Effective for multi-class problems with class imbalance"}, {"location": "classes/metrics/classification/#loss-functions", "title": "Loss Functions", "text": "<p>Loss functions commonly used in classification tasks:</p> Metric Purpose Use Case Cross-Entropy Loss \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\) Measures the performance of a classification model whose output is a probability value between 0 and 1. It increases as the predicted probability diverges from the actual label. Commonly used in classification tasks with probabilistic outputs. Binary Cross-Entropy<sup>2</sup> \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\) Used for binary classification tasks, measuring the difference between two probability distributions. Commonly used in binary classification problems. Categorical Cross-Entropy \\( \\displaystyle -\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) \\) Used when there are two or more label classes. It is a generalization of binary cross-entropy to multi-class problems. Suitable for multi-class classification tasks with one-hot encoded labels. Sparse Categorical Cross-Entropy \\( \\displaystyle -\\sum_{i=1}^{N} \\log(\\hat{y}_{i,y_i}) \\) Similar to categorical cross-entropy but used when labels are provided as integers rather than one-hot encoded vectors. Useful for multi-class classification with integer labels. Balanced Cross-Entropy \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ w_1 y_i \\log(\\hat{y}_i) + w_0 (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\) Adjusts the standard cross-entropy loss to account for class imbalance by weighting classes inversely proportional to their frequency. Useful in imbalanced classification tasks. Kullback-Leibler Divergence \\( \\displaystyle D_{KL}(P \\| Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right) \\) Measures how one probability distribution diverges from a second, expected probability distribution. It is often used in variational autoencoders and other probabilistic models. Useful in scenarios involving probabilistic models and distributions. Hinge Loss \\( \\displaystyle \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i) \\) Used for \"maximum-margin\" classification, primarily for support vector machines (SVMs). It is designed to ensure that the correct class is not only predicted but also separated from the decision boundary by a margin. Effective for SVMs and tasks requiring a margin between classes. Focal Loss<sup>2</sup> \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\) A modified version of cross-entropy loss that addresses class imbalance by down-weighting easy examples and focusing training on hard negatives. Beneficial in scenarios with significant class imbalance, such as object detection. Multi-Class Log Loss \\( \\displaystyle -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) \\) Extends binary log loss to multi-class classification problems, penalizing incorrect predictions based on predicted probabilities. Suitable for multi-class classification tasks. Hamming Loss \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{L} \\sum_{j=1}^L \\mathbf{1}(y_{ij} \\neq \\hat{y}_{ij}) \\) Measures the fraction of incorrect labels to the total number of labels, useful for multi-label classification tasks. Effective for multi-label classification scenarios."}, {"location": "classes/metrics/classification/#additional", "title": "Additional", "text": ""}, {"location": "classes/metrics/classification/#explanation-of-roc-curve-auc-roc", "title": "Explanation of ROC Curve (AUC-ROC)", "text": "<p>An ROC curve plots the True Positive Rate (TPR, or sensitivity/recall) against the False Positive Rate (FPR) at various classification thresholds. It helps visualize the trade-off between sensitivity and specificity for a classifier:</p> <ul> <li> <p>True Positive Rate (TPR): The proportion of actual positives correctly identified (TP / (TP + FN)).</p> </li> <li> <p>False Positive Rate (FPR): The proportion of actual negatives incorrectly classified as positives (FP / (FP + TN)).</p> </li> <li> <p>The Area Under the Curve (AUC) quantifies the overall performance, with AUC = 1 indicating a perfect classifier and AUC = 0.5 indicating a random classifier.</p> </li> </ul> 2025-11-06T12:09:06.678500 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <ol> <li> <p> Confusion Matrix \u21a9</p> </li> <li> <p>Focal Loss vs. Binary Cross-Entropy \u21a9\u21a9</p> </li> <li> <p>Binary Cross-Entropy Loss for Binary Classification \u21a9</p> </li> </ol>"}, {"location": "classes/metrics/generative/", "title": "9.3. Generative", "text": ""}, {"location": "classes/metrics/generative/#metrics-for-generative-ai", "title": "Metrics for Generative AI", "text": "<p>Generative AI models, such as those for text (e.g., GPT series), images (e.g., DALL-E), or audio, are evaluated using a mix of automated quantitative metrics and qualitative human assessments. These metrics assess aspects like quality, coherence, diversity, fidelity to inputs, and ethical considerations. Below is a table summarizing key metrics commonly used across generative tasks, with descriptions and primary use cases. Note that no single metric is perfect, and combinations (including human evaluation) are often recommended.</p>"}, {"location": "classes/metrics/generative/#1-text-generation-language-modeling", "title": "1. Text Generation &amp; Language Modeling", "text": "Metric Description Key Use Cases Perplexity Measures how well a probability model predicts a sample; lower = better fluency &amp; coherence Language modeling, next-word prediction BLEU N-gram precision overlap with reference(s); penalizes short outputs Machine translation, dialogue, text generation ROUGE Recall-oriented n-gram/LCS overlap Summarization, headline generation METEOR Aligns unigrams with synonyms, stemming, and word order Translation, paraphrasing BERTScore Cosine similarity of BERT embeddings (semantic) Any text: faithfulness, QA, summarization Self-BLEU / Unique n-grams Measures diversity by treating one output as \"reference\" for others Story generation, open-ended chat"}, {"location": "classes/metrics/generative/#2-image-visual-generation", "title": "2. Image &amp; Visual Generation", "text": "Metric Description Key Use Cases FID (Fr\u00e9chet Inception Distance) Compares feature distributions of real vs. generated images GANs, diffusion models (e.g., Stable Diffusion) Inception Score (IS) Quality + diversity via classifier confidence &amp; entropy GAN evaluation (legacy; less used now) Precision &amp; Recall for Distributions Separately measures realism (precision) and coverage (recall) High-res image synthesis CLIP Score Cosine similarity between image and text prompt embeddings Text-to-image alignment (DALL\u00b7E, Midjourney)"}, {"location": "classes/metrics/generative/#3-multimodal-cross-modal-tasks", "title": "3. Multimodal &amp; Cross-Modal Tasks", "text": "Metric Description Key Use Cases CLIP Score / T5 Score Text-image or text-text semantic alignment Image captioning, visual QA, retrieval R@K (Recall at K) Retrieval accuracy in joint embedding space Image-text retrieval Human Preference (Elo, A/B) Pairwise human judgments Text-to-image, video, music"}, {"location": "classes/metrics/generative/#4-safety-ethics-fairness", "title": "4. Safety, Ethics &amp; Fairness", "text": "Metric Description Key Use Cases Toxicity Score (Perspective API, RealToxicityPrompts) Probability of harmful content Chatbots, content generation Bias Metrics (WEAT, CrowS-Pairs, Bias-in-Bios) Measures stereotyping in embeddings or outputs Fairness in hiring, gender/race bias Regard / Honesty Scores Evaluates respectfulness or truthfulness Dialogue systems, factuality"}, {"location": "classes/metrics/generative/#5-general-human-centric-evaluation", "title": "5. General / Human-Centric Evaluation", "text": "Metric Description Key Use Cases Human Evaluation (Likert, Ranking, Fluency/Coherence) Crowdsourced ratings on multiple axes All domains \u2013 gold standard LLM-as-a-Judge (GPT-4 Eval, Reward Models) Uses strong LLM to score outputs vs. references Scalable alternative to human eval HELM / BIG-bench / MMLU-style Probes Holistic benchmark suites General capability assessment"}, {"location": "classes/metrics/generative/#quick-reference-by-task", "title": "Quick Reference by Task", "text": "Task Recommended Metrics Machine Translation BLEU, METEOR, BERTScore, chrF Summarization ROUGE, BERTScore, Factuality (e.g., QAGS) Text-to-Image FID, CLIP Score, Human pref Dialogue / Chat Perplexity, Diversity, Toxicity, Human rating Creative Writing Self-BLEU, MAUVE, Human creativity score"}, {"location": "classes/metrics/regression/", "title": "9.2. Regression", "text": "<p>Regression tasks predict continuous values. The following metrics evaluate the accuracy of predicted values against true values:</p> Metric Purpose Use Case Mean Absolute Error (MAE) \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\vert y_i - \\hat{y}_i \\vert \\) Measures average absolute difference between predictions and true values Robust to outliers, interpretable as average error Mean Squared Error (MSE) \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\) Measures average squared difference between predictions and true values Sensitive to outliers, commonly used in neural network loss functions Root Mean Squared Error (RMSE) \\( \\displaystyle \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2} \\) Square root of MSE, providing error in same units as target Preferred for interpretable error magnitude, widely used in forecasting Mean Absolute Percentage Error (MAPE) \\( \\displaystyle \\frac{1}{N} \\sum_{i=1}^N \\left \\vert \\frac{y_i - \\hat{y}_i}{y_i} \\right \\vert \\cdot 100 \\) Measures average percentage error relative to true values Useful when relative errors matter (e.g., financial predictions), but sensitive to zero or near-zero true values \\(R^2\\) (Coefficient of Determination) \\( \\displaystyle 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2} \\) Measures proportion of variance in dependent variable explained by model Indicates model fit, with values closer to 1 indicating better fit Adjusted \\(R^2\\) \\( \\displaystyle 1 - \\left( \\frac{(1 - R^2)(N - 1)}{N - k - 1} \\right) \\) Adjusts R\u00b2 for number of predictors, penalizing overly complex models Useful when comparing models with different numbers of features Median Absolute Error (\\(\\text{MedAE}\\)) \\( \\displaystyle \\text{median}(\\vert y_1 - \\hat{y}_1 \\vert, \\dots, \\vert y_N - \\hat{y}_N \\vert) \\) Measures median of absolute differences, highly robust to outliers Preferred in datasets with extreme values or non-Gaussian errors"}, {"location": "classes/mlp/", "title": "6. Multi-Layer Perceptron", "text": ""}, {"location": "classes/mlp/#fast-forward-multi-layer-perceptrons-mlps", "title": "Fast Forward: Multi-Layer Perceptrons (MLPs)", "text": "<pre><code>flowchart LR\n    classDef default fill:transparent,stroke:#333,stroke-width:1px;\n    classDef others fill:transparent,stroke:transparent,stroke-width:0px;\n    subgraph in[\" \"]\n        in1@{ shape: circle, label: \" \" }\n        in2@{ shape: circle, label: \" \" }\n        in3@{ shape: circle, label: \" \" }\n        inn@{ shape: circle, label: \" \" }\n    end\n    subgraph input\n        x1([\"x&lt;sub&gt;1&lt;/sub&gt;\"])\n        x2([\"x&lt;sub&gt;2&lt;/sub&gt;\"])\n        x3([\"x&lt;sub&gt;3&lt;/sub&gt;\"])\n        xd([\"...\"]):::others\n        xn([\"x&lt;sub&gt;n&lt;/sub&gt;\"])\n        xb([\"1\"])\n    end\n    subgraph hidden\n        direction TB\n        h1([\"h&lt;sub&gt;1&lt;/sub&gt;\"])\n        h2([\"h&lt;sub&gt;2&lt;/sub&gt;\"])\n        hd([\"...\"]):::others\n        hm([\"h&lt;sub&gt;m&lt;/sub&gt;\"])\n        hb([\"1\"])\n    end\n    subgraph output\n        y1([\"y&lt;sub&gt;1&lt;/sub&gt;\"])\n        yd([\"...\"]):::others\n        yk([\"y&lt;sub&gt;k&lt;/sub&gt;\"])\n    end\n    in1@{ shape: circle, label: \" \" } --&gt; x1\n    in2@{ shape: circle, label: \" \" } --&gt; x2\n    in3@{ shape: circle, label: \" \" } --&gt; x3\n    inn@{ shape: circle, label: \" \" } --&gt; xn\n\n    x1 --&gt;|\"w&lt;sub&gt;11&lt;/sub&gt;\"|h1\n    x1 --&gt;|\"w&lt;sub&gt;12&lt;/sub&gt;\"|h2\n    x1 --&gt;|\"w&lt;sub&gt;1n&lt;/sub&gt;\"|hm\n    x2 --&gt;|\"w&lt;sub&gt;21&lt;/sub&gt;\"|h1\n    x2 --&gt;|\"w&lt;sub&gt;22&lt;/sub&gt;\"|h2\n    x2 --&gt;|\"w&lt;sub&gt;2n&lt;/sub&gt;\"|hm\n    x3 --&gt;|\"w&lt;sub&gt;31&lt;/sub&gt;\"|h1\n    x3 --&gt;|\"w&lt;sub&gt;32&lt;/sub&gt;\"|h2\n    x3 --&gt;|\"w&lt;sub&gt;3n&lt;/sub&gt;\"|hm\n    xn --&gt;|\"w&lt;sub&gt;i1&lt;/sub&gt;\"|h1\n    xn --&gt;|\"w&lt;sub&gt;i2&lt;/sub&gt;\"|h2\n    xn --&gt;|\"w&lt;sub&gt;in&lt;/sub&gt;\"|hm\n    xb --&gt;|\"b&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;\"|h1\n    xb --&gt;|\"b&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;2&lt;/sub&gt;\"|h2\n    xb --&gt;|\"b&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;n&lt;/sub&gt;\"|hm\n\n    h1 --&gt;|\"v&lt;sub&gt;11&lt;/sub&gt;\"|y1\n    h1 --&gt;|\"v&lt;sub&gt;1k&lt;/sub&gt;\"|yk\n    h2 --&gt;|\"v&lt;sub&gt;21&lt;/sub&gt;\"|y1\n    h2 --&gt;|\"v&lt;sub&gt;2k&lt;/sub&gt;\"|yk\n    hm --&gt;|\"v&lt;sub&gt;m1&lt;/sub&gt;\"|y1\n    hm --&gt;|\"v&lt;sub&gt;mk&lt;/sub&gt;\"|yk\n    hb --&gt;|\"b&lt;sup&gt;h&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;\"|y1\n    hb --&gt;|\"b&lt;sup&gt;h&lt;/sup&gt;&lt;sub&gt;k&lt;/sub&gt;\"|yk\n\n    y1 --&gt; out1@{ shape: dbl-circ, label: \" \" }\n    yk --&gt; outn@{ shape: dbl-circ, label: \" \" }\n\n    style in fill:#fff,stroke:#666,stroke-width:0px\n    style input fill:#fff,stroke:#666,stroke-width:1px\n    style hidden fill:#fff,stroke:#666,stroke-width:1px\n    style output fill:#fff,stroke:#666,stroke-width:1px</code></pre> Multi-Layer Perceptron (MLP) Architecture. \\[ y_k = \\sigma \\left( \\sum_{j=1}^{m} \\sigma \\left( \\sum_{i=1}^{n} x_i w_{ij} + b^{h}_{i} \\right) v_{jk} + b^{y}_{j} \\right) \\] <p>where:</p> <ul> <li>\\( y_k \\) is the output for the \\( k \\)-th output neuron.</li> <li>\\( x_i \\) are the input features.</li> <li>\\( w_{ij} \\) are the weights connecting the \\( i \\)-th input to the \\( j \\)-th hidden neuron.</li> <li>\\( v_{jk} \\) are the weights connecting the \\( j \\)-th hidden neuron to the \\( k \\)-th output neuron.</li> <li>\\( b^{h}_{i} \\) is the bias for the \\( i \\)-th hidden neuron.</li> <li>\\( b^{y}_{j} \\) is the bias for the \\( j \\)-th output neuron.</li> <li>\\( m \\) is the number of hidden neurons.</li> <li>\\( n \\) is the number of input features.</li> <li>\\( \\sigma \\) is the activation function applied to the weighted sums at each layer, such as sigmoid, tanh, or ReLU.</li> </ul> <p>Matrix representation of the MLP architecture:</p> \\[ \\begin{align*} \\text{Input Layer:} &amp; \\quad \\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T \\\\ \\text{Hidden Layer:} &amp; \\quad \\mathbf{h} = \\sigma (\\mathbf{W} \\mathbf{x} + \\mathbf{b}^h) \\\\ \\text{Output Layer:} &amp; \\quad \\mathbf{y} = \\sigma (\\mathbf{V} \\mathbf{h} + \\mathbf{b}^y) \\end{align*} \\] <p>Multi-Layer Perceptron (MLP) Architecture. </p> Sigmoid Tanh ReLU \\( \\sigma(x) = \\displaystyle \\frac{1}{1 + e^{-x}} \\) \\( \\tanh(x) = \\displaystyle \\frac{e^{2x} - 1}{e^{2x} + 1} \\) \\( \\text{ReLU}(x) = \\max(0, x) \\) \\( \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) \\) \\( \\tanh'(x) = 1 - \\tanh^2(x) \\) \\( \\text{ReLU}'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases} \\) Sigmoid is a smooth, S-shaped curve that outputs values between 0 and 1, making it suitable for binary classification tasks. Tanh is a smooth curve that outputs values between -1 and 1, centering the data around zero, which can help with convergence in training. ReLU is a piecewise linear function that outputs zero for negative inputs and the input itself for positive inputs, allowing for faster training and reducing the vanishing gradient problem. <p>Backpropagation is the algorithm used to train multi-layer perceptrons (MLPs) by adjusting the weights and biases based on the error between the predicted output and the actual target. The process involves two main steps:</p> <ol> <li>Forward Pass: The input data is passed through the network, layer by layer, to compute the output. The output is compared to the target value to calculate the loss (error).</li> <li>Loss Calculation: Calculate the loss (error) between the predicted output and the actual target using a loss function, such as mean squared error or cross-entropy.</li> <li>Backward Pass: The error is propagated backward through the network to compute the gradients of the loss with respect to each weight and bias. These gradients are then used to update the weights and biases using an optimization algorithm, such as stochastic gradient descent (SGD) or Adam.</li> </ol>"}, {"location": "classes/mlp/#feedforward", "title": "Feedforward", "text": "<p>Consider a Multi-Layer Perceptron (MLP) with:</p> <ul> <li>2 input neurons: \\(x_1\\) and \\(x_2\\)</li> <li>1 hidden layer with 2 neurons: \\(h_1\\) and \\(h_2\\)</li> <li>1 output neuron: \\(y\\)</li> </ul> <p>We assume sigmoid activation functions for both the hidden and output layers:</p> \\[\\displaystyle \\sigma(z) = \\frac{1}{1 + e^{-z}}\\] <p>, with derivative</p> \\[\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\] <p>The architecture can be visualized as follows:</p> <pre><code>flowchart LR\n    classDef default fill:transparent,stroke:#333,stroke-width:1px;\n    classDef others fill:transparent,stroke:transparent,stroke-width:0px;\n    subgraph input\n        x1([\"x&lt;sub&gt;1&lt;/sub&gt;\"])\n        x2([\"x&lt;sub&gt;2&lt;/sub&gt;\"])\n        xb([\"1\"])\n    end\n    subgraph hidden\n        h1([\"h&lt;sub&gt;1&lt;/sub&gt;\"])\n        h2([\"h&lt;sub&gt;2&lt;/sub&gt;\"])\n        hb([\"1\"])\n    end\n    subgraph output\n        y([\"y\"])\n    end\n    in1@{ shape: circle, label: \" \" } --&gt; x1\n    in2@{ shape: circle, label: \" \" } --&gt; x2\n\n    x1 --&gt;|\"w&lt;sub&gt;11&lt;/sub&gt;\"|h1\n    x1 --&gt;|\"w&lt;sub&gt;12&lt;/sub&gt;\"|h2\n    xb --&gt;|\"b&lt;sup&gt;h&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;\"|h1\n    x2 --&gt;|\"w&lt;sub&gt;21&lt;/sub&gt;\"|h1\n    x2 --&gt;|\"w&lt;sub&gt;22&lt;/sub&gt;\"|h2\n    xb --&gt;|\"b&lt;sup&gt;h&lt;/sup&gt;&lt;sub&gt;2&lt;/sub&gt;\"|h2\n\n    h1 --&gt;|\"v&lt;sub&gt;11&lt;/sub&gt;\"|y\n    h2 --&gt;|\"v&lt;sub&gt;21&lt;/sub&gt;\"|y\n    hb --&gt;|\"b&lt;sup&gt;y&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;\"|y\n\n    y((\"\u0177\")) --&gt; out1@{ shape: dbl-circ, label: \" \" }\n\n    style input fill:#fff,stroke:#666,stroke-width:0px\n    style hidden fill:#fff,stroke:#666,stroke-width:0px\n    style output fill:#fff,stroke:#666,stroke-width:0px</code></pre> Multi-Layer Perceptron (MLP) Architecture. <p>In mathematical terms, the feedforward process can be described as follows:</p> \\[ \\begin{align*} \\text{Input Layer:} &amp; \\quad \\mathbf{x} = [x_1, x_2]^T \\\\ \\text{Hidden Layer:} &amp; \\quad \\mathbf{h} = \\sigma (\\mathbf{W} \\mathbf{x} + \\mathbf{b}^h) \\\\ \\text{Output Layer:} &amp; \\quad \\mathbf{y} = \\sigma (\\mathbf{V} \\mathbf{h} + \\mathbf{b}^y) \\end{align*} \\] <p>or, more canonical for our simple MLP:</p> \\[ \\hat{y} = \\sigma \\left( \\underbrace{v_{11}     \\underbrace{\\sigma \\left(         \\underbrace{w_{11} x_1 + w_{21} x_2 + b^h_1}_{z_1}     \\right)}_{h_1}     + v_{21}     \\underbrace{\\sigma \\left(         \\underbrace{w_{12} x_1 + w_{22} x_2 + b^h_2}_{z_2}     \\right)}_{h_2} + b^y_1 }_{u} \\right) \\] <ol> <li>Hidden layer pre-activation:</li> </ol> \\[ \\begin{align} z_1 &amp; = w_{11} x_1 + w_{21} x_2 + b^h_1 \\\\ z_2 &amp; = w_{12} x_1 + w_{22} x_2 + b^h_2 \\end{align} \\] <ol> <li>Hidden layer activations:</li> </ol> \\[ \\begin{align} h_1 &amp; = \\sigma(z_1) \\\\ h_2 &amp; = \\sigma(z_2) \\end{align} \\] <ol> <li>Output layer pre-activation:</li> </ol> \\[ \\begin{align} u &amp; = v_{11} h_1 + v_{21} h_2 + b^y_1 \\end{align} \\] <ol> <li>Output layer activation:</li> </ol> \\[ \\begin{align} \\hat{y} &amp; = \\sigma(u) \\end{align} \\] <p>where \\( \\sigma \\) is the activation function, \\( w_{ij} \\) are the weights connecting inputs to hidden neurons, and \\( v_{ij} \\) are the weights connecting hidden neurons to output neurons. The biases \\( b^h_1, b^h_2, \\) and \\( b^y_1 \\) are added to the respective layers. \\( \\hat{y} \\) is the predicted output of the MLP.</p>"}, {"location": "classes/mlp/#loss-calculation", "title": "Loss Calculation", "text": "<p>The loss function quantifies the difference between the predicted output and the actual target. For regression tasks, a common loss function is the Mean Squared Error (MSE):</p> \\[ L = \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 \\] <p>where \\( N \\) is the number of samples, \\( y_i \\) is the true output, and \\( \\hat{y}_i \\) is the predicted output.</p>"}, {"location": "classes/mlp/#backpropagation-computing-gradients", "title": "Backpropagation: Computing Gradients", "text": "<p>The backpropagation algorithm is a method used to train multi-layer perceptrons (MLPs) by minimizing the error between the predicted output and the actual target. It involves two main steps: the forward pass and the backward pass. The update of weights and biases is done using the gradients computed during the backward pass.</p> <p>Backpropagation computes the partial derivatives of \\(L\\) with respect to each parameter using the chain rule, starting from the output and propagating errors backward.</p>"}, {"location": "classes/mlp/#update-rule", "title": "Update Rule", "text": "<p>To update parameters during training (e.g., via gradient descent with learning rate \\(\\eta\\)), for each weight/bias \\(p\\):</p> \\[ p \\leftarrow p - \\eta \\cdot \\frac{\\partial L}{\\partial p} \\] <p>This derivation assumes a single example; for batches, average the gradients. For other activations or losses (e.g., softmax + cross-entropy), the deltas would adjust accordingly, but the chain rule structure remains similar.</p>"}, {"location": "classes/mlp/#step-1-output-layer-error", "title": "Step 1: Output Layer Error", "text": "<p>The error term (delta) for the output is:</p> \\[ \\begin{align}     \\sigma_y = \\frac{\\partial L}{\\partial u} &amp;= \\overbrace{\\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial u}}^{\\text{chain rule}} \\\\      &amp;= \\overbrace{\\frac{2}{N}(y - \\hat{y})}^{\\text{MSE}} \\cdot \\overbrace{\\sigma'(u)}^{\\text{sigmoid}} \\\\     \\\\     &amp;= \\frac{2}{N}(y - \\hat{y}) \\cdot \\hat{y}(1 - \\hat{y}) \\end{align} \\]"}, {"location": "classes/mlp/#step-2-gradients-for-output-weights-and-bias", "title": "Step 2: Gradients for Output Weights and Bias", "text": "<p>Remember</p> \\[ \\begin{align} u &amp; = v_{11} h_1 + v_{21} h_2 + b^y_1 \\end{align} \\] <p>Using \\(\\sigma_y\\):</p> <p>\\(\\begin{align}     \\frac{\\partial L}{\\partial v_{11}} &amp;= \\sigma_y \\cdot h_1 \\\\     \\\\     \\overbrace{\\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial v_{11}}}^{\\text{chain rule}} &amp;= \\sigma_y \\cdot h_1 \\end{align}\\)</p> <p>Similarly:</p> <p>\\(\\begin{align}     \\frac{\\partial L}{\\partial v_{21}} &amp;= \\sigma_y \\cdot h_2 \\\\     \\\\     \\overbrace{\\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial v_{21}}}^{\\text{chain rule}} &amp;= \\sigma_y \\cdot h_2 \\end{align}\\)</p> <p>For the bias:</p> <p>\\(\\begin{align}     \\frac{\\partial L}{\\partial b^y_1} &amp;= \\sigma_y \\cdot 1 \\\\     \\\\     \\overbrace{\\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial b^y_1}}^{\\text{chain rule}} &amp;= \\sigma_y \\end{align}\\)</p>"}, {"location": "classes/mlp/#step-3-hidden-layer-errors", "title": "Step 3: Hidden Layer Errors", "text": "<p>Remember</p> \\[ \\begin{align} z_1 &amp; = w_{11} x_1 + w_{21} x_2 + b^h_1 \\\\ z_2 &amp; = w_{12} x_1 + w_{22} x_2 + b^h_2 \\end{align} \\] <p>Propagate the error back to the hidden layer. For each hidden neuron:</p> <p>\\(\\begin{array}      \\displaystyle \\sigma_{h_1} &amp;= \\displaystyle \\frac{\\partial L}{\\partial z_1} \\\\     &amp;= \\displaystyle \\frac{\\partial L}{\\partial h_1} &amp; \\displaystyle \\cdot \\frac{\\partial h_1}{\\partial z_1} \\\\     &amp;=  \\displaystyle \\overbrace{ \\left( \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial h_1} \\right)}^{\\text{chain rule}} &amp; \\cdot \\sigma'(z_1) \\\\     &amp;= (\\sigma_y \\cdot v_1) &amp; \\cdot \\sigma'(z_1)  \\\\     &amp;= (\\sigma_y \\cdot v_1) &amp; \\cdot h_1(1 - h_1)  \\end{array}\\)</p> <p>Similarly:</p> <p>\\(\\sigma_{h_2} = (\\sigma_y \\cdot v_2) \\cdot h_2(1 - h_2)\\)</p>"}, {"location": "classes/mlp/#step-4-gradients-for-hidden-weights-and-biases", "title": "Step 4: Gradients for Hidden Weights and Biases", "text": "<p>Using the hidden deltas:</p> <p>\\(\\begin{align} \\frac{\\partial L}{\\partial w_{11}} &amp;= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{11}} &amp;= \\sigma_{h_1} \\cdot x_1 \\end{align}\\)</p> <p>\\(\\begin{align} \\frac{\\partial L}{\\partial w_{21}} &amp;= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_{21}} &amp;= \\sigma_{h_1} \\cdot x_2 \\end{align}\\)</p> <p>\\(\\begin{align} \\frac{\\partial L}{\\partial w_{12}} &amp;= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{12}} &amp;= \\sigma_{h_2} \\cdot x_1 \\end{align}\\)</p> <p>\\(\\begin{align} \\frac{\\partial L}{\\partial w_{22}}  &amp;= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_{22}} &amp;= \\sigma_{h_2} \\cdot x_2 \\end{align}\\)</p> <p>similarly for biases:</p> <p>\\(\\begin{align} \\frac{\\partial L}{\\partial b_1} &amp;= \\sigma_{h_1} \\cdot 1 &amp;= \\sigma_{h_1} \\end{align}\\)</p> <p>\\(\\begin{align} \\frac{\\partial L}{\\partial b_2} &amp;= \\sigma_{h_2} \\cdot 1 &amp;= \\sigma_{h_2} \\end{align}\\)</p>"}, {"location": "classes/mlp/#step-5-update-weights-and-biases", "title": "Step 5: Update Weights and Biases", "text": "<p>Remember</p> \\[ p \\leftarrow p - \\eta \\cdot \\frac{\\partial L}{\\partial p} \\] <p>Finally, update the weights and biases using the computed gradients and a learning rate \\(\\eta\\):</p> <p>\\(\\begin{align} v_{11} &amp; \\leftarrow v_{11} - \\eta \\cdot \\frac{\\partial L}{\\partial v_{11}} \\\\ v_{21} &amp; \\leftarrow v_{21} - \\eta \\cdot \\frac{\\partial L}{\\partial v_{21}} \\\\ \\\\ w_{11} &amp; \\leftarrow w_{11} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{11}} \\\\ w_{21} &amp; \\leftarrow w_{21} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{21}} \\\\ w_{12} &amp; \\leftarrow w_{12} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{12}} \\\\ w_{22} &amp; \\leftarrow w_{22} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{22}} \\\\ \\\\ b^h_1 &amp; \\leftarrow b^h_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b^h_1} \\\\ b^h_2 &amp; \\leftarrow b^h_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b^h_2} \\\\ \\\\ b^y_1 &amp; \\leftarrow b^y_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b^y_1} \\end{align}\\)</p>"}, {"location": "classes/mlp/#numerical-simulation", "title": "Numerical Simulation", "text": "<p>Based on the MLP architecture and backpropagation steps described above, we can implement a simple numerical simulation demonstrate the training process of a multi-layer perceptron (MLP) using backpropagation.</p>"}, {"location": "classes/mlp/#initizalization", "title": "Initizalization", "text": "<p>The weight matrices and bias vectors are initialized as follows (randomically in \\([0,1]\\)):</p> \\[ \\mathbf{W} = \\begin{bmatrix} 0.2 &amp; 0.4 \\\\ 0.6 &amp; 0.8 \\end{bmatrix}, \\quad \\mathbf{b}^h = [0.1, 0.2]^T \\] \\[ \\mathbf{V} = \\begin{bmatrix} 0.3 &amp; 0.5 \\end{bmatrix}, \\quad b^y = 0.4 \\] \\[ \\eta = 0.7 \\]"}, {"location": "classes/mlp/#forward-pass", "title": "Forward Pass", "text": "<p>For the sample:</p> \\[ \\mathbf{x} = \\begin{bmatrix} 0.5 \\\\ 0.8 \\end{bmatrix}, \\quad y = 0 \\] <ol> <li> <p>Compute hidden layer pre-activation:</p> \\[ \\begin{array}{ll} \\mathbf{z} &amp;= \\mathbf{W} \\mathbf{x} + \\mathbf{b}^h \\\\ &amp;= \\begin{bmatrix} 0.2 &amp; 0.4 \\\\ 0.6 &amp; 0.8 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 0.2*0.5 + 0.4*0.8 + 0.1 \\\\ 0.6*0.5 + 0.8*0.8 + 0.2 \\end{bmatrix} \\\\ \\mathbf{z} &amp;= \\begin{bmatrix} 0.52 \\\\ 1.14 \\end{bmatrix} \\end{array} \\] </li> <li> <p>Compute hidden layer activations:</p> \\[ \\begin{array}{ll} \\mathbf{h} &amp;= \\sigma(\\mathbf{z}) \\\\ &amp;= \\sigma \\left( \\begin{bmatrix} 0.52 \\\\ 1.14 \\end{bmatrix} \\right) \\\\ &amp;= \\begin{bmatrix} \\displaystyle \\frac{1}{1 + e^{-0.52}} \\\\ \\displaystyle \\frac{1}{1 + e^{-1.14}} \\end{bmatrix} \\\\ \\mathbf{h} &amp;\\approx \\begin{bmatrix} 0.627 \\\\ 0.758 \\end{bmatrix} \\end{array} \\] </li> <li> <p>Compute output layer pre-activation:</p> \\[ \\begin{array}{ll} u &amp;= \\mathbf{V} \\mathbf{h} + b^y \\\\ &amp;= \\begin{bmatrix} 0.3 &amp; 0.5 \\end{bmatrix} \\begin{bmatrix} 0.627 \\\\ 0.758 \\end{bmatrix} + 0.4 \\\\ &amp;= 0.3*0.627 + 0.5*0.758 + 0.4 \\\\ u &amp;\\approx 0.967 \\end{array} \\] </li> <li> <p>Compute output layer activation:</p> \\[ \\begin{array}{ll} \\hat{y} &amp;= \\sigma(u) \\\\ &amp;= \\sigma(0.967) \\\\ &amp;= \\displaystyle \\frac{1}{1 + e^{-0.967}} \\\\ \\hat{y} &amp;\\approx 0.725 \\end{array} \\] </li> </ol>"}, {"location": "classes/mlp/#loss-calculation_1", "title": "Loss Calculation", "text": "<p>Using Mean Squared Error (MSE):</p> \\[ \\begin{array}{ll} \\text{MSE} =&amp; L &amp;= \\displaystyle \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 \\\\   &amp; L &amp;= \\displaystyle \\frac{1}{1} (y - \\hat{y})^2 \\\\   &amp; L &amp;\\approx (0 - 0.725)^2 \\\\   &amp; L &amp;\\approx 0.5249 \\end{array} \\]"}, {"location": "classes/mlp/#backward-pass", "title": "Backward Pass", "text": "<ol> <li> <p>Compute output layer gradients:</p> \\[ \\begin{array}{ll} \\displaystyle \\frac{\\partial L}{\\partial u} &amp;= \\displaystyle \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial u} \\\\ &amp;= 2(y - \\hat{y}) \\cdot \\underbrace{\\hat{y} (1 - \\hat{y})}_{\\sigma'(u)} \\\\ &amp;\\approx 2(0 - 0.725) \\cdot 0.725 \\cdot (1 - 0.725) \\\\ &amp;\\approx -0.289 \\end{array} \\] </li> <li> <p>Compute hidden layer gradients:</p> \\[ \\begin{array}{ll} \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}} &amp;= \\displaystyle \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial \\mathbf{h}} \\\\ &amp;= \\displaystyle \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\mathbf{V} \\cdot \\mathbf{h} (1 - \\mathbf{h}) \\\\ &amp;\\approx -0.289 \\cdot \\begin{bmatrix} 0.3 &amp; 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.627 \\\\ 0.758 \\end{bmatrix} \\cdot \\left(1 - \\begin{bmatrix} 0.627 \\\\ 0.758 \\end{bmatrix} \\right) \\\\ &amp;\\approx -0.289 \\cdot \\underbrace{ \\begin{bmatrix} 0.3 &amp; 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.627 \\\\ 0.758 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.373 \\\\ 0.242 \\end{bmatrix} }_{\\text{element-wise multiplication}} \\\\ &amp;\\approx -0.289 \\cdot \\begin{bmatrix} 0.07 \\\\ 0.092 \\end{bmatrix} \\\\ &amp;\\approx \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\end{array} \\] </li> <li> <p>Compute weight gradients:</p> \\[ \\begin{array}{ll} \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}} &amp;= \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{W}} \\\\ &amp;= \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\cdot \\mathbf{x} \\\\ &amp;\\approx \\underbrace{ \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 &amp; 0.5 \\\\ 0.8 &amp; 0.8 \\end{bmatrix} }_{\\text{element-wise multiplication}} \\\\ &amp;\\approx \\begin{bmatrix} -0.010 &amp; -0.013 \\\\ -0.016 &amp; -0.021 \\end{bmatrix} \\end{array} \\] </li> <li> <p>Compute bias gradients:</p> \\[ \\begin{array}{ll} \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b^y}} &amp;= \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{b^y}} \\\\ &amp;\\approx -0.289 \\cdot 1 \\\\ &amp;\\approx -0.289 \\end{array} \\] \\[ \\begin{array}{ll} \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b^h}} &amp;= \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{b^h}} \\\\ &amp;\\approx \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\cdot 1 \\\\ &amp;\\approx \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\end{array} \\] </li> <li> <p>Update the parameters:</p> \\[ \\begin{array}{ll} \\mathbf{W} &amp;\\leftarrow \\mathbf{W} - \\eta \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}} \\\\ &amp;\\leftarrow \\displaystyle \\begin{bmatrix} 0.2 &amp; 0.4 \\\\ 0.6 &amp; 0.8 \\end{bmatrix} - 0.7 \\cdot \\displaystyle \\begin{bmatrix} -0.010 &amp; -0.013 \\\\ -0.016 &amp; -0.021 \\end{bmatrix} \\\\ &amp;\\leftarrow \\begin{bmatrix} 0.207 &amp; 0.409 \\\\ 0.611 &amp; 0.815 \\end{bmatrix} \\\\ \\mathbf{V} &amp;\\leftarrow \\mathbf{V} - \\eta \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{V}} \\\\ &amp;\\leftarrow \\displaystyle \\begin{bmatrix} 0.3 &amp; 0.5 \\end{bmatrix} - 0.7 \\cdot \\displaystyle \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\\\ &amp;\\leftarrow \\begin{bmatrix} 0.314 &amp; 0.518 \\end{bmatrix}\\\\ \\\\ \\mathbf{b^y} &amp;\\leftarrow \\mathbf{b^y} - \\eta \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b^y}} \\\\ &amp;\\leftarrow 0.4 - 0.7 \\cdot (-0.289) \\\\ &amp;\\leftarrow 0.602 \\\\ \\mathbf{b^h} &amp;\\leftarrow \\mathbf{b^h} - \\eta \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b^h}} \\\\ &amp;\\leftarrow \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix} - 0.7 \\cdot \\begin{bmatrix} -0.020 \\\\ -0.026 \\end{bmatrix} \\\\ &amp;\\leftarrow \\begin{bmatrix} 0.114 \\\\ 0.218 \\end{bmatrix} \\end{array} \\] </li> <li> <p>Repeat the training process for each sample or multiple epochs. About the training process, there are two main approaches: online learning and batch learning:</p> <ul> <li> <p>Online learning is a method of training multi-layer perceptrons (MLPs) where the model is updated after each training example. This approach allows for faster convergence and can be more effective in scenarios with large datasets or when the data is not stationary.</p> </li> <li> <p>Batch learning, on the other hand, involves updating the model after processing a batch of training examples. This method can lead to more stable updates and is often used in practice due to its efficiency in utilizing computational resources.</p> </li> </ul> </li> </ol>"}, {"location": "classes/mlp/#additional", "title": "Additional", "text": "<p>For a more intuitive understanding of neural networks, I highly recommend the following video series by 3Blue1Brown, which provides excellent visual explanations of the concepts: https://www.3blue1brown.com/lessons/neural-networks</p> <ol> <li> <p>Haykin, S. (1994). Neural Networks: A Comprehensive Foundation. Prentice Hall.  \u21a9</p> </li> <li> <p>Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.  \u21a9</p> </li> <li> <p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.  \u21a9</p> </li> <li> <p>Physics of Neural Networks, Book Series.\u00a0\u21a9</p> </li> <li> <p>Introduction to Mathematical Optimization, by Indrajit Ghosh.\u00a0\u21a9</p> </li> <li> <p>TensorFlow Playground \u21a9</p> </li> </ol>"}, {"location": "classes/optimization/", "title": "7. Optimization", "text": "<p>Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning models. It works by iteratively adjusting the model parameters in the direction of the steepest descent of the loss function, as defined by the negative gradient.</p> <p>The main idea behind gradient descent is to update the model parameters in the opposite direction of the gradient of the loss function with respect to the parameters. This is done using the following update rule:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\nabla J(\\theta_t) \\] <p>Vanilla Gradient Descent</p> <p>where:</p> <ul> <li>\\(\\theta\\) represents the model parameters,</li> <li>\\(\\eta\\) is the learning rate (a hyperparameter that controls the step size),</li> <li>\\(\\nabla J(\\theta)\\) is the gradient of the loss function with respect to the parameters.</li> </ul> <p>In the standard Supervised Learning paradigm, the loss (per sample) is simply the output of the cost function. Machine Learning is mostly about optimizing functions (usually minimizing them). It could also involve finding Nash Equilibria between two functions like with GANs. This is done using Gradient Based Methods, though not necessarily Gradient Descent<sup>1</sup>.</p> <p>A Gradient Based Method is a method/algorithm that finds the minima of a function, assuming that one can easily compute the gradient of that function. It assumes that the function is continuous and differentiable almost everywhere (it need not be differentiable everywhere)<sup>1</sup>.</p> <p>Gradient Descent Intuition - Imagine being in a mountain in the middle of a foggy night. Since you want to go down to the village and have only limited vision, you look around your immediate vicinity to find the direction of steepest descent and take a step in that direction<sup>1</sup>.</p> <p>To visualize the gradient descent process, we can create a simple 3D plot that shows how the parameters of a model are updated over iterations to minimize a loss function. Below is an example code using Python with Matplotlib to create such a visualization.</p> <p> </p> Once Loop Reflect <p></p>"}, {"location": "classes/optimization/#gradient-descent-variants", "title": "Gradient Descent Variants", "text": "<p>There are several variants of gradient descent, each with its own characteristics:</p> <ol> <li> <p>Batch Gradient Descent: Computes the gradient using the entire dataset. It provides a stable estimate of the gradient but can be slow for large datasets. Formula is given by:</p> \\[ \\theta = \\theta - \\eta \\frac{1}{N} \\sum_{i=1}^{N} \\nabla J(\\theta; x^{(i)}, y^{(i)}) \\] <pre><code>for epoch in range(num_epochs):\n    gradients = compute_gradients(X, y, model)\n    model.parameters -= learning_rate * gradients\n</code></pre> </li> <li> <p>Stochastic Gradient Descent (SGD): Computes the gradient using a single data point at a time. It introduces noise into the optimization process, which can help escape local minima but may lead to oscillations.  Formula is given by:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\nabla J(\\theta_t; x^{(i)}, y^{(i)}) \\] <pre><code>for epoch in range(num_epochs):\n    for i in range(len(X)):\n        gradients = compute_gradients(X[i], y[i], model)\n        model.parameters -= learning_rate * gradients\n</code></pre> </li> <li> <p>Mini-Batch Gradient Descent: Combines the advantages of batch and stochastic gradient descent by using a small random subset of the data (mini-batch) to compute the gradient. It balances the stability of batch gradient descent with the speed of SGD.</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{1}{B} \\sum_{i=1}^{B} \\nabla J(\\theta_t; x^{(i)}, y^{(i)}) \\] <pre><code>for epoch in range(num_epochs):\n    for batch in get_mini_batches(X, y, batch_size):\n        gradients = compute_gradients(batch.X, batch.y, model)\n        model.parameters -= learning_rate * gradients\n</code></pre> </li> </ol>"}, {"location": "classes/optimization/#momentum", "title": "Momentum", "text": "<p>Momentum is a variant of gradient descent that helps accelerate the optimization process by using the past gradients to smooth out the updates. It introduces a \"momentum\" term that accumulates the past gradients and adds it to the current gradient update. The update rule with momentum is given by:</p> <p>In Momentum, we have two iterates (\\(p\\) and \\(\\theta\\)) instead of just one. The updates are as follows:</p> \\[ V_{t+1} = \\beta V_t + (1 - \\beta) \\nabla J(\\theta_t) \\] \\[ \\theta_{t+1} = \\theta_t - \\eta V_{t+1} \\] <p>\\(V\\) is called the SGD momentum. At each update step we add the stochastic gradient to the old value of the momentum, after dampening it by a factor \\(\\beta\\) (value between 0 and 1). \\(V\\) can be thought of as a running average of the gradients. Finally we move \\(\\theta\\) in the direction of the new momentum \\(V\\) <sup>1</sup>.</p> <p>Alternate Form: Stochastic Heavy Ball Method</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\nabla f_i(\\theta_t) + \\beta( \\theta_t - \\theta_{t-1} ) \\quad 0 \\leq \\beta &lt; 1 \\] <p>This form is mathematically equivalent to the previous form. Here, the next step is a combination of previous step\u2019s direction and the new negative gradient.</p> <p>The momentum term helps to dampen oscillations and can lead to faster convergence, especially in scenarios with noisy gradients or ravines in the loss landscape.</p>"}, {"location": "classes/optimization/#rmsprop", "title": "RMSProp", "text": "<p>RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to address the diminishing learning rates of AdaGrad. It maintains a moving average of the squared gradients and uses this to normalize the gradients. The update rule is given by:</p> \\[ V_{t+1} = \\beta V_t + (1 - \\beta) \\nabla (\\theta_t)^2 \\] \\[ \\theta_{t+1} = \\theta_t - \\eta\\frac{\\nabla (\\theta_t)}{\\sqrt{V_{t+1} + \\epsilon}} \\] <p>Where:</p> <ul> <li>\\(V_t\\) is the moving average of the squared gradients,</li> <li>\\(\\epsilon\\) is a small constant added for numerical stability, helping to prevent division by zero.</li> </ul>"}, {"location": "classes/optimization/#adam-optimizer", "title": "ADAM Optimizer", "text": "<p>ADAM (Adaptive Moment Estimation) is an advanced optimization algorithm that combines the benefits of both AdaGrad and RMSProp. It maintains two moving averages for each parameter: the first moment (mean) and the second moment (uncentered variance). The update rules are as follows:</p> <ol> <li>Compute the gradients:</li> </ol> \\[ g_t = \\nabla J(\\theta_t) \\] <ol> <li>Update the first moment estimate:</li> </ol> \\[ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\] <ol> <li>Update the second moment estimate:</li> </ol> \\[ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\] <ol> <li>Compute bias-corrected estimates:</li> </ol> \\[ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\] \\[ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\] <ol> <li>Update the parameters:</li> </ol> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\] <p>Where:</p> <ul> <li>\\(m_t\\) is the first moment (the mean of the gradients),</li> <li>\\(v_t\\) is the second moment (the uncentered variance of the gradients),</li> <li>\\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters that control the decay rates of the moving averages,</li> <li>\\(\\epsilon\\) is a small constant added for numerical stability. The default values for the hyperparameters are typically set to \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), and \\(\\epsilon = 10^{-8}\\)<sup>2</sup><sup>3</sup>.</li> </ul> <p>ADAM is widely used in practice due to its adaptive learning rate properties and is particularly effective for training deep learning models.</p>"}, {"location": "classes/optimization/#comparison", "title": "Comparison", "text": "<p>Here's a tabular comparison to highlight key differences:</p> Gradient Descent (GD) Stochastic Gradient Descent (SGD) Momentum ADAM Computational Cost per Update High (full dataset) Low (mini-batch) Low (similar to SGD) Medium (stores moments) Convergence Speed Slow (few updates) Medium (many noisy updates) Fast (accelerates in consistent directions) Fast (adaptive + momentum) Stability/Noise High stability, low noise Low stability, high noise Medium stability, reduced oscillations High stability, low effective noise Adaptivity None (fixed \u03b7) None (fixed \u03b7, but can schedule) Low (momentum helps indirectly) High (per-parameter adaptation) Best For Small datasets, convex problems Large datasets, online learning Noisy gradients, ravine-like landscapes Complex MLPs, sparse data Common Hyperparameters Learning rate (\u03b7) Learning rate (\u03b7), batch size Learning rate (\u03b7), momentum (\u03b2 ~0.9) Learning rate (\u03b7 ~0.001), \u03b21 (0.9), \u03b22 (0.999), \u03b5 Limitations Scalability issues; stuck in local minima Oscillations; requires tuning Can overshoot; still fixed \u03b7 Potential overfitting; higher memory <p>In summary,</p> <ul> <li>GD is basic but inefficient;</li> <li>SGD adds speed at the cost of noise; </li> <li>Momentum smooths SGD; </li> <li>and, ADAM offers the most automation and efficiency for MLPs, though SGD/Momentum may edge out in generalization for fine-tuned tasks.</li> </ul> <p> </p> Once Loop Reflect <p></p> <p>Choice depends on dataset size, computational resources, and problem complexity\u2014experiment with validation sets for best results.</p>"}, {"location": "classes/optimization/#additional-resources", "title": "Additional Resources", "text": "<ol> <li> <p>Introduction to Gradient Descent and Backpropagation Algorithm, LeCun, Y.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>ADAM: A Method for Stochastic Optimization, Kingma, D. P., &amp; Ba, J.\u00a0\u21a9</p> </li> <li> <p>Dive into Deep Learning, Zhang, A., &amp; Lipton, Z. C.\u00a0\u21a9</p> </li> <li> <p>Stochastic and Mini-batch Gradient Descent \u21a9</p> </li> </ol>"}, {"location": "classes/optimization/comparison/", "title": "Comparison", "text": ""}, {"location": "classes/optimization/comparison/#comparison", "title": "Comparison", "text": "<p>Here's a tabular comparison to highlight key differences:</p> Gradient Descent (GD) Stochastic Gradient Descent (SGD) Momentum ADAM Computational Cost per Update High (full dataset) Low (mini-batch) Low (similar to SGD) Medium (stores moments) Convergence Speed Slow (few updates) Medium (many noisy updates) Fast (accelerates in consistent directions) Fast (adaptive + momentum) Stability/Noise High stability, low noise Low stability, high noise Medium stability, reduced oscillations High stability, low effective noise Adaptivity None (fixed \u03b7) None (fixed \u03b7, but can schedule) Low (momentum helps indirectly) High (per-parameter adaptation) Best For Small datasets, convex problems Large datasets, online learning Noisy gradients, ravine-like landscapes Complex MLPs, sparse data Common Hyperparameters Learning rate (\u03b7) Learning rate (\u03b7), batch size Learning rate (\u03b7), momentum (\u03b2 ~0.9) Learning rate (\u03b7 ~0.001), \u03b21 (0.9), \u03b22 (0.999), \u03b5 Limitations Scalability issues; stuck in local minima Oscillations; requires tuning Can overshoot; still fixed \u03b7 Potential overfitting; higher memory <p>In summary,</p> <ul> <li>GD is basic but inefficient;</li> <li>SGD adds speed at the cost of noise; </li> <li>Momentum smooths SGD; </li> <li>and, ADAM offers the most automation and efficiency for MLPs, though SGD/Momentum may edge out in generalization for fine-tuned tasks.</li> </ul> <p> </p> Once Loop Reflect <p></p> <p>Choice depends on dataset size, computational resources, and problem complexity\u2014experiment with validation sets for best results.</p>"}, {"location": "classes/perceptron/", "title": "5. Perceptron", "text": ""}, {"location": "classes/perceptron/#biological-inspiration", "title": "Biological Inspiration", "text": "<p>Since the inception of artificial neural networks (ANNs), their design has been heavily influenced by the structure and function of biological neural networks. The human brain, with its complex network of neurons, serves as a foundational model for understanding how ANNs can process information. The base of an ANN is a neuron, which mimics the behavior of biological neurons. Each neuron receives inputs, processes them, and produces an output, similar to how biological neurons communicate through synapses.</p> <p></p> <p>Diagram of Neuron. Source: Wikipedia - Neuron</p> <p>The biological neuron consists of a cell body (soma), dendrites, and an axon. Dendrites receive signals from other neurons, the soma processes these signals, and the axon transmits the output to other neurons. This structure allows for complex interactions and information processing, which is essential for learning and decision-making in biological systems. The signal produced by a neuron is known as an action potential, which is an electrical impulse that travels along the axon to communicate with other neurons. The action potential is generated when the neuron receives sufficient input from its dendrites, leading to a change in the electrical potential across its membrane. This process is known as neural activation and is crucial for the functioning of both biological and artificial neurons.</p> <p></p> <p>Action potential. Source: Wikipedia - Action potential</p> <p>Basead on this biological inspiration, McCulloch and Pitts<sup>1</sup> proposed the first mathematical model of a neuron in 1943. This model laid the groundwork for the development of artificial neurons, which are the building blocks of ANNs. The McCulloch-Pitts neuron is a simple binary model that outputs a signal based on whether the weighted sum of its inputs exceeds a certain threshold.</p> \\[ \\begin{align*}     N_i(t+1) = H \\left( \\sum_{j=1}^n w_{ij}(t) N_j(t) - \\theta_ i (t) \\right), &amp; \\\\     &amp; H(x) :=      \\begin{cases}         1, &amp; x \\geq 0 \\\\         0, &amp; x &lt; 0     \\end{cases} \\end{align*} \\] <p>This equation describes how the output of neuron \\(N_i\\) at time \\(t+1\\) is determined by the weighted sum of its inputs \\(N_j\\) at time \\(t\\), adjusted by a threshold \\(\\theta_i(t)\\). The function \\(H\\) is a step function that activates the neuron if the input exceeds the threshold. Note, the results of the McCulloch-Pitts model are binary, meaning the output is either 0 or 1, which corresponds to the neuron being inactive or active, respectively - see a connection to symbolic logic.</p> <p>This model, while simplistic, captures the essence of how neurons process information and has been foundational in the development of more complex neural network architectures.</p>"}, {"location": "classes/perceptron/#mathematical-foundations", "title": "Mathematical Foundations", "text": "<p>The mathematical foundations of artificial neural networks are built upon linear algebra, calculus, and probability theory. These areas provide the tools necessary to understand how ANNs operate, how they learn from data, and how they can be optimized for various tasks.</p> <p>The Perceptron, introduced by Rosenblatt<sup>2</sup> in 1958, is one of the earliest and simplest forms of an ANN. It consists of a single layer of neurons that can classify linearly separable data. The Perceptron algorithm adjusts the weights of the inputs based on the error in the output, allowing it to learn from examples.</p> <p>The Perceptron can be mathematically described as follows:</p> <pre><code>flowchart LR\n    classDef default fill:#fff,stroke:#333,stroke-width:1px;\n    subgraph input[\"inputs\"]\n        direction TB\n        x1([\"x&lt;sub&gt;1&lt;/sub&gt;\"])\n        x2([\"x&lt;sub&gt;2&lt;/sub&gt;\"])        \n    end\n    subgraph hidden[\" \"]\n        direction LR\n        S([\"\u03a3\"])\n        subgraph af[\"activation function\"]\n            f([\"\u0192(\u03a3)\"])\n        end\n    end\n    subgraph bias[\"bias\"]\n        direction TB\n        b1([\"b\"])\n    end\n    in1@{ shape: circle, label: \" \" } --&gt; x1\n    in2@{ shape: circle, label: \" \" } --&gt; x2\n    x1 --&gt;|\"w&lt;sub&gt;1&lt;/sub&gt;\"| S\n    x2 --&gt;|\"w&lt;sub&gt;2&lt;/sub&gt;\"| S\n    b1 --&gt; S\n    S --&gt; f\n    subgraph outputs[\"output\"]\n        direction TB\n        out1\n    end\n    f --&gt; out1@{ shape: dbl-circ, label: \"y\" }\n    style input fill:#fff,stroke:#fff,stroke-width:0px\n    style bias fill:#fff,stroke:#fff,stroke-width:0px\n    style hidden fill:#fff,stroke:#fff,stroke-width:0px\n    style outputs fill:#fff,stroke:#fff,stroke-width:0px</code></pre> <p>The Perceptron computes the output \\(y\\) as follows:</p> \\[ y = \\text{activation}\\left(\\sum_{i=1}^n w_i x_i + b\\right) \\] <p>A perceptron is a simple artificial neuron that takes multiple inputs, applies weights to them, sums them up, adds a bias, and passes the result through an activation function (typically a step function) to produce a binary output (e.g., 0 or 1). It\u2019s used to solve linearly separable classification problems.</p> <p>The perceptron\u2019s output is computed as:</p> <ul> <li> <p>Input: a vector of features \\( \\mathbf{x} = [x_1, x_2, \\dots, x_n] \\).</p> </li> <li> <p>Weights: a vector \\( \\mathbf{w} = [w_1, w_2, \\dots, w_n] \\) representing the importance of each input.</p> </li> <li> <p>Bias: a scalar \\( b \\) that shifts the decision boundary.</p> </li> <li> <p>Output:</p> \\[ y = \\text{activation}(\\mathbf{w} \\cdot \\mathbf{x} + b) \\] <p>where \\( \\mathbf{w} \\cdot \\mathbf{x} = w_1x_1 + w_2x_2 + \\dots + w_nx_n \\), and the activation function is typically a step function:</p> \\[ \\text{activation}(z) =  \\begin{cases}  1 &amp; \\text{if } z \\geq 0 \\\\ 0 &amp; \\text{if } z &lt; 0  \\end{cases} \\] </li> </ul> <p>The goal of training is to find the optimal weights \\( \\mathbf{w} \\) and bias \\( b \\) so the perceptron correctly classifies the training data.</p>"}, {"location": "classes/perceptron/#perceptron-training-process", "title": "Perceptron Training Process", "text": "<p>The perceptron training algorithm adjusts the weights and bias iteratively based on errors in classification. Here\u2019s a step-by-step explanation:</p>"}, {"location": "classes/perceptron/#1-initialize-weights-and-bias", "title": "1. Initialize Weights and Bias", "text": "<p>Start with small random values for the weights \\( \\mathbf{w} \\) and bias \\( b \\), or initialize them to zero. These initial values don\u2019t need to be perfect, as the algorithm will adjust them during training.</p>"}, {"location": "classes/perceptron/#2-provide-training-data", "title": "2. Provide Training Data", "text": "<p>The training dataset consists of input-output pairs \\( \\{(\\mathbf{x}^{(i)}, y^{(i)})\\} \\), where:</p> <ul> <li>\\( \\mathbf{x}^{(i)} \\) is the feature vector for the \\( i \\)-th example.</li> <li>\\( y^{(i)} \\) is the true label (0 or 1).</li> </ul> <p>The data must be linearly separable for the perceptron to converge (i.e., a straight line or hyperplane can separate the two classes).</p>"}, {"location": "classes/perceptron/#3-forward-pass-compute-prediction", "title": "3. Forward Pass: Compute Prediction", "text": "<p>For each training example \\( \\mathbf{x}^{(i)} \\):</p> <p>Calculate the weighted sum:</p> <p>\\( z = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\).</p> <p>Apply the activation function to get the predicted output \\( \\hat{y}^{(i)} \\):</p> \\[ \\hat{y}^{(i)} = \\text{activation}(z) \\] <p>Compare the predicted output \\( \\hat{y}^{(i)} \\) with the true label \\( y^{(i)} \\).</p>"}, {"location": "classes/perceptron/#4-compute-error", "title": "4. Compute Error", "text": "<p>The error is the difference between the true label and the predicted label:</p> \\[ \\text{error} = y^{(i)} - \\hat{y}^{(i)} \\] <p>Since \\( y^{(i)} \\) and \\( \\hat{y}^{(i)} \\) are binary (0 or 1), the error can be:</p> <ul> <li>\\( 0 \\) (correct prediction),</li> <li>\\( 1 \\) (predicted 0, but true label is 1),</li> <li>\\( -1 \\) (predicted 1, but true label is 0).</li> </ul>"}, {"location": "classes/perceptron/#5-update-weights-and-bias", "title": "5. Update Weights and Bias", "text": "<p>If the prediction is correct (\\( \\text{error} = 0 \\)), no update is needed.</p> <p>If the prediction is incorrect, adjust the weights and bias using the perceptron learning rule<sup>4</sup>:</p> \\[ \\mathbf{w} \\gets \\mathbf{w} + \\eta \\cdot \\text{error} \\cdot \\mathbf{x}^{(i)} \\] \\[ b \\gets b + \\eta \\cdot \\text{error} \\] <p>where:</p> <p>\\( \\eta \\) (eta) is the learning rate, a small positive number (e.g., 0.01) that controls the size of the update.</p> <p>The update moves the decision boundary to reduce the error for the current example.</p>"}, {"location": "classes/perceptron/#6-iterate", "title": "6. Iterate", "text": "<p>Repeat steps 3\u20135 for all training examples in the dataset (one pass through the dataset is called an epoch).</p> <p>Stop Criteria</p> <p>Continue iterating for a fixed number of epochs or until the perceptron correctly classifies all training examples (i.e., no errors).</p>"}, {"location": "classes/perceptron/#7-convergence", "title": "7. Convergence", "text": "<p>If the data is linearly separable, the perceptron is guaranteed to converge to a solution that correctly classifies all training examples.</p> <p>Stop Criteria</p> <p>If the data is not linearly separable, the algorithm will not converge and may oscillate. In such cases, you may need to limit the number of epochs or use a different model.</p>"}, {"location": "classes/perceptron/#intuition-behind-the-training", "title": "Intuition Behind the Training", "text": "<ul> <li>The perceptron learns by adjusting the decision boundary (a hyperplane defined by \\( \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\)) to separate the two classes.</li> <li>Each weight update moves the hyperplane slightly to reduce misclassification errors.</li> <li>The learning rate \\( \\eta \\) controls how aggressively the hyperplane is adjusted:</li> <li>A large \\( \\eta \\) makes big changes, which can lead to faster learning but may overshoot.</li> <li>A small \\( \\eta \\) makes small changes, leading to slower but more stable learning.</li> </ul>"}, {"location": "classes/perceptron/#example-training-a-perceptron", "title": "Example: Training a Perceptron", "text": "<p>Suppose we have a dataset with two features \\( \\mathbf{x} = [x_1, x_2] \\) and binary labels (0 or 1). Let\u2019s train a perceptron to classify points.</p>"}, {"location": "classes/perceptron/#dataset", "title": "Dataset:", "text": "2025-11-06T12:10:36.161872 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ \\( x_1 \\) \\( x_2 \\) Label (\\( y \\)) 1 1 1 2 2 1 -1 -1 0 -2 -1 0"}, {"location": "classes/perceptron/#step-by-step", "title": "Step-by-Step:", "text": "<ol> <li> <p>Initialize: Set \\( \\mathbf{w} = [0, 0] \\), \\( b = 0 \\), and \\( \\eta = 0.1 \\).</p> </li> <li> <p>First Example: \\( \\mathbf{x}^{(1)} = [1, 1] \\), \\( y^{(1)} = 1 \\).</p> <ul> <li>Compute: \\( z = \\underbrace{0}_{w_1} \\cdot \\underbrace{1}_{x_1} + \\underbrace{0}_{w_2} \\cdot \\underbrace{1}_{x_2} + \\underbrace{0}_{b} = 0 \\), so \\( \\hat{y}^{(1)} = 1 \\) (since \\( z \\geq 0 \\)).</li> <li>Error: \\( y^{(1)} - \\hat{y}^{(1)} = 1 - 1 = 0 \\). No update needed.</li> </ul> </li> <li> <p>Second Example: \\( \\mathbf{x}^{(2)} = [2, 2] \\), \\( y^{(2)} = 1 \\).</p> <ul> <li>Compute: \\( z = \\underbrace{0}_{w_1} \\cdot \\underbrace{2}_{x_1} + \\underbrace{0}_{w_2} \\cdot \\underbrace{2}_{x_2} + \\underbrace{0}_{b} = 0 \\), so \\( \\hat{y}^{(2)} = 1 \\).</li> <li>Error: \\( \\underbrace{1}_{y^{(1)}} - \\underbrace{1}_{\\hat{y}^{(1)}} = 0 \\). No update.</li> </ul> </li> <li> <p>Third Example: \\( \\mathbf{x}^{(3)} = [-1, -1] \\), \\( y^{(3)} = 0 \\).</p> <ul> <li>Compute: \\( z = \\underbrace{0}_{w_1} \\cdot \\underbrace{-1}_{x_1} + \\underbrace{0}_{w_2} \\cdot \\underbrace{-1}_{x_2} + \\underbrace{0}_{b} = 0 \\), so \\( \\hat{y}^{(3)} = 1 \\).</li> <li>Error: \\( \\underbrace{0}_{y^{(1)}} - \\underbrace{1}_{\\hat{y}^{(1)}} = -1 \\).</li> <li>Update:</li> </ul> \\[ \\mathbf{w} = \\underbrace{[0, 0]}_{\\mathbf{w}} + \\underbrace{0.1}_{\\eta} \\cdot \\underbrace{-1}_{\\text{error}} \\cdot \\underbrace{[-1, -1]}_{\\mathbf{x}^{(3)}} = [0, 0] + [0.1, 0.1] = [0.1, 0.1] \\] \\[ b = \\underbrace{0}_{b} + \\underbrace{0.1}_{\\eta} \\cdot \\underbrace{-1}_{\\text{error}} = -0.1 \\] </li> <li> <p>Fourth Example: \\( \\mathbf{x}^{(4)} = [-2, -1] \\), \\( y^{(4)} = 0 \\).</p> <ul> <li> <p>Compute: \\( z = \\underbrace{0.1}_{w_1} \\cdot \\underbrace{-2}_{x_1} + \\underbrace{0.1}_{w_2} \\cdot \\underbrace{-1}_{x_2} + \\underbrace{-0.1}_{b} = -0.4 \\), so \\( \\hat{y}^{(4)} = 0 \\).</p> </li> <li> <p>Error: \\( \\underbrace{0}_{y^{(4)}} - \\underbrace{0}_{\\hat{y}^{(4)}} = 0 \\). No update.</p> </li> </ul> </li> <li> <p>Repeat: Continue iterating through the dataset, updating weights and bias when errors occur, until all examples are correctly classified or a maximum number of epochs is reached.</p> </li> </ol> <p> </p> Once Loop Reflect <p></p>"}, {"location": "classes/perceptron/#key-points", "title": "Key Points", "text": "<ul> <li>Linear Separability: The perceptron only works for datasets where a single hyperplane can separate the classes. For non-linearly separable data (e.g., XOR problem), a single perceptron fails, and you\u2019d need a multi-layer perceptron (MLP) or other models.</li> <li>Learning Rate: Choosing an appropriate \\( \\eta \\) is crucial. Too large, and the algorithm may oscillate; too small, and it may converge too slowly.</li> <li>Convergence: The perceptron convergence theorem guarantees that the algorithm will find a solution in a finite number of steps if the data is linearly separable.</li> <li>Limitations: The perceptron is a simple model and cannot handle complex patterns or multi-class problems without extensions (e.g., combining multiple perceptrons).</li> </ul>"}, {"location": "classes/perceptron/#visualizing-the-decision-boundary", "title": "Visualizing the Decision Boundary", "text": "<p>The decision boundary is the hyperplane where \\( \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\). For a 2D dataset (\\( x_1, x_2 \\)), this is a line:</p> \\[ w_1x_1 + w_2x_2 + b = 0 \\implies x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{b}{w_2} \\] <p>During training, the weights and bias are adjusted to move this line so that it separates the two classes correctly.</p>"}, {"location": "classes/perceptron/#practical-considerations", "title": "Practical Considerations", "text": "<ul> <li>Preprocessing: Normalize or standardize input features to ensure they\u2019re on similar scales, which helps the perceptron learn more effectively.</li> <li>Epochs: Set a maximum number of epochs to prevent infinite loops if the data is not linearly separable.</li> <li>Extensions: To handle multi-class problems, you can use multiple perceptrons (one-vs-rest) or move to more advanced models like MLPs or support vector machines (SVMs).</li> </ul>"}, {"location": "classes/perceptron/#limitations-of-the-perceptron", "title": "Limitations of the Perceptron", "text": "<p>While the perceptron is a foundational model in machine learning, it has several limitations:</p> <ol> <li> <p>Linearly Separable Data: The perceptron can only solve problems where the classes are linearly separable. If the data cannot be separated by a straight line (or hyperplane in higher dimensions), the perceptron will fail to converge. A classic example of this limitation is the XOR problem<sup>5</sup>, which cannot be solved by a single-layer perceptron.</p> 2025-11-06T12:10:36.381320 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </li> <li> <p>Binary Classification: The basic perceptron is designed for binary classification tasks. While it can be extended to multi-class problems using techniques like one-vs-rest, it is not inherently suited for multi-class classification.</p> </li> <li>No Probability Estimates: The perceptron outputs binary decisions (0 or 1) without providing probability estimates for its predictions. This can be a limitation in applications where understanding the confidence of predictions is important.</li> <li>Sensitivity to Learning Rate: The choice of learning rate can significantly affect the training process. A learning rate that is too high can cause the algorithm to overshoot the optimal solution, while a learning rate that is too low can lead to slow convergence.</li> </ol>"}, {"location": "classes/perceptron/#summary", "title": "Summary", "text": "<p>The perceptron training algorithm is a simple, iterative process that adjusts weights and bias to minimize classification errors on a linearly separable dataset. It involves initializing parameters, computing predictions, calculating errors, and updating weights based on the perceptron learning rule. While limited to binary classification and linearly separable data, it\u2019s a foundational concept for understanding more complex neural networks.</p> <p></p> <ol> <li> <p>McCulloch, W. S., &amp; Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, 5(4), 115-133. doi:10.1007/BF02478259 \u21a9</p> </li> <li> <p>Rosenblatt, F. (1958). The Perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386-408. doi:10.1037/h0042519 \u21a9</p> </li> <li> <p>Jurafsky, D., &amp; Martin, J. H. (2025). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models.  \u21a9</p> </li> <li> <p>Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. John Wiley &amp; Sons. doi.org/10.1002/sce.37303405110 \u21a9</p> </li> <li> <p>Minsky, M., &amp; Papert, S. (1969). Perceptrons: An introduction to computational geometry. MIT Press.  ,\u00a0\u21a9</p> </li> </ol>"}, {"location": "classes/preprocessing/", "title": "3. Preprocessing", "text": "<p>Data preprocessing is a critical phase in the development of neural network models, ensuring that raw data is transformed into a suitable format for effective training and inference. This text explores both basic and advanced preprocessing techniques, drawing from established methodologies in machine learning and deep learning. Basic techniques focus on cleaning and normalizing data to handle inconsistencies and scale issues, while advanced methods address complex challenges such as data scarcity, imbalance, and high dimensionality. The discussion highlights their relevance to neural networks, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers, with emphasis on improving model convergence, generalization, and performance.</p> <p>Neural networks, as powerful function approximators, are highly sensitive to the quality and format of input data. Poorly prepared data can lead to slow convergence, overfitting, or suboptimal accuracy. Preprocessing mitigates these issues by addressing noise, inconsistencies, and structural mismatches in datasets. It encompasses a series of steps that transform raw data into a form that aligns with the assumptions and requirements of neural architectures. For instance, in supervised learning tasks, preprocessing ensures features are scaled appropriately to prevent gradient issues during backpropagation. This text delineates basic techniques, which are foundational and widely applicable, and advanced techniques, which are more specialized and often domain-specific, such as for image, text, or time-series data.</p>"}, {"location": "classes/preprocessing/#typical-preprocessing-tasks", "title": "Typical Preprocessing Tasks", "text": "Task Description Text Cleaning Remove unwanted characters, stop words, and perform stemming/lemmatization. Normalization Standardize text formats, such as date and currency formats. Tokenization Split text into words or subwords for easier analysis. Feature Extraction Convert text into numerical features using techniques like TF-IDF or word embeddings. Data Augmentation Generate synthetic data to increase dataset size and diversity. <p>A typical dataset for machine learning tasks might include columns of different data types, such as numerical, categorical, and text, eg.:</p> PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 7 0 1 McCarthy, Mr. Timothy J male 54 0 0 17463 51.8625 E46 S 169 0 1 Baumann, Mr. John D male nan 0 0 PC 17318 25.925 nan S 462 0 3 Morley, Mr. William male 34 0 0 364506 8.05 nan S 310 1 1 Francatelli, Miss. Laura Mabel female 30 0 0 PC 17485 56.9292 E36 C 552 0 2 Sharp, Mr. Percival James R male 27 0 0 244358 26 nan S 399 0 2 Pain, Dr. Alfred male 23 0 0 244278 10.5 nan S 264 0 1 Harrison, Mr. William male 40 0 0 112059 0 B94 S 96 0 3 Shorney, Mr. Charles Joseph male nan 0 0 374910 8.05 nan S 27 0 3 Emir, Mr. Farred Chehab male nan 0 0 2631 7.225 nan C 691 1 1 Dick, Mr. Albert Adrian male 31 1 0 17474 57 B20 S <p>Sample rows from the Titanic dataset</p>"}, {"location": "classes/preprocessing/#data-cleaning", "title": "Data Cleaning", "text": "<p>Data cleaning involves identifying and rectifying errors, inconsistencies, and missing values in the dataset. Missing values, common in real-world data, can be handled by imputation methods such as mean, median, or mode substitution, or by removing affected rows/columns if the loss is minimal. For example, in pandas, this can be implemented as <code>df.fillna(df.mean())</code> for mean imputation. Outliers, which may skew neural network training, are detected using statistical methods like z-scores or interquartile ranges and can be winsorized or removed. Noise reduction, such as smoothing time-series data with moving averages, is also essential, particularly for RNNs where temporal dependencies are critical. Inconsistent data, like varying formats in text (e.g., dates), requires standardization to ensure uniformity. Overall, data cleaning enhances data quality, reducing the risk of misleading patterns during neural network optimization.</p> ResultCode Pclass Sex Age SibSp Parch Fare Embarked 1 male 35 0 0 26.2875 S 3 male 21 0 0 7.925 S 1 female 35 0 0 135.633 S 3 male 32 1 0 19.9667 S 1 male 29 1 0 66.6 S 3 male 32 0 0 7.775 S 3 male 20 0 0 9.225 S 2 male 27 0 0 13 S 3 female 39 1 5 31.275 S 3 male 41 0 0 7.125 S <pre><code>import pandas as pd\n\n# Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n\n# Load the Titanic dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\ndf = df.sample(n=10)\n\n# Preprocessing\ndf = preprocess(df)\n\n# Display the first few rows of the dataset\nprint(df.to_markdown(index=False))\n</code></pre>"}, {"location": "classes/preprocessing/#encoding-categorical-variables", "title": "Encoding Categorical Variables", "text": "<p>Categorical data, non-numeric by nature, must be converted for neural network input. One-hot encoding creates binary vectors for each category, e.g., transforming colors <code>['red', 'blue', 'green']</code> into <code>[[1,0,0], [0,1,0], [0,0,1]]</code>. This avoids ordinal assumptions but increases dimensionality, which can be mitigated by embedding layers in neural networks for high-cardinality features. Label encoding assigns integers (e.g., 0 for \"red\", 1 for \"blue\"), suitable for ordinal categories but risky for nominal ones due to implied ordering. For text data in NLP tasks with transformers, tokenization and subword encoding (e.g., WordPiece) are basic steps to map words to integer IDs.</p> ResultCode Pclass Sex Age SibSp Parch Fare Embarked 3 1 19 0 0 8.05 1 2 0 22 1 1 29 1 2 0 23 0 0 33 1 3 1 23 0 0 7.8958 0 2 0 45 0 0 13.5 1 2 0 55 0 0 16 1 3 1 19 0 0 7.65 1 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Preprocess the data\ndef preprocess(df):\n    # Fill missing values\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\n    # Convert categorical variables\n    label_encoder = LabelEncoder()\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n\n    # Select features\n    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n    return df[features]\n\n# Load the Titanic dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/hsandmann/ml/refs/heads/main/data/kaggle/titanic-dataset.csv')\ndf = df.sample(n=10)\n\n# Preprocessing\ndf = preprocess(df)\n\n# Display the first few rows of the dataset\nprint(df.sample(n=7).to_markdown(index=False))\n</code></pre>"}, {"location": "classes/preprocessing/#normalization-and-standardization", "title": "Normalization and Standardization", "text": "<p>Normalization scales features to a bounded range, typically \\([0, 1]\\), using min-max scaling:</p> \\[ x' = \\displaystyle \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\] <p>This is crucial for neural networks employing sigmoid or tanh activations, as it prevents saturation.</p> <p>Standardization, or z-score normalization, transforms data to have a mean of \\(0\\) and standard deviation of \\(1\\):</p> \\[ x' = \\frac{x - \\mu}{\\sigma}, \\] <p>where \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation. It is preferred for networks with ReLU activations or when data distributions are Gaussian-like, aiding faster gradient descent convergence. In practice, libraries like scikit-learn provide <code>MinMaxScaler</code> and <code>StandardScaler</code> for these operations. These techniques are especially vital in multilayer perceptrons (MLPs) and CNNs, where feature scales can dominate loss landscapes.</p> <p>Below is an example of how to apply normalization and standardization using pandas, based on the NASDAQ Apple stock price dataset:</p> ResultOriginalCode Date Volume N-Volume Z-Volume Change N-Change Z-Change 2025-10-07 00:00:00-04:00 3.19558e+07 0 -1.00194 -0.000818074 0.455701 -0.220665 2025-10-08 00:00:00-04:00 3.64969e+07 0.0775896 -0.712462 0.00616027 0.550053 0.253846 2025-10-09 00:00:00-04:00 3.8322e+07 0.108773 -0.59612 -0.0155778 0.25614 -1.22429 2025-10-10 00:00:00-04:00 6.19991e+07 0.513322 0.913194 -0.0345221 0 -2.51246 2025-10-13 00:00:00-04:00 3.81429e+07 0.105713 -0.607537 0.00974436 0.598512 0.497555 2025-10-14 00:00:00-04:00 3.5478e+07 0.0601806 -0.777413 0.00044416 0.472767 -0.134836 2025-10-15 00:00:00-04:00 3.38936e+07 0.0331094 -0.878411 0.00633649 0.552435 0.265828 2025-10-16 00:00:00-04:00 3.9777e+07 0.133634 -0.50337 -0.00758001 0.364275 -0.680461 2025-10-17 00:00:00-04:00 4.9147e+07 0.29373 0.0939274 0.0195595 0.731219 1.16496 2025-10-20 00:00:00-04:00 9.0483e+07 1 2.72892 0.0394387 1 2.5167 Date Open High Low Close Volume Dividends Stock Splits Change 2025-10-06 00:00:00-04:00 257.99 259.07 255.05 256.69 4.46641e+07 0 0 nan 2025-10-07 00:00:00-04:00 256.81 257.4 255.43 256.48 3.19558e+07 0 0 -0.000818074 2025-10-08 00:00:00-04:00 256.52 258.52 256.11 258.06 3.64969e+07 0 0 0.00616027 2025-10-09 00:00:00-04:00 257.81 258 253.14 254.04 3.8322e+07 0 0 -0.0155778 2025-10-10 00:00:00-04:00 254.94 256.38 244 245.27 6.19991e+07 0 0 -0.0345221 2025-10-13 00:00:00-04:00 249.38 249.69 245.56 247.66 3.81429e+07 0 0 0.00974436 2025-10-14 00:00:00-04:00 246.6 248.85 244.7 247.77 3.5478e+07 0 0 0.00044416 2025-10-15 00:00:00-04:00 249.49 251.82 247.47 249.34 3.38936e+07 0 0 0.00633649 2025-10-16 00:00:00-04:00 248.25 249.04 245.13 247.45 3.9777e+07 0 0 -0.00758001 2025-10-17 00:00:00-04:00 248.02 253.38 247.27 252.29 4.9147e+07 0 0 0.0195595 <pre><code>import pandas as pd\nimport yfinance as yf\n\ndat = yf.Ticker(\"AAPL\")\ndf = dat.history(period='1mo')\n\ndf['Change'] = df['Close'].pct_change()\ndf['Z-Volume'] = df['Volume'].apply(lambda x: (x-df['Volume'].mean())/df['Volume'].std())\ndf['N-Volume'] = df['Volume'].apply(lambda x: (x-df['Volume'].min())/(df['Volume'].max()-df['Volume'].min()))\ndf['Z-Change'] = df['Change'].apply(lambda x: (x-df['Change'].mean())/df['Change'].std())\ndf['N-Change'] = df['Change'].apply(lambda x: (x-df['Change'].min())/(df['Change'].max()-df['Change'].min()))\ndf = df[['Volume', 'N-Volume', 'Z-Volume', 'Change', 'N-Change', 'Z-Change']].dropna()\nprint(df.head(10).to_markdown())\n</code></pre>"}, {"location": "classes/preprocessing/#feature-scaling", "title": "Feature Scaling", "text": "<p>Feature scaling overlaps with normalization but specifically addresses disparate scales across features. Beyond min-max and z-score, logarithmic scaling (\\( x' = \\log(x + 1) \\)) handles skewed distributions, common in financial data for neural forecasting models. Scaling ensures equal contribution of features during weight updates in stochastic gradient descent (SGD).</p>"}, {"location": "classes/preprocessing/#data-augmentation", "title": "Data Augmentation", "text": "<p>Data augmentation artificially expands datasets to combat overfitting, particularly in CNNs for image classification. Basic operations include flipping, rotation (e.g., by 90\u00b0 or random angles), and cropping, while advanced methods involve adding noise (Gaussian or salt-and-pepper) or color jittering. For text data in RNNs or transformers, techniques like synonym replacement, random insertion/deletion, or back-translation (translating to another language and back) generate variations while preserving semantics. In time-series for LSTMs, window slicing or synthetic minority over-sampling technique (SMOTE)<sup>8</sup> variants create augmented sequences. Generative models like GANs (Generative Adversarial Networks) represent cutting-edge augmentation, producing realistic synthetic samples. These methods improve generalization by exposing models to diverse inputs.</p>"}, {"location": "classes/preprocessing/#handling-imbalanced-data", "title": "Handling Imbalanced Data", "text": "<p>Imbalanced datasets, where classes are unevenly represented, bias neural networks toward majority classes. Advanced resampling includes oversampling minorities (e.g., SMOTE, which interpolates new instances) or undersampling majorities. Class weighting assigns higher penalties to minority misclassifications in the loss function, e.g., weighted cross-entropy. Ensemble methods, like balanced random forests integrated with neural embeddings, or focal loss in object detection CNNs, further address this. For sequential data, temporal resampling ensures balanced windows.</p>"}, {"location": "classes/preprocessing/#feature-engineering-and-selection", "title": "Feature Engineering and Selection", "text": "<p>Feature engineering crafts new features from existing ones, such as polynomial terms or interactions (e.g., \\( x_1 \\times x_2 \\)) to capture non-linearities before neural input. Selection techniques like mutual information or recursive feature elimination reduce irrelevant features, alleviating the curse of dimensionality in high-dimensional data for autoencoders or dense networks. Embedded methods, like L1 regularization in neural training, perform selection during optimization.</p>"}, {"location": "classes/preprocessing/#dimensionality-reduction", "title": "Dimensionality Reduction", "text": "<p>High-dimensional data can lead to overfitting and increased computational costs in neural networks. Dimensionality reduction techniques help mitigate these issues by projecting data onto lower-dimensional spaces. Basic techniques include:</p>"}, {"location": "classes/preprocessing/#principal-component-analysis-pca", "title": "Principal Component Analysis (PCA)", "text": "<p>A linear method that identifies orthogonal axes (principal components) capturing maximum variance in the data. It is widely used for reducing dimensionality while retaining essential information.</p> <p>Techniques like Principal Component Analysis (PCA) project data onto lower-dimensional spaces while preserving variance:</p> \\[ X' = X \\cdot W \\] <p>where \\(W\\) are principal components. Autoencoders, a neural-based approach, learn compressed representations through encoder-decoder architectures. t-SNE or UMAP are used for visualization but less for preprocessing due to non-linearity. These are vital for CNNs on high-resolution images or transformers on long sequences to reduce computational load.</p> <p>PCA is widely used for dimensionality reduction<sup>5</sup>, while t-SNE<sup>6</sup> and UMAP<sup>7</sup> are popular for visualizing high-dimensional data in 2D or 3D spaces.</p> <p>Basically, PCA identifies orthogonal axes (principal components) capturing maximum variance, enabling efficient data representation. Autoencoders, trained to reconstruct inputs, learn compact latent spaces, useful for denoising or anomaly detection.</p> <p>PCA Steps<sup>5</sup></p> <p>1. Standardize the data:</p> \\[ X' = \\frac{X - \u03bc}{\u03c3} \\] <p>2. Compute the covariance matrix:</p> \\[ C = \\frac{1}{n} * (X'\u1d40 * X') \\] <p>3. Calculate eigenvalues and eigenvectors:</p> \\[ \\text{eigvals}, \\text{eigvecs} = \\text{np.linalg.eig}(C) \\] <p>4. Sort eigenvectors by eigenvalues in descending order.</p> <p>5. Select top \\(k\\) eigenvectors to form a new feature space</p> \\[ Y = X' * W \\] <p>where \\(W\\) is the matrix of selected eigenvectors.</p> <p>Eigenfaces, a PCA variant, is used in face recognition tasks to reduce image dimensions while retaining essential features<sup>4</sup>. In NLP, techniques like Latent Semantic Analysis (LSA) apply SVD (Singular Value Decomposition) to reduce term-document matrices, enhancing transformer efficiency.</p>"}, {"location": "classes/preprocessing/#t-sne-t-distributed-stochastic-neighbor-embedding", "title": "t-SNE (t-distributed Stochastic Neighbor Embedding)", "text": "<p>A non-linear technique that preserves local structures in data, making it suitable for visualizing clusters in high-dimensional spaces.</p>"}, {"location": "classes/preprocessing/#umap-uniform-manifold-approximation-and-projection", "title": "UMAP (Uniform Manifold Approximation and Projection)", "text": "<p>Another non-linear method that maintains both local and global data structures, often faster than t-SNE for large datasets.</p>"}, {"location": "classes/preprocessing/#domain-specific-advanced-techniques", "title": "Domain-Specific Advanced Techniques", "text": "<p>For time-series in RNNs, techniques include Fast Fourier Transform (FFT) for frequency domain conversion or segmentation into fixed windows. In text preprocessing for sentiment analysis, advanced steps encompass negation handling (e.g., marking \"not good\" as \"not_pos\"), intensification (e.g., \"very good\" as \"strong_pos\"), and POS tagging to retain sentiment-bearing words. For images in CNNs, advanced signal processing like wavelet transforms or conversion to spectrograms enhances fault diagnosis applications.</p>"}, {"location": "classes/preprocessing/#appendix", "title": "Appendix", "text": ""}, {"location": "classes/preprocessing/#normalization-vs-standardization-in-neural-network-preprocessing", "title": "Normalization vs. Standardization in Neural Network Preprocessing", "text": "<p>In data preprocessing for neural networks, both normalization and standardization are feature scaling techniques used to handle features with different scales, improve model convergence, stabilize gradients during training, and prevent features with larger ranges from dominating the learning process. These methods are particularly important for optimization algorithms like gradient descent, which neural networks rely on.</p> <ul> <li> <p>Normalization (Min-Max Scaling): This scales the data to a fixed range, typically [0, 1] or [-1, 1], using the formula: \\( \\displaystyle x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\). It preserves the original data distribution but is sensitive to outliers, as extreme values can compress the rest of the data into a narrow interval.</p> </li> <li> <p>Standardization (Z-Score Scaling): This transforms the data to have a mean of 0 and a standard deviation of 1, using the formula: \\( \\displaystyle z = \\frac{x - \\mu}{\\sigma} \\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. It centers the data and is more robust to outliers, but it does not bound the values to a specific range.</p> </li> </ul> <p>There is no universal \"better\" method; the choice depends on the data characteristics, the neural network architecture, and empirical testing. If the dataset is small, it's often worth experimenting with both to see which yields better performance. Below, I'll outline guidelines for when each is appropriate, with specific situations or cases, especially in the context of neural networks.</p> <p>When to Use Normalization</p> <p>Normalization is preferred when the data needs to be bounded within a specific range, the distribution is unknown or non-Gaussian, and there are no significant outliers. It helps avoid numeric overflow in neural networks, speeds up learning, and works well with activation functions that expect inputs in a constrained range. Key situations include:</p> <ul> <li> <p>Bounded Data or Activation Functions Sensitive to Range: Use normalization for neural networks with sigmoid or tanh activations, as these functions perform better with inputs scaled to [0, 1] or [-1, 1] to prevent saturation (where gradients become near zero). For example, in image classification tasks with convolutional neural networks (CNNs), pixel values (typically 0-255) are often normalized to [0, 1] to ensure consistent scaling and faster convergence.</p> </li> <li> <p>Features with Known Min/Max Bounds and No Outliers: When the data has clear minimum and maximum values (e.g., sensor readings bounded between fixed limits), normalization prevents larger-scale features from dominating. A case is processing demographic data like age (e.g., 0-100) in a feedforward neural network for prediction tasks, where scaling to [0, 1] maintains proportionality without assuming a normal distribution.</p> </li> <li> <p>General Speed Improvements in Training: In scenarios where neural networks handle features like age and weight, normalization to [0, 1] can accelerate training and testing by keeping inputs small and consistent, reducing the risk of overflow.</p> </li> </ul> <p>When to Use Standardization</p> <p>Standardization is suitable when the data approximates a Gaussian (normal) distribution, outliers are present, or the model benefits from centered data with unit variance. It helps prevent gradient saturation in neural networks, improves numerical stability, and is often the default choice for many algorithms. Specific cases include:</p> <ul> <li> <p>Data with Outliers or Unknown Distribution: Standardization is more robust to outliers, as it doesn't compress values into a fixed range like normalization does. For instance, in financial datasets for stock price prediction using recurrent neural networks (RNNs), where extreme values (e.g., market crashes) are common, standardization preserves the relative importance of outliers without skewing the scale.</p> </li> <li> <p>Gaussian-Like Data or Convergence-Focused Models: When features follow a bell-curve distribution (verifiable by plotting), standardization aligns with assumptions in techniques like batch normalization in deep neural networks. An example is sensor data analysis in IoT applications with neural networks, where standardization ensures faster gradient descent convergence by centering the data.</p> </li> <li> <p>Standard Practice for Neural Networks: As recommended in foundational work like Yann LeCun's efficient backpropagation paper, scaling to mean 0 and variance 1 is a go-to method to avoid saturating hidden units and handle numerical issues in training. This is common in large-scale datasets for tasks like natural language processing with transformers.</p> </li> </ul> <p>In practice, for neural networks, standardization is often preferred as a starting point due to its robustness, but normalization shines in bounded, outlier-free scenarios. Always apply scaling after splitting data into train/test sets to avoid data leakage, and use libraries like scikit-learn's <code>MinMaxScaler</code> or <code>StandardScaler</code> for implementation.</p>"}, {"location": "classes/preprocessing/#latent-space-visualisation-pca-t-sne-umap", "title": "Latent Space Visualisation: PCA, t-SNE, UMAP", "text": "<ol> <li> <p>Scikit-learn - Preprocessing data \u21a9</p> </li> <li> <p>TensorFlow - Data Augmentation \u21a9</p> </li> <li> <p>AutoML - Automated Machine Learning \u21a9</p> </li> <li> <p>Face Recognition with OpenCV \u21a9</p> </li> <li> <p>PCA - Principal Component Analysis \u21a9\u21a9</p> </li> <li> <p>Latent Space Visualisation: PCA, t-SNE, UMAP \u21a9</p> </li> <li> <p>Principal Component Analysis (PCA) from Scratch \u21a9</p> </li> <li> <p>t-SNE - t-distributed Stochastic Neighbor Embedding \u21a9</p> </li> <li> <p>Visualizing Data using t-SNE \u21a9</p> </li> <li> <p>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction \u21a9</p> </li> <li> <p>SMOTE - Synthetic Minority Over-sampling Technique \u21a9</p> </li> <li> <p>Focal Loss for Dense Object Detection \u21a9</p> </li> <li> <p>Word Embeddings - Word2Vec, GloVe, FastText \u21a9</p> </li> </ol>"}, {"location": "classes/preprocessing/normalization-vs-standardization/", "title": "Normalization vs standardization", "text": ""}, {"location": "classes/preprocessing/normalization-vs-standardization/#normalization-vs-standardization-in-neural-network-preprocessing", "title": "Normalization vs. Standardization in Neural Network Preprocessing", "text": "<p>In data preprocessing for neural networks, both normalization and standardization are feature scaling techniques used to handle features with different scales, improve model convergence, stabilize gradients during training, and prevent features with larger ranges from dominating the learning process. These methods are particularly important for optimization algorithms like gradient descent, which neural networks rely on.</p> <ul> <li> <p>Normalization (Min-Max Scaling): This scales the data to a fixed range, typically [0, 1] or [-1, 1], using the formula: \\( \\displaystyle x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\). It preserves the original data distribution but is sensitive to outliers, as extreme values can compress the rest of the data into a narrow interval.</p> </li> <li> <p>Standardization (Z-Score Scaling): This transforms the data to have a mean of 0 and a standard deviation of 1, using the formula: \\( \\displaystyle z = \\frac{x - \\mu}{\\sigma} \\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. It centers the data and is more robust to outliers, but it does not bound the values to a specific range.</p> </li> </ul> <p>There is no universal \"better\" method; the choice depends on the data characteristics, the neural network architecture, and empirical testing. If the dataset is small, it's often worth experimenting with both to see which yields better performance. Below, I'll outline guidelines for when each is appropriate, with specific situations or cases, especially in the context of neural networks.</p> <p>When to Use Normalization</p> <p>Normalization is preferred when the data needs to be bounded within a specific range, the distribution is unknown or non-Gaussian, and there are no significant outliers. It helps avoid numeric overflow in neural networks, speeds up learning, and works well with activation functions that expect inputs in a constrained range. Key situations include:</p> <ul> <li> <p>Bounded Data or Activation Functions Sensitive to Range: Use normalization for neural networks with sigmoid or tanh activations, as these functions perform better with inputs scaled to [0, 1] or [-1, 1] to prevent saturation (where gradients become near zero). For example, in image classification tasks with convolutional neural networks (CNNs), pixel values (typically 0-255) are often normalized to [0, 1] to ensure consistent scaling and faster convergence.</p> </li> <li> <p>Features with Known Min/Max Bounds and No Outliers: When the data has clear minimum and maximum values (e.g., sensor readings bounded between fixed limits), normalization prevents larger-scale features from dominating. A case is processing demographic data like age (e.g., 0-100) in a feedforward neural network for prediction tasks, where scaling to [0, 1] maintains proportionality without assuming a normal distribution.</p> </li> <li> <p>General Speed Improvements in Training: In scenarios where neural networks handle features like age and weight, normalization to [0, 1] can accelerate training and testing by keeping inputs small and consistent, reducing the risk of overflow.</p> </li> </ul> <p>When to Use Standardization</p> <p>Standardization is suitable when the data approximates a Gaussian (normal) distribution, outliers are present, or the model benefits from centered data with unit variance. It helps prevent gradient saturation in neural networks, improves numerical stability, and is often the default choice for many algorithms. Specific cases include:</p> <ul> <li> <p>Data with Outliers or Unknown Distribution: Standardization is more robust to outliers, as it doesn't compress values into a fixed range like normalization does. For instance, in financial datasets for stock price prediction using recurrent neural networks (RNNs), where extreme values (e.g., market crashes) are common, standardization preserves the relative importance of outliers without skewing the scale.</p> </li> <li> <p>Gaussian-Like Data or Convergence-Focused Models: When features follow a bell-curve distribution (verifiable by plotting), standardization aligns with assumptions in techniques like batch normalization in deep neural networks. An example is sensor data analysis in IoT applications with neural networks, where standardization ensures faster gradient descent convergence by centering the data.</p> </li> <li> <p>Standard Practice for Neural Networks: As recommended in foundational work like Yann LeCun's efficient backpropagation paper, scaling to mean 0 and variance 1 is a go-to method to avoid saturating hidden units and handle numerical issues in training. This is common in large-scale datasets for tasks like natural language processing with transformers.</p> </li> </ul> <p>In practice, for neural networks, standardization is often preferred as a starting point due to its robustness, but normalization shines in bounded, outlier-free scenarios. Always apply scaling after splitting data into train/test sets to avoid data leakage, and use libraries like scikit-learn's <code>MinMaxScaler</code> or <code>StandardScaler</code> for implementation.</p>"}, {"location": "classes/regularization/", "title": "8. Regularization", "text": "<p>Regularization techniques to prevent overfitting in MLPs, such as dropout, L1 and L2 regularization, batch normalization, and early stopping.</p> <p>Balance Between Bias and Variance</p> <p>The relationship between bias and variance is often referred to as the bias-variance tradeoff, which highlights the need for balance:</p> <p>Increasing model complexity reduces bias but increases variance (risk of overfitting). Simplifying the model reduces variance but increases bias (risk of underfitting). The goal is to find an optimal balance where both bias and variance are minimized, resulting in good generalization performance.</p> <p></p> <p>Illustration of underfitting and overfitting in neural networks. Source: GeeksforGeeks</p> <ul> <li> <p>Reducing Underfitting</p> <ul> <li>Increase model complexity.</li> <li>Increase the number of features, performing feature engineering.</li> <li>Remove noise from the data.</li> <li>Increase the number of epochs or increase the duration of training to get better results.</li> </ul> </li> <li> <p>Reducing Overfitting</p> <ul> <li>Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the risk of fitting the noise or irrelevant features.</li> <li>Increase the training data can improve the model's ability to generalize to unseen data and reduce the likelihood of overfitting.</li> <li>Reduce model complexity.</li> <li>Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).</li> <li>Ridge Regularization and Lasso Regularization.</li> <li>Use dropout for neural networks to tackle overfitting.</li> </ul> </li> </ul>"}, {"location": "classes/regularization/#dropout", "title": "Dropout", "text": "<p>Dropout<sup>1</sup> is a regularization technique where, during training, a random subset of neurons (or their connections) is \"dropped\" (set to zero) in each forward and backward pass. This prevents the network from relying too heavily on specific neurons.</p> <p>During training, each neuron has a probability \\( p \\) (typically 0.2 to 0.5) of being dropped. This forces the network to learn redundant representations, making it more robust and less likely to memorize the training data. At test time, all neurons are active, but their weights are scaled by \\( 1-p \\) to account for the reduced activation during training.</p> <p>Dropout acts like training an ensemble of smaller subnetworks, reducing co-dependency between neurons. It introduces noise, making the model less sensitive to specific patterns in the training data, thus improving generalization.</p> <p></p> <p>Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right: An example of a thinned net produced by applying dropout to the network on the left. Crossed units have been dropped. Source: Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p>"}, {"location": "classes/regularization/#practical-tips", "title": "Practical tips", "text": "<ul> <li> <p>Common dropout rates: 20\u201350% for hidden layers, lower (10\u201320%) for input layers.</p> </li> <li> <p>Use in deep networks, especially in fully connected layers or convolutional neural networks (CNNs). Avoid dropout in the output layer or when the network is small (it may hurt performance).</p> </li> </ul> <ul> <li> <p>Pros</p> <ul> <li>Effective for deep networks;</li> <li>Computationally cheap.</li> </ul> </li> <li> <p>Cons</p> <ul> <li>Requires tuning \\( p \\); can slow convergence;</li> <li>May not work well with all datasets;</li> <li>Can introduce noise, making optimization harder.</li> </ul> </li> </ul>"}, {"location": "classes/regularization/#l1-and-l2-regularizations", "title": "L1 and L2 Regularizations", "text": "<ul> <li> <p>L1 regularization (Lasso) adds a penalty term to the loss function based on the absolute values of the model\u2019s weights, encouraging sparsity in the weight matrix. This means that some weights can become exactly zero, effectively performing feature selection.</p> <p>The loss function is modified to include an L1 penalty:</p> \\[\\text{Loss} = \\text{Original Loss} + \\lambda \\sum |w_i|\\] <p>where \\( w_i \\) are the model\u2019s weights, and \\( \\lambda \\) (regularization strength) controls the penalty\u2019s impact. During training, this encourages the model to focus on the most important features, potentially improving generalization.</p> </li> <li> <p>L2 regularization (Weight Decay) adds a penalty term to the loss function based on the magnitude of the model\u2019s weights, discouraging large weights that can lead to complex, overfitted models.</p> <p>The loss function is modified to include an L2 penalty:</p> \\[\\text{Loss} = \\text{Original Loss} + \\lambda \\sum w_i^2\\] <p>where \\( w_i \\) are the model\u2019s weights, and \\( \\lambda \\) (regularization strength) controls the penalty\u2019s impact.</p> <p>During optimization, this penalty encourages smaller weights, simplifying the model.</p> <p>Large weights amplify small input changes, leading to overfitting. L2 regularization constrains weights, making the model smoother and less sensitive to noise. It effectively reduces the model\u2019s capacity to memorize training data.</p> </li> </ul>"}, {"location": "classes/regularization/#key-differences", "title": "Key Differences", "text": "<ul> <li> <p>L1: Encourages sparse models (some weights = 0), useful for feature selection or when you want a simpler model with fewer active connections.</p> </li> <li> <p>L2: Produces small but non-zero weights, often leading to better generalization in many cases, especially in deep neural networks.</p> </li> </ul>"}, {"location": "classes/regularization/#practical-tips_1", "title": "Practical tips", "text": "<ul> <li>Common \\( \\lambda \\): \\( 10^{-5} \\) to \\( 10^{-2} \\), tuned via cross-validation.</li> <li>Works well in linear models, fully connected NNs, and CNNs.</li> <li>Combine with other techniques (e.g., dropout) for better results.</li> <li>The regularization affects the weights (not biases, typically) of the chosen layers.</li> </ul> <ul> <li> <p>Pros</p> <ul> <li>Encourages simpler models;</li> <li>Can improve generalization.</li> </ul> </li> <li> <p>Cons</p> <ul> <li>Requires careful tuning of \\( \\lambda \\);</li> <li>May not work well with all datasets;</li> </ul> </li> </ul>"}, {"location": "classes/regularization/#batch-normalization", "title": "Batch Normalization", "text": "<p>Batch normalization<sup>2</sup> is a technique to improve the training of deep neural networks by normalizing the inputs to each layer. It reduces internal covariate shift, allowing for faster training and potentially better performance.</p> <p>During training, batch normalization standardizes the inputs to a layer for each mini-batch:</p> \\[\\hat{x} = \\frac{x - \\mu}{\\sigma + \\epsilon}\\] <p>where \\( \\mu \\) is the batch mean, \\( \\sigma \\) is the batch standard deviation, and \\( \\epsilon \\) is a small constant for numerical stability.</p> <p>After normalization, the layer can learn a scale ( \\( \\gamma \\) ) and shift ( \\( \\beta \\) ) parameter:</p> \\[y = \\gamma \\hat{x} + \\beta\\] <p>This allows the model to maintain the representational power while benefiting from the normalization.</p>"}, {"location": "classes/regularization/#practical-tips_2", "title": "Practical tips", "text": "<ul> <li>Use batch normalization after convolutional layers or fully connected layers.</li> <li>It can be used with other regularization techniques (e.g., dropout) for improved generalization.</li> <li>Consider using it in the training phase only, and not during inference, to avoid introducing noise.</li> </ul> <ul> <li> <p>Pros</p> <ul> <li>Speeds up training;</li> <li>Allows higher learning rates;</li> <li>Reduces sensitivity to initialization.</li> </ul> </li> <li> <p>Cons</p> <ul> <li>Adds complexity to the model;</li> <li>May not always improve performance.</li> </ul> </li> </ul>"}, {"location": "classes/regularization/#early-stopping", "title": "Early Stopping", "text": "<p>Early stopping is a regularization technique used to prevent overfitting in machine learning models, particularly in deep learning. The idea is to monitor the model's performance on a validation set during training and stop the training process once the performance starts to degrade.</p> <p>The key steps involved in early stopping are:</p> <ol> <li>Split the Data: Divide the dataset into training, validation, and test sets.</li> <li>Monitor Performance: During training, periodically evaluate the model on the validation set and track the performance metric (e.g., accuracy, loss).</li> <li>Set Patience: Define a patience parameter, which is the number of epochs to wait for an improvement in the validation performance before stopping training.</li> <li>Stop Training: If the validation performance does not improve for a specified number of epochs (patience), stop the training process.</li> </ol> <p>Early stopping helps to find a balance between underfitting and overfitting by allowing the model to train long enough to learn the underlying patterns in the data while preventing it from fitting noise.</p>"}, {"location": "classes/regularization/#practical-tips_3", "title": "Practical tips", "text": "<ul> <li>Use early stopping in conjunction with other regularization techniques (e.g., dropout, weight decay) for better results.</li> <li>Monitor multiple metrics (e.g., training loss, validation loss) to make informed decisions about stopping.</li> </ul> <ul> <li> <p>Pros</p> <ul> <li>Helps prevent overfitting;</li> <li>Can save training time.</li> </ul> </li> <li> <p>Cons</p> <ul> <li>Requires careful tuning of patience;</li> <li>May stop training too early.</li> </ul> </li> </ul>"}, {"location": "classes/regularization/#additional", "title": "Additional", "text": "<ol> <li> <p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R.\u00a0\u21a9</p> </li> <li> <p>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Ioffe, S., &amp; Szegedy, C.\u00a0\u21a9</p> </li> <li> <p>Underfitting and Overfitting in Machine Learning \u21a9</p> </li> </ol>"}, {"location": "classes/stable-diffusion/", "title": "16. Stable Diffusion", "text": "<p>Stable Diffusion is a state-of-the-art text-to-image generative model developed by Stability AI in collaboration with researchers at EleutherAI and LAION. It leverages Latent Diffusion Models (LDMs) to generate high-quality images from textual descriptions efficiently. Stable Diffusion has gained significant attention for its ability to produce detailed and diverse images, making it a popular choice for various applications in art, design, and content creation.</p> <p>It is based on the principles of diffusion models, which involve a two-step process: first, adding noise to an image to create a noisy version, and then training a neural network to reverse this process by denoising the image step-by-step. Stable Diffusion operates in a latent space, which allows it to generate images more efficiently than traditional pixel-space diffusion models.</p>"}, {"location": "classes/stable-diffusion/#diffusion-models", "title": "Diffusion Models", "text": "<p>Diffusion models are trained to predict a way to slightly denoise a sample in each step, and after a few iterations, a result is obtained. Diffusion models have already been applied to a variety of generation tasks, such as image, speech, 3D shape, and graph synthesis.</p> <p>Diffusion models consist of two steps:</p> <ul> <li> <p>Forward Diffusion</p> <p>Maps data to noise by gradually perturbing the input data. This is formally achieved by a simple stochastic process that starts from a data sample and iteratively generates noisier samples using a simple Gaussian diffusion kernel.</p> <p>This process is used only during training and not on inference.</p> </li> <li> <p>Reverse Diffusion</p> <p>Undoes the forward diffusion and performs iterative denoising. This process represents data synthesis and is trained to generate data by converting random noise into realistic data.</p> </li> </ul> <p></p> <p>Overview of DDPM. Source: <sup>12</sup>.</p>"}, {"location": "classes/stable-diffusion/#forward-diffusion-process", "title": "Forward Diffusion Process", "text": "<p>In the forward diffusion process, a data sample \\( x_0 \\) (e.g., an image) is gradually corrupted by adding Gaussian noise over a series of time steps \\( t = 1, 2, \\ldots, T \\). At each time step, a small amount of noise is added to the sample, resulting in a sequence of increasingly noisy samples \\( x_1, x_2, \\ldots, x_T \\). The process can be mathematically described as:</p> \\[ q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I) \\] <p>where \\( \\beta_t \\) is a variance schedule that controls the amount of noise added at each step.</p> <p></p> <p>Forward Diffusion Process. Source: <sup>12</sup>.</p>"}, {"location": "classes/stable-diffusion/#reverse-diffusion-process", "title": "Reverse Diffusion Process", "text": "<p>The reverse diffusion process aims to reconstruct the original data sample from the noisy version by iteratively denoising it. A neural network, typically a U-Net architecture, is trained to predict the noise added at each time step. The reverse process can be expressed as:</p> \\[ p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\] <p>where \\( \\mu_\\theta \\) and \\( \\Sigma_\\theta \\) are the mean and covariance predicted by the neural network parameterized by \\( \\theta \\).</p> <p></p> <p>Reverse Diffusion Process. Source: <sup>12</sup>.</p> <p>Unlike the forward process, we cannot use \\( q(x_{t-1} | x_t) \\) directly because it requires knowledge of the original data distribution - it is intractable (uncomputable). Instead, we train the neural network to approximate this distribution by minimizing a loss function that measures the difference between the predicted noise and the actual noise added during the forward process.</p> <p>Usually, a U-Net architecture is used as the neural network for the reverse diffusion process due to its ability to capture multi-scale features effectively.</p>"}, {"location": "classes/stable-diffusion/#u-net-training", "title": "U-Net Training", "text": "<p>The U-Net is trained using a dataset of images <sup>8</sup>.</p>"}, {"location": "classes/stable-diffusion/#dataset", "title": "Dataset", "text": "<p>In each epoch:</p> <ol> <li>A random time step  \\( t \\) will be selected for each training sample (image).</li> <li>Apply the Gaussian noise (corresponding to \\( t \\)) to each image.</li> <li>Convert the time steps to embeddings (vectors).</li> </ol> <p></p> <p>U-Net Training Dataset Preparation. Source: <sup>8</sup>.</p>"}, {"location": "classes/stable-diffusion/#training", "title": "Training", "text": "<p>The official training algorithm is as above, and the following diagram is an illustration of how a training step works:</p> <p></p> <p>U-Net Training Step. Source: <sup>8</sup>.</p>"}, {"location": "classes/stable-diffusion/#reverse-diffusion-denoising-sampling", "title": "Reverse Diffusion / Denoising / Sampling", "text": "<p>Once the U-Net is trained, the reverse diffusion process can be used to generate new images from random noise. The generation process involves the following steps:</p> <p></p> <p>U-Net Sampling. Source: <sup>8</sup>.</p>"}, {"location": "classes/stable-diffusion/#milestones", "title": "Milestones", "text": "<pre><code>graph TD\n    A[2015: Diffusion Concept] --&gt; B[2020: Denoising Diffusion Probabilistic Models - DDPM]\n    B --&gt; C[2021: Denoising Diffusion Implicit Models - DDIM]\n    C --&gt; D[2021: Latent Diffusion - LDM]\n    D --&gt; E[2022: Stable Diffusion v1&lt;br&gt;LDM + CLIP]\n    E --&gt; F[2022: SD 2.0]\n    F --&gt; G[2023: SDXL]\n    G --&gt; H[2024: SD3 / SD3.5]\n    E --&gt; I[&lt;a href=\"https://github.com/lllyasviel/ControlNet\" target=\"_blank\"&gt;ControlNet&lt;/a&gt;, &lt;a href=\"https://arxiv.org/abs/2308.06721\" target=\"_blank\"&gt;IP-Adapter&lt;/a&gt;, &lt;a href=\"https://arxiv.org/abs/2106.09685\" target=\"_blank\"&gt;LoRA&lt;/a&gt;]\n    E --&gt; J[Text-to-3D: DreamFusion \u2192 Magic3D \u2192 ...]\n    H --&gt; K[2024: FLUX.1 - Post-Stability AI]\n    click A \"https://arxiv.org/abs/1503.03585\" \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\"\n    click B \"https://arxiv.org/abs/2006.11239\" \"Denoising Diffusion Probabilistic Models\"\n    click C \"https://arxiv.org/abs/2010.02502\" \"Denoising Diffusion Implicit Models\"\n    click D \"https://github.com/CompVis/latent-diffusion\" \"Latent Diffusion Models\"\n    click E \"https://huggingface.co/blog/stable_diffusion\" \"Stable Diffusion v1 Release\"\n    click F \"https://stability.ai/news/stable-diffusion-v2-release\" \"Stable Diffusion 2.0 Release\"\n    click G \"https://arxiv.org/abs/2307.01952\" \"SDXL: High-Resolution Image Synthesis with Latent Diffusion Models\"\n    click K \"https://flux1.ai/\" \"FLUX.1 by Stability AI\"</code></pre>"}, {"location": "classes/stable-diffusion/#stable-diffusion", "title": "Stable Diffusion", "text": "<p>Stable Diffusion is a text-to-image generative model that utilizes Latent Diffusion Models (LDMs) to create high-quality images from textual descriptions. The model operates in a latent space, which allows for efficient image synthesis while maintaining high fidelity.</p> <p>The Stable Diffusion architecture consists of three main components:</p> <ul> <li> <p>The Text Encoder: A pre-trained text encoder (like CLIP) is used to convert the input text prompt into a semantic embedding that guides the denoising process.</p> </li> <li> <p>The Diffusion Model: The core of the LDM is a U-Net architecture that learns to denoise the latent representations. It takes as input the noisy latent tensor and the text embedding (from the text encoder) and iteratively refines the latent representation over a series of time steps.</p> </li> <li> <p>The Autoencoder: The input of the model is a random noise of the size of the desired output. It will first reduce the sample to a lower dimensional latent space. For that, the authors used the VAE Architecture, which consists of two parts - encoder and decoder. The encoder is used during training to convert the sample into a lower latent representation and passes it as input to the next block. On inference, the denoised, generated samples undergo reverse diffusion and are transformed back to their original dimensional latent space.</p> </li> </ul> <p></p> <p>Latent Diffusion Model Architecture. Source: <sup>6</sup>.</p>"}, {"location": "classes/stable-diffusion/#inference", "title": "Inference", "text": "Simplified PipelineDetailed Pipeline <pre><code>graph TD\n    A[Text Prompt] --&gt; B(CLIP Text Encoder)\n    B --&gt; C[Text Embedding]\n\n    D[Random Noise&lt;br&gt;&lt;small&gt;Latent&lt;/small&gt;] --&gt; E[Diffusion Model&lt;br&gt;&lt;small&gt;UNet + Scheduler&lt;/small&gt;]\n    C --&gt; E\n\n    E --&gt; F[Latent Image&lt;br&gt;&lt;small&gt;after denoising&lt;/small&gt;]\n\n    F --&gt; G(VAE Decoder)\n    G --&gt; H[Final Image&lt;br&gt;&lt;small&gt;in pixels&lt;/small&gt;]\n\n    subgraph \"Latent Space\"\n        D\n        E\n        F\n    end\n\n    style A fill:#a8e6cf,stroke:#333\n    style B fill:#ffccbc,stroke:#333\n    style C fill:#ffccbc,stroke:#333\n    style D fill:#ffd3b6,stroke:#333\n    style E fill:#dcedc1,stroke:#333\n    style F fill:#dcedc1,stroke:#333\n    style G fill:#c7ceea,stroke:#333\n    style H fill:#c7ceea,stroke:#333</code></pre> <pre><code>graph TD\n    A[\"Text Prompt&lt;br&gt;'A cat in space'\"] --&gt; B[\"CLIP Text Encoder&lt;br&gt;&lt;small&gt;(Transformer)&lt;/small&gt;\"]\n    B --&gt; C[\"Text Embedding&lt;br&gt;&lt;small&gt;(77 tokens \u00d7 768 dim)&lt;/small&gt;\"]\n\n    D[\"Random Gaussian Noise&lt;br&gt;z\u2080 ~ N(0,1)&lt;br&gt;&lt;small&gt;(4 \u00d7 64 \u00d7 64)&lt;/small&gt;\"] \n\n    subgraph Diffusion_Model [\"Diffusion Model&lt;br&gt;(Latent Space)\"]\n        direction TB\n        E[\"UNet with Cross-Attention&lt;br&gt;Predicting noise \u03b5(\u03b8)\"]\n        F[\"Scheduler&lt;br&gt;&lt;small&gt;DDIM, PLMS, etc.&lt;/small&gt;\"]\n        G[\"Cross-Attention Layers&lt;br&gt;Query: latent image&lt;br&gt;Key/Value: CLIP embedding\"]\n\n        D --&gt; E\n        C --&gt; G\n        G --&gt; E\n        E --&gt; F\n        F --&gt; H{Denoising Loop&lt;br&gt;T steps}\n        H --&gt;|Step t| E\n    end\n\n    H --&gt; I[\"Final Latent Image&lt;br&gt;z\u0302_T&lt;br&gt;&lt;small&gt;(4 \u00d7 64 \u00d7 64)&lt;/small&gt;\"]\n\n    I --&gt; J[\"VAE Decoder\"]\n    J --&gt; K[\"Final Image in Pixels&lt;br&gt;&lt;small&gt;(3 \u00d7 512 \u00d7 512)&lt;/small&gt;\"]\n\n    subgraph Training [\"Training&lt;br&gt;&lt;small&gt;optional&lt;/small&gt;\"]\n        L[\"Real Image&lt;br&gt;(3 \u00d7 512 \u00d7 512)\"] --&gt; M[\"VAE Encoder&lt;br&gt;(Downsampling)\"]\n        M --&gt; N[\"Latent Image&lt;br&gt;z = \u03bc + \u03c3\u2299\u03b5\"]\n        N --&gt; O[\"Add Noise&lt;br&gt;q(z_t | z_0)\"]\n        O --&gt; E\n    end\n\n    classDef text fill:#fadadd,stroke:#e74c3c,stroke-width:2px\n    classDef latent fill:#fff2cc,stroke:#f39c12,stroke-width:2px\n    classDef pixel fill:#d5f5e3,stroke:#27ae60,stroke-width:2px\n    classDef model fill:#ebebeb,stroke:#7f8c8d,stroke-width:2px\n\n    class A,C text\n    class D,I latent\n    class K,L pixel\n    class B,E,F,G,J model</code></pre> <ol> <li> <p>Text Encoding: The input text prompt is processed using a text encoder (like CLIP) to generate a text embedding that captures the semantic meaning of the prompt.</p> </li> <li> <p>Latent Space Initialization: A random noise tensor is generated in the latent space, which serves as the starting point for the image generation process.</p> </li> <li> <p>Diffusion Process: The diffusion model, typically a U-Net architecture, takes the noisy latent tensor and the text embedding as inputs. It iteratively denoises the latent tensor over a series of time steps, guided by the text embedding to ensure that the generated image aligns with the input prompt.</p> </li> <li> <p>Image Decoding: Once the denoising process is complete, the final latent representation is passed through a Variational Autoencoder (VAE) decoder to convert it back into pixel space, resulting in the final generated image.</p> </li> </ol>"}, {"location": "classes/stable-diffusion/#examples", "title": "Examples", "text": "<ul> <li> <p>GeeksForGeeks - Generate Images from Text in Python - Stable Diffusion<sup>9</sup>. Coded at: https://colab.research.google.com/drive/1LzkO8GySnbTLMNQj_xVJCVRAjWD3JkpX</p> </li> <li> <p>Data Camp - How to Use the Stable Diffusion 3 API<sup>17</sup></p> </li> </ul>"}, {"location": "classes/stable-diffusion/#additional", "title": "Additional", "text": ""}, {"location": "classes/stable-diffusion/#ddpm-vs-ddim", "title": "DDPM vs DDIM", "text": "Aspect DDPM DDIM Background Probabilistic Deterministic Speed More steps (slower) Fewer steps (faster) Quality High variability More consistent"}, {"location": "classes/stable-diffusion/#sde", "title": "SDE", "text": "<p>Denoising Diffusion Probabilistic Models (DDPM) and Score-based Generative Modeling through Stochastic Differential Equations (SDE). Source: <sup>7</sup>.</p>"}, {"location": "classes/stable-diffusion/#u-net-architecture", "title": "U-Net Architecture", "text": "<p>U-Net is a convolutional neural network architecture originally designed for biomedical image segmentation<sup>13</sup>. It has since been widely adopted in various image generation tasks, including diffusion models like Stable Diffusion. The U-Net architecture is characterized by its U-shaped structure, which consists of an encoder (contracting path) and a decoder (expanding path) with skip connections between corresponding layers.</p> <p></p> <p>U-Net Architecture. Source: <sup>14</sup>.</p> Stable Diffusion U-Net Architecture <pre><code>graph TD\n    subgraph Input\n        Z[\"Noisy Latent z_t&lt;br&gt;(B,4,64,64)\"] \n        T[Timestep t]\n        C[\"CLIP Text Emb&lt;br&gt;(B,77,768)\"]\n    end\n\n    Z --&gt; ConvIn[Initial Conv&lt;br&gt;\u2192 320 ch]\n    T --&gt; TEmb[Sinusoidal \u2192 MLP \u2192 320]\n    C --&gt; CProj[Linear 768\u2192320]\n\n    ConvIn --&gt; D1[Down Block 1&lt;br&gt;320 \u2192 320]\n    D1 --&gt; P1[Downsample]\n    P1 --&gt; D2[Down Block 2&lt;br&gt;320 \u2192 640]\n    D2 --&gt; P2[Downsample]\n    P2 --&gt; D3[Down Block 3&lt;br&gt;640 \u2192 1280]\n    D3 --&gt; P3[Downsample]\n    P3 --&gt; Bottleneck[Bottleneck&lt;br&gt;1280 ch + Self-Attn]\n\n    %% Skip connections\n    D1 --&gt; S1[Skip 1&lt;br&gt;320,32x32]\n    D2 --&gt; S2[Skip 2&lt;br&gt;640,16x16]\n    D3 --&gt; S3[Skip 3&lt;br&gt;1280,8x8]\n\n    Bottleneck --&gt; U1[Up Block 1&lt;br&gt;+ Skip 3]\n    S3 --&gt; U1\n    U1 --&gt; Up1[Upsample]\n    Up1 --&gt; U2[Up Block 2&lt;br&gt;+ Skip 2 + Cross-Attn]\n    S2 --&gt; U2\n    CProj --&gt; U2\n    U2 --&gt; Up2[Upsample]\n    Up2 --&gt; U3[Up Block 3&lt;br&gt;+ Skip 1 + Cross-Attn]\n    S1 --&gt; U3\n    CProj --&gt; U3\n\n    U3 --&gt; Out[Final Conv&lt;br&gt;\u2192 4 ch]\n    Out --&gt; Eps[\"\u03b5_pred(z_t, t, c)\"]\n\n    style Z fill:#ffd3b6\n    style Eps fill:#a8e6cf\n    style Bottleneck fill:#ff9999\n    style U1,U2,U3 fill:#dcedc1</code></pre>"}, {"location": "classes/stable-diffusion/#videos", "title": "Videos", "text": "Deepia: DDPMDeepia: Score-basedWelch Labs: AI images/videosLatent Diffusion Models <p>Deepia: Diffusion Models: DDPM | Generative AI Animated</p> <p></p> <p>Deepia: Score-based Diffusion Models | Generative AI Animated</p> <p></p> <p>But how do AI images and videos actually work? | Guest video by Welch Labs</p> <p></p> <p></p> <ol> <li> <p>Deep Unsupervised Learning using Nonequilibrium Thermodynamics, 2015.\u00a0\u21a9</p> </li> <li> <p>Denoising Diffusion Probabilistic Models, 2020.\u00a0\u21a9</p> </li> <li> <p>Denoising Diffusion Implicit Models, 2020.\u00a0\u21a9</p> </li> <li> <p>Latent Diffusion Models, 2021.\u00a0\u21a9</p> </li> <li> <p>Hugging Face - Stable Diffusion v1 - Release \u21a9</p> </li> <li> <p>Dagshub - Stable Diffusion: Best Open Source Version of DALL-E 2 \u21a9</p> </li> <li> <p>Score-Based Generative Modeling through Stochastic Differential Equations \u21a9</p> </li> <li> <p>Diffusion Models Clearly Explained \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Generate Images from Text in Python - Stable Diffusion \u21a9</p> </li> <li> <p>Hugging Face - Diffusers \u21a9</p> </li> <li> <p>Hugging Face - Diffusion Course \u21a9</p> </li> <li> <p>How to Run Stable Diffusion: A Step-by-Step Guide \u21a9\u21a9\u21a9</p> </li> <li> <p>U-Net: Convolutional Networks for Biomedical Image Segmentation \u21a9</p> </li> <li> <p>GeeksForGeeks - U-Net Architecture Explained \u21a9</p> </li> <li> <p>Polo Club - Diffusion Explainer \u21a9</p> </li> <li> <p>Hugging Face - Stable Diffusion 3.5 Large \u21a9</p> </li> <li> <p>How to Use Stable Diffusion 3 API \u21a9</p> </li> <li> <p>Stable Diffusion Models, by Ankit Kumar.\u00a0\u21a9</p> </li> <li> <p>Scalable Diffusion Models with Transformers (DiT), 2023.\u00a0\u21a9</p> </li> <li> <p>Flow-Matching: A New Paradigm for Generative Modeling, 2022.\u00a0\u21a9</p> </li> <li> <p>Flux: A General Framework for Diffusion Models, 2024.\u00a0\u21a9</p> </li> <li> <p>StreamDiffusion, suggested by Pedro Fracassi.\u00a0\u21a9</p> </li> </ol>"}, {"location": "classes/variational-autoencoders/", "title": "14. VAE", "text": ""}, {"location": "classes/variational-autoencoders/#autoencoders", "title": "Autoencoders", "text": "<p>Autoencoders (AEs) are neural networks designed to learn efficient:</p> <ol> <li>codings of input data by compressing it into a lower-dimensional representation, and then;</li> <li>reconstructing it back to the original form.</li> </ol> <p>Autoencoders consist of two main components:</p> <ul> <li> <p>an encoder, which compresses input data into a lower-dimensional representation known as the latent space or code. This latent space, often called embedding, aims to retain as much information as possible, allowing the decoder to reconstruct the data with high precision. If we denote our input data as \\( x \\) and the encoder as \\( E \\), then the output latent space representation, \\( s \\), would be \\( s=E(x) \\).</p> </li> <li> <p>a decoder, which reconstructs the original input data by accepting the latent space representation \\( s \\). If we denote the decoder function as \\( D \\) and the output of the decoder as \\( o \\), then we can represent the decoder as \\( o = D(s) \\).</p> </li> </ul> <p>Both encoder and decoder are typically composed of one or more layers, which can be fully connected, convolutional, or recurrent, depending on the input data\u2019s nature and the autoencoder\u2019s architecture.<sup>1</sup> The entire autoencoder process can be summarized as:</p> \\[ o = D(E(x)) \\] <p></p> <p>An illustration of the architecture of autoencoders. Source: <sup>1</sup>.</p>"}, {"location": "classes/variational-autoencoders/#types-of-autoencoder", "title": "Types of Autoencoder", "text": "<p>There are several type of autoencoders, each one with this particularity:</p>"}, {"location": "classes/variational-autoencoders/#vanilla-autoencoders", "title": "Vanilla Autoencoders", "text": "<p>Vanilla encoders are fully connected layers for encoder and decoder. It works to compress input information and are applied over simple data.</p> <p></p> <p>The encoder such as the decoder are fully connected networks. The encoder addresses the input data to the latent space (compressed space - encoded data). The decoder addresses the latent space data to output (reconstructed data). Source: <sup>1</sup>.</p> <p>Latent Space is a compressed representation of the input data. The dimensionality of the latent space is typically much smaller than that of the input data, which forces the autoencoder to learn a compact representation that captures the most important features of the data.</p>"}, {"location": "classes/variational-autoencoders/#convolutional-autoencoders", "title": "Convolutional Autoencoders", "text": "<p>In convolutional autoencoders, the encoder and the decoder are neural networks based on Convolutional Neural Networks. So, the approach is more intensive for handling image data.</p> <p></p> <p>In convolutional autoencoders, the encoder and decoder are based on Convolutional Neural Networks (CNNs). This architecture is particularly effective for image data, as it can capture spatial hierarchies and patterns. Source: <sup>2</sup>.</p>"}, {"location": "classes/variational-autoencoders/#variational-autoencoders", "title": "Variational Autoencoders", "text": "<p>Variational Autoencoders (VAEs) are generative models that learn to encode data into a lower-dimensional latent space and then decode it back to the original space. VAEs can generate new samples from the learned latent distribution, making them ideal for image generation and style transfer tasks.</p> <p></p> <p>A VAE maps a input data \\( \\mathbf{x} \\) into latent space \\( \\mathbf{z} \\) and then reconstructs it back to the original space \\( \\mathbf{\\hat{x}} \\) (output). The encoder learns to capture the underlying structure of the data, while the decoder generates new samples from the latent space. Source: <sup>2</sup>.</p> <p>VAEs were introduced in 2013 by Diederik et al. - Auto-Encoding Variational Bayes.</p> <p></p> <p>Figure: Comparison between a standard Autoencoder and a Variational Autoencoder (VAE). In a standard Autoencoder, the encoder maps input data \\( \\mathbf{x} \\) to a fixed latent representation \\( \\mathbf{z} \\), which is then used by the decoder to reconstruct the input as \\( \\mathbf{\\hat{x}} \\). In contrast, a VAE encodes the input data into a distribution over the latent space, typically modeled as a Gaussian distribution with mean \\( \\mu \\) and standard deviation \\( \\sigma \\). During training, the VAE samples from this distribution to obtain \\( \\mathbf{z} \\), which is then used by the decoder to reconstruct the input. This probabilistic approach allows VAEs to generate new samples by sampling from the latent space, making them powerful generative models. Dataset: Fashion-MNIST. Source: <sup>3</sup>.</p>"}, {"location": "classes/variational-autoencoders/#key-features-of-vaes", "title": "Key Features of VAEs", "text": "<p>VAEs have the ability to learn smooth and continuous latent spaces, which allows for meaningful interpolation between data points. This is particularly useful in applications such as image generation, where one can generate new images by sampling from the latent space. Also, the probabilistic nature of VAEs helps in regularizing the latent space, preventing overfitting and introducing the same level of randomness, ensuring that the model generalizes well to unseen data.</p> <p>Aspects of VAEs include:</p> <ul> <li> <p>Regularization and Continuity: The latent space in VAEs is regularized to follow a prior distribution (usually a standard normal distribution). This encourages the model to learn a continuous and smooth latent space, allowing for meaningful interpolation between data points.</p> </li> <li> <p>Simplicity in Sampling: VAEs can generate new samples by simply sampling from the latent space distribution - the Gaussian distribution is mathematically tractable and is a universal approximator -, making them efficient for generative tasks.</p> </li> <li> <p>Reparameterization Trick: To enable backpropagation through the stochastic sampling process, VAEs employ the reparameterization trick. This involves expressing the sampled latent variable \\( \\mathbf{z} \\) as a deterministic function of the input data \\( \\mathbf{x} \\) and a random noise variable \\( \\mathbf{\\epsilon} \\), allowing gradients to flow through the network during training.</p> </li> <li> <p>Balanced Latent Space: The KL divergence term in the VAE loss function encourages the learned latent space to be similar to the prior distribution, promoting a well-structured and balanced latent space.</p> </li> </ul>"}, {"location": "classes/variational-autoencoders/#training-vaes", "title": "Training VAEs", "text": "<p>VAEs uses Kullback-Leibler (KL) divergence as its loss function, which measures the difference between the learned latent distribution and the prior distribution. The loss function is a combination of the reconstruction loss (how well the decoder reconstructs the input) and the KL divergence term.</p> <p>Suppose we have a distribution \\( z \\) and we want to approximate it with a distribution \\( p(z|x) \\), where \\( x \\) is the input data. In other words, we want to find a distribution \\( p(z|x) \\), then we can to it by following Bayes' theorem:</p> \\[ p(z|x) = \\displaystyle \\frac{p(x|z)p(z)}{p(x)} \\] <p>But, the problem is that \\(p(x)\\) is intractable:</p> \\[ p(x) = \\displaystyle \\int p(x|z)p(z)dz \\] <p>This integral is often intractable distribution. Hence, we can approximate it with a variational distribution \\( q(z|x) \\), which is easier to compute. So, we want to minimize the KL divergence between \\( q(z|x) \\) and \\( p(z|x) \\):</p> \\[ \\min \\text{KL} ( q(z|x) || (z|x) ) \\] <p>By simplifying the above minimization problem is equivalent to the following maximization problem:</p> \\[ \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\text{KL}(q(z|x) || p(z)) \\] <p>where:</p> <ul> <li>The first term, \\( \\mathbb{E}_{q(z|x)}[\\log p(x|z)] \\), is the expected log-likelihood of the data given the latent variable, which encourages the model to reconstruct the input data accurately.</li> <li>The second term, \\( \\text{KL}(q(z|x) || p(z)) \\), is the KL divergence between the approximate posterior and the prior distribution, which regularizes the latent space.</li> </ul> <p>Thus, the loss function for training a VAE can be expressed as:</p> \\[ \\mathcal{L} = -\\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\text{KL}(q(z|x) || p(z)) \\] <p></p> <p>Figure: Basic architecture of a Variational Autoencoder (VAE). The encoder maps input data \\( \\mathbf{x} \\) to a latent representation \\( \\mathbf{z} \\), and the decoder reconstructs \\( \\mathbf{x'} \\) from \\( \\mathbf{z} \\). Source: Wikipedia</p>"}, {"location": "classes/variational-autoencoders/#reparameterization-trick", "title": "Reparameterization Trick", "text": "<p>The reparameterization trick is a key innovation that allows for efficient backpropagation through the stochastic layers of a VAE. Instead of sampling \\( z \\) directly from \\( q(z|x) \\), we can express \\( z \\) as a deterministic function of \\( x \\) and some noise \\( \\epsilon \\) drawn from a simple distribution (e.g., Gaussian):</p> \\[ z = \\mu + \\sigma \\cdot \\epsilon \\] <p>where \\( \\mu \\) and \\( \\sigma \\) are the mean and standard deviation outputs of the encoder. This transformation allows us to backpropagate through the network while still maintaining the stochastic nature of the latent variable.</p> <p></p>"}, {"location": "classes/variational-autoencoders/#numerical-simulation", "title": "Numerical Simulation", "text": "VAE - Variational Autoencoder <p>A Variational Autoencoder (VAE) encodes input data into a probabilistic latent space (defined by mean \u03bc and log-variance log(\u03c3\u00b2)) and decodes it back to reconstruct the input. The latent space is sampled using the reparameterization trick for differentiability. The loss combines reconstruction error (MSE) and KL divergence to regularize the latent distribution toward a standard normal.</p> <p>For this numerical example, we've scaled up to:</p> <ul> <li>Input dimension: 4 (e.g., a vector like <code>[1.0, 2.0, 3.0, 4.0]</code>)</li> <li>Latent dimension: 2</li> <li>Output dimension: 4 (reconstruction of input)</li> <li>Hidden layer size: 8 (for both encoder and decoder, to add capacity)</li> </ul> <p>The model uses PyTorch with random initialization (seeded at 42 for reproducibility). All calculations are shown step-by-step, including matrix multiplications where relevant. Weights and biases are explicitly listed below.</p>"}, {"location": "classes/variational-autoencoders/#model-architecture", "title": "Model Architecture", "text": "<ul> <li> <p>Encoder:</p> <ul> <li>Linear (fc1): 4 inputs \u2192 8 hidden units, followed by ReLU.</li> <li>Linear to \u03bc (fc_mu): 8 \u2192 2.</li> <li>Linear to logvar (fc_logvar): 8 \u2192 2.</li> </ul> </li> <li> <p>Latent: Sample z from N(\u03bc, \u03c3\u00b2) using reparameterization trick.</p> </li> <li> <p>Decoder:</p> <ul> <li>Linear (fc_dec1): 2 latent \u2192 8 hidden units, followed by ReLU.</li> <li>Linear to output (fc_dec2): 8 \u2192 4 (no final activation, assuming Gaussian output for simplicity).</li> </ul> </li> <li> <p>Loss: Summed MSE for reconstruction + KL divergence (without \u03b2 annealing).</p> </li> </ul>"}, {"location": "classes/variational-autoencoders/#weights-and-biases", "title": "Weights and Biases", "text": "<p>All parameters are initialized randomly (via torch.manual_seed(42)). Here they are:</p> <p>Encoder</p> <ul> <li> <p>fc1.weight (encoder input to hidden, shape [8, 4]):</p> <pre><code>[\n    [ 0.3823,  0.4150, -0.1171,  0.4593],\n    [-0.1096,  0.1009, -0.2434,  0.2936],\n    [ 0.4408, -0.3668,  0.4346,  0.0936],\n    [ 0.3694,  0.0677,  0.2411, -0.0706],\n    [ 0.3854,  0.0739, -0.2334,  0.1274],\n    [-0.2304, -0.0586, -0.2031,  0.3317],\n    [-0.3947, -0.2305, -0.1412, -0.3006],\n    [ 0.0472, -0.4938,  0.4516, -0.4247]\n]\n</code></pre> </li> <li> <p>fc1.bias (shape [8]):</p> <pre><code>[ 0.3860,  0.0832, -0.1624,  0.3090,  0.0779,  0.4040,  0.0547, -0.1577 ]\n</code></pre> </li> <li> <p>fc_mu.weight (hidden to \u03bc, shape [2, 8]):</p> <pre><code>[\n    [ 0.0950, -0.0959,  0.1488,  0.3157,  0.2044, -0.1546,  0.2041,  0.0633],\n    [ 0.1795, -0.2155, -0.3500, -0.1366, -0.2712,  0.2901,  0.1018,  0.1464]\n]\n</code></pre> </li> <li> <p>fc_mu.bias (shape [2]):</p> <pre><code>[ 0.1118, -0.0062 ]\n</code></pre> </li> <li> <p>fc_logvar.weight (hidden to logvar, shape [2, 8]):</p> <pre><code>[\n    [ 0.2767, -0.2512,  0.0223, -0.2413,  0.1090, -0.1218,  0.1083, -0.0737],\n    [ 0.2932, -0.2096, -0.2109, -0.2109,  0.3180,  0.1178,  0.3402, -0.2918]\n]\n</code></pre> </li> <li> <p>fc_logvar.bias (shape [2]):</p> <pre><code>[ -0.3507, -0.2766 ]\n</code></pre> </li> </ul> <p>Decoder</p> <ul> <li> <p>fc_dec1.weight (latent to decoder hidden, shape [8, 2]):</p> <pre><code>[\n    [-0.4757,  0.2864],\n    [ 0.2532,  0.5876],\n    [-0.3652, -0.4820],\n    [ 0.3752, -0.2858],\n    [ 0.4292, -0.1678],\n    [ 0.4045, -0.5494],\n    [-0.3568,  0.2156],\n    [ 0.1495, -0.1803]\n]\n</code></pre> </li> <li> <p>fc_dec1.bias (shape [8]):</p> <pre><code>[ 0.4215,  0.4807, -0.5128, -0.3775,  0.6475, -0.2386, -0.2507, -0.6842 ]\n</code></pre> </li> <li> <p>fc_dec2.weight (decoder hidden to output, shape [4, 8]):</p> <pre><code>[\n    [-0.2025,  0.0883, -0.0467, -0.2566,  0.0083, -0.2415, -0.3000, -0.1947],\n    [-0.3094, -0.2251,  0.3534,  0.0668,  0.1090, -0.3298, -0.2322, -0.1177],\n    [ 0.0553, -0.3111, -0.1523, -0.2117,  0.0010, -0.1316, -0.0245, -0.2396],\n    [-0.2427, -0.2063, -0.1210, -0.2791,  0.2964, -0.0702,  0.3042,  0.1102]\n]\n</code></pre> </li> <li> <p>fc_dec2.bias (shape [4]):</p> <pre><code>[ -0.2994,  0.2447, -0.0973, -0.1355 ]\n</code></pre> </li> </ul>"}, {"location": "classes/variational-autoencoders/#forward-pass", "title": "Forward Pass", "text": "<ol> <li> <p>Input:</p> <pre><code>x = [ 1.0, 2.0, 3.0, 4.0 ]\n</code></pre> <p>(batch size 1, dim 4).</p> </li> <li> <p>Encoding to Hidden Layer:</p> <ul> <li> <p>Compute pre-ReLU: fc1(x) = fc1.weight @ x^T + fc1.bias.</p> <ul> <li>This is a matrix multiplication: Each row of fc1.weight dotted with x, plus bias.</li> <li>Result (pre-ReLU):     <pre><code>[ 3.0842,  0.6196,  1.223,   1.2547,  0.4205,  0.7739, -2.427,  -1.4421 ]\n</code></pre></li> <li>After ReLU (non-negative):     <pre><code>encoder_8 = [ 3.0842, 0.6196, 1.223,  1.2547, 0.4205, 0.7739, 0., 0. ]\n</code></pre>     (note: last two are zeroed by ReLU).</li> </ul> </li> </ul> </li> <li> <p>Compute Mean (\u03bc) in Latent Space:</p> <ul> <li>\u03bc = fc_mu.weight @ hidden^T + fc_mu.bias.<ul> <li>Result:     <pre><code>\u03bc = [ 0.88977581, -0.07508313 ]\n</code></pre></li> <li>This is the mean of the 2D latent Gaussian.</li> </ul> </li> </ul> </li> <li> <p>Compute Log-Variance (logvar) in Latent Space:</p> <ul> <li> <p>logvar = fc_logvar.weight @ hidden^T + fc_logvar.bias.</p> <ul> <li>Result:     <pre><code>logvar = [ 0.02314189, 0.20015677 ]\n</code></pre></li> <li>Variance \u03c3\u00b2 = exp(logvar):     <pre><code>variance = [ 1.02341174, 1.22159425 ]\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Latent Space: Sampling z (Reparameterization Trick):</p> <ul> <li> <p>std (\u03c3) = exp(0.5 * logvar):     </p><pre><code>std = [ 1.01163815, 1.10525755 ]\n</code></pre><p></p> </li> <li> <p>\u03b5 ~ N(0, 1) (seeded random):     </p><pre><code>\u03b5 = [ -0.2387, -0.5050 ]\n</code></pre><p></p> </li> <li> <p>\\( z = \\mu + std * \\epsilon \\)</p> </li> <li> <p>Result:     </p><pre><code>z = [ 0.88977581 + 1.01163815*(-0.2387), -0.07508313 + 1.10525755*(-0.5050) ] \n    \u2248 [ 0.64829778, -0.63323819 ]\n</code></pre><p></p> </li> </ul> </li> <li> <p>Decoding to Reconstructed Output:</p> <ul> <li>Decoder: ReLu( fc_dec1.weight @ z^T + fc_dec1.bias ).<ul> <li>pre-ReLU:     <pre><code>decoder_hidden = [ -0.06825467,  0.27275824, -0.44433754,  0.0467208,   1.03200678,  0.37153752, -0.6185388,  -0.47310664 ]\n</code></pre></li> <li>After ReLU:     <pre><code>decoder_hidden = [ 0., 0.27275824, 0., 0.0467208,  1.03200678, 0.37153752, 0., 0. ]\n</code></pre></li> </ul> </li> <li> <p>recon_x = fc_dec2.weight @ decoder_hidden^T + fc_dec2.bias.</p> <ul> <li> <p>Result:</p> <pre><code>recon_x = [ -0.36846466,  0.17637874, -0.23990821,  0.07499507 ]\n</code></pre> </li> </ul> </li> </ul> </li> </ol>"}, {"location": "classes/variational-autoencoders/#loss-calculation", "title": "Loss Calculation", "text": "<ul> <li> <p>Reconstruction Loss (MSE):</p> <p>Sum over dimensions of \\( (x - \\hat{x})^2 \u2248 31.100958927489703 \\)</p> </li> <li> <p>KL Divergence:</p> \\[ \\text{KL} = -0.5 * \\sum \\left(1 + \\text{logvar} - \\mu^2 - \\exp(\\text{logvar})\\right) \\approx 0.40952290104490313 \\] </li> <li> <p>Total Loss:</p> \\[ \\text{Loss} = \\text{MSE} + \\text{KL} \\approx 31.510481828534605 \\] </li> </ul>"}, {"location": "classes/variational-autoencoders/#backward-pass", "title": "Backward Pass", "text": "<p>The backward pass computes gradients via autograd (chain rule from loss back through the network). This enables training by updating weights (e.g., via SGD). Gradients are zero-initialized before .backward().</p> <p>After loss.backward(), key gradients \\( \\displaystyle \\frac{\\partial \\text{Loss}}{\\partial \\text{param}} \\) are:</p> <p>Decoder</p> <ul> <li> <p>fc_dec2.weight.grad (shape [4, 8]):</p> \\[ \\displaystyle \\frac{\\partial \\text{L}}{\\partial \\text{fc_dec2.weight}} \\] <pre><code>[\n    [-0.0000, -0.7467, -0.0000, -0.1278, -2.8242, -1.0167, -0.0000, -0.0000],\n    [-0.0000, -0.9951, -0.0000, -0.1703, -3.7638, -1.3549, -0.0000, -0.0000],\n    [-0.0000, -1.7679, -0.0000, -0.3025, -6.6867, -2.4071, -0.0000, -0.0000],\n    [-0.0000, -2.1417, -0.0000, -0.3664, -8.1006, -2.9161, -0.0000, -0.0000]\n]\n</code></pre> </li> <li> <p>fc_dec2.bias.grad (shape [4]): </p> <pre><code>[ -2.7369, -3.6474, -6.4798, -7.8500 ]\n</code></pre> </li> <li> <p>fc_dec1.weight.grad (shape [8, 2]):</p> <pre><code>[\n    [ 0.0000, -0.0000],\n    [ 2.7321, -2.6684],\n    [ 0.0000, -0.0000],\n    [ 2.6066, -2.5459],\n    [-1.7850,  1.7434],\n    [ 2.1179, -2.0685],\n    [ 0.0000, -0.0000],\n    [ 0.0000, -0.0000]\n]\n</code></pre> <ul> <li>Primarily from MSE, backpropagated through decoder.</li> </ul> </li> <li> <p>fc_dec1.bias.grad (shape [8]):</p> <pre><code>[ 0.0000,  4.2144,  0.0000,  4.0209, -2.7535,  3.2670,  0.0000,  0.0000 ]\n</code></pre> </li> </ul> <p>Encoder</p> <ul> <li> <p>fc_mu.weight.grad (shape [2, 8]):</p> <p></p><pre><code>[\n    [11.1188,  2.2342,  4.4088,  4.5235,  1.5168,  2.7899,  0.0000,  0.0000],\n    [-0.2493, -0.0501, -0.0989, -0.1014, -0.0340, -0.0626, -0.0000, -0.0000]\n]\n</code></pre> - Includes \u2202KL/\u2202\u03bc \u2248 \u03bc (pulling toward 0) + flow from MSE via z.<p></p> </li> <li> <p>fc_mu.bias.grad (shape [2]):</p> <pre><code>[ 3.6052, -0.0808 ]\n</code></pre> </li> <li> <p>fc_logvar.weight.grad (shape [2, 8]):</p> <p></p><pre><code>[\n    [-0.9752, -0.1960, -0.3867, -0.3967, -0.1330, -0.2447, -0.0000, -0.0000],\n    [ 0.3473,  0.0698,  0.1377,  0.1413,  0.0474,  0.0871,  0.0000,  0.0000]\n]\n</code></pre> - From \u2202KL/\u2202logvar \u2248 0.5*(exp(logvar) - 1) + MSE flow.<p></p> </li> <li> <p>fc_logvar.bias.grad (shape [2]):</p> <pre><code>[ -0.3162,  0.1126 ]\n</code></pre> </li> <li> <p>fc1.weight.grad (shape [8, 4]):     </p><pre><code>[\n    [ 0.2735,  0.5470,  0.8204,  1.0939],\n    [-0.2724, -0.5448, -0.8172, -1.0896],\n    [ 0.5339,  1.0679,  1.6018,  2.1358],\n    [ 1.2016,  2.4032,  3.6049,  4.8065],\n    [ 0.7601,  1.5201,  2.2802,  3.0403],\n    [-0.5289, -1.0578, -1.5868, -2.1157],\n    [ 0.0000,  0.0000,  0.0000,  0.0000],\n    [ 0.0000,  0.0000,  0.0000,  0.0000]\n]\n</code></pre><p></p> <ul> <li>These flow from both MSE (via reconstruction) and KL (via \u03bc/logvar). Zeros in last rows due to ReLU zeroing those hidden units.</li> </ul> </li> <li> <p>fc1.bias.grad (shape [8]):</p> <pre><code>[ 0.2735, -0.2724,  0.5339,  1.2016,  0.7601, -0.5289,  0.0000,  0.0000 ]\n</code></pre> </li> </ul> <p>These gradients would update parameters in training (e.g., param -= lr * grad). Note zeros where ReLU gates flow. This example uses a single pass; real training iterates over datasets. If you change the seed, input, or dimensions, values will differ, but the process remains identical.</p>"}, {"location": "classes/variational-autoencoders/#additional", "title": "Additional", "text": ""}, {"location": "classes/variational-autoencoders/#relation-between-log-variance-and-standard-deviation", "title": "Relation between Log Variance and Standard Deviation", "text": "Relation between Log Variance and Standard Deviation <ul> <li>In VAEs, the encoder outputs the mean \\( \\mu \\) and log variance \\( \\log(\\sigma^2) \\) of the latent space distribution.</li> <li>The standard deviation \\( \\sigma \\) can be derived from the log variance using the relationship:</li> </ul> \\[ \\sigma = \\exp\\left(\\frac{1}{2} \\log(\\sigma^2)\\right) \\] <ul> <li>This transformation ensures numerical stability and positivity of the variance during training.</li> </ul> <ol> <li> <p>Sharma, A. \u201cIntroduction to Autoencoders,\u201d PyImageSearch, P. Chugh, A. R. Gosthipaty, S. Huot, K. Kidriavsteva, and R. Raha, eds., 2023.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Bandyopadhyay, H. \"What is an autoencoder and how does it work? Learn about most common types of autoencoders and their applications in machine learning.\".\u00a0\u21a9\u21a9</p> </li> <li> <p>Sharma, A. \u201cA Deep Dive into Variational Autoencoders with PyTorch,\u201d PyImageSearch, P. Chugh, A. R. Gosthipaty, S. Huot, K. Kidriavsteva, and R. Raha, eds., 2023.\u00a0\u21a9</p> </li> <li> <p>Wikipedia - Kullback\u2013Leibler divergence.\u00a0\u21a9</p> </li> <li> <p>GeeksforGeeks - Understanding KL Divergence in PyTorch.\u00a0\u21a9</p> </li> <li> <p>DataCamp - Variational Autoencoders.\u00a0\u21a9</p> </li> </ol>"}, {"location": "classes/variational-autoencoders/#1-definitions", "title": "1. Definitions", "text": "<p>For a random variable ( x ) that follows a normal distribution:</p> \\[ x \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] <p>where:</p> <ul> <li>\\( \\mu \\): mean</li> <li>\\( \\sigma^2 \\): variance</li> <li>\\( \\sigma \\): standard deviation</li> </ul>"}, {"location": "classes/variational-autoencoders/#2-log-variance", "title": "2. Log variance", "text": "<p>Often, instead of directly predicting or storing the variance \\( \\sigma^2 \\) or standard deviation \\( \\sigma \\), models work with the log variance:</p> \\[ \\displaystyle \\text{log_var} = \\log(\\sigma^2) \\]"}, {"location": "classes/variational-autoencoders/#3-relationship-between-log-variance-and-std", "title": "3. Relationship between log variance and std", "text": "<p>From the above definition:</p> \\[ \\displaystyle \\sigma^2 = e^{\\text{log_var}} \\] <p>Taking the square root to get the standard deviation:</p> \\[ \\displaystyle \\sigma = \\displaystyle \\sqrt{e^{\\text{log_var}}} = \\displaystyle e^{\\frac{1}{2}\\text{log_var}} \\] <p>So:</p> \\[ \\displaystyle \\boxed{\\sigma = \\exp\\left(\\frac{1}{2} \\cdot \\text{log_var}\\right)} \\] <p>and conversely,</p> \\[ \\displaystyle \\boxed{\\text{log_var} = 2 \\cdot \\log(\\sigma)} \\]"}, {"location": "classes/variational-autoencoders/#4-why-use-log-variance", "title": "4. Why use log variance?", "text": "<p>It\u2019s common in neural nets because:</p> <ul> <li>It ensures the variance is always positive (since \\( e^x &gt; 0 \\)).</li> <li>It\u2019s numerically more stable when optimizing.</li> <li>It allows unconstrained outputs from the network (no need to force positivity).</li> </ul>"}, {"location": "classes/variational-autoencoders/#summary", "title": "Summary", "text": "Quantity Expression In terms of log_var Variance \\( \\sigma^2 \\) \\( e^{\\text{log_var}} \\) Std. deviation \\( \\sigma \\) \\( e^{\\frac{1}{2}\\text{log_var}} \\) Log variance \\( \\text{log_var} \\) \\( 2 \\log(\\sigma) \\)"}, {"location": "classes/variational-autoencoders/relation-log-variance-std/", "title": "Relation log variance std", "text": ""}, {"location": "classes/variational-autoencoders/relation-log-variance-std/#1-definitions", "title": "1. Definitions", "text": "<p>For a random variable ( x ) that follows a normal distribution:</p> \\[ x \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] <p>where:</p> <ul> <li>\\( \\mu \\): mean</li> <li>\\( \\sigma^2 \\): variance</li> <li>\\( \\sigma \\): standard deviation</li> </ul>"}, {"location": "classes/variational-autoencoders/relation-log-variance-std/#2-log-variance", "title": "2. Log variance", "text": "<p>Often, instead of directly predicting or storing the variance \\( \\sigma^2 \\) or standard deviation \\( \\sigma \\), models work with the log variance:</p> \\[ \\displaystyle \\text{log_var} = \\log(\\sigma^2) \\]"}, {"location": "classes/variational-autoencoders/relation-log-variance-std/#3-relationship-between-log-variance-and-std", "title": "3. Relationship between log variance and std", "text": "<p>From the above definition:</p> \\[ \\displaystyle \\sigma^2 = e^{\\text{log_var}} \\] <p>Taking the square root to get the standard deviation:</p> \\[ \\displaystyle \\sigma = \\displaystyle \\sqrt{e^{\\text{log_var}}} = \\displaystyle e^{\\frac{1}{2}\\text{log_var}} \\] <p>So:</p> \\[ \\displaystyle \\boxed{\\sigma = \\exp\\left(\\frac{1}{2} \\cdot \\text{log_var}\\right)} \\] <p>and conversely,</p> \\[ \\displaystyle \\boxed{\\text{log_var} = 2 \\cdot \\log(\\sigma)} \\]"}, {"location": "classes/variational-autoencoders/relation-log-variance-std/#4-why-use-log-variance", "title": "4. Why use log variance?", "text": "<p>It\u2019s common in neural nets because:</p> <ul> <li>It ensures the variance is always positive (since \\( e^x &gt; 0 \\)).</li> <li>It\u2019s numerically more stable when optimizing.</li> <li>It allows unconstrained outputs from the network (no need to force positivity).</li> </ul>"}, {"location": "classes/variational-autoencoders/relation-log-variance-std/#summary", "title": "Summary", "text": "Quantity Expression In terms of log_var Variance \\( \\sigma^2 \\) \\( e^{\\text{log_var}} \\) Std. deviation \\( \\sigma \\) \\( e^{\\frac{1}{2}\\text{log_var}} \\) Log variance \\( \\text{log_var} \\) \\( 2 \\log(\\sigma) \\)"}, {"location": "classes/variational-autoencoders/vae-numerical-simulation/", "title": "Vae numerical simulation", "text": "<p>A Variational Autoencoder (VAE) encodes input data into a probabilistic latent space (defined by mean \u03bc and log-variance log(\u03c3\u00b2)) and decodes it back to reconstruct the input. The latent space is sampled using the reparameterization trick for differentiability. The loss combines reconstruction error (MSE) and KL divergence to regularize the latent distribution toward a standard normal.</p> <p>For this numerical example, we've scaled up to:</p> <ul> <li>Input dimension: 4 (e.g., a vector like <code>[1.0, 2.0, 3.0, 4.0]</code>)</li> <li>Latent dimension: 2</li> <li>Output dimension: 4 (reconstruction of input)</li> <li>Hidden layer size: 8 (for both encoder and decoder, to add capacity)</li> </ul> <p>The model uses PyTorch with random initialization (seeded at 42 for reproducibility). All calculations are shown step-by-step, including matrix multiplications where relevant. Weights and biases are explicitly listed below.</p>"}, {"location": "classes/variational-autoencoders/vae-numerical-simulation/#model-architecture", "title": "Model Architecture", "text": "<ul> <li> <p>Encoder:</p> <ul> <li>Linear (fc1): 4 inputs \u2192 8 hidden units, followed by ReLU.</li> <li>Linear to \u03bc (fc_mu): 8 \u2192 2.</li> <li>Linear to logvar (fc_logvar): 8 \u2192 2.</li> </ul> </li> <li> <p>Latent: Sample z from N(\u03bc, \u03c3\u00b2) using reparameterization trick.</p> </li> <li> <p>Decoder:</p> <ul> <li>Linear (fc_dec1): 2 latent \u2192 8 hidden units, followed by ReLU.</li> <li>Linear to output (fc_dec2): 8 \u2192 4 (no final activation, assuming Gaussian output for simplicity).</li> </ul> </li> <li> <p>Loss: Summed MSE for reconstruction + KL divergence (without \u03b2 annealing).</p> </li> </ul>"}, {"location": "classes/variational-autoencoders/vae-numerical-simulation/#weights-and-biases", "title": "Weights and Biases", "text": "<p>All parameters are initialized randomly (via torch.manual_seed(42)). Here they are:</p> <p>Encoder</p> <ul> <li> <p>fc1.weight (encoder input to hidden, shape [8, 4]):</p> <pre><code>[\n    [ 0.3823,  0.4150, -0.1171,  0.4593],\n    [-0.1096,  0.1009, -0.2434,  0.2936],\n    [ 0.4408, -0.3668,  0.4346,  0.0936],\n    [ 0.3694,  0.0677,  0.2411, -0.0706],\n    [ 0.3854,  0.0739, -0.2334,  0.1274],\n    [-0.2304, -0.0586, -0.2031,  0.3317],\n    [-0.3947, -0.2305, -0.1412, -0.3006],\n    [ 0.0472, -0.4938,  0.4516, -0.4247]\n]\n</code></pre> </li> <li> <p>fc1.bias (shape [8]):</p> <pre><code>[ 0.3860,  0.0832, -0.1624,  0.3090,  0.0779,  0.4040,  0.0547, -0.1577 ]\n</code></pre> </li> <li> <p>fc_mu.weight (hidden to \u03bc, shape [2, 8]):</p> <pre><code>[\n    [ 0.0950, -0.0959,  0.1488,  0.3157,  0.2044, -0.1546,  0.2041,  0.0633],\n    [ 0.1795, -0.2155, -0.3500, -0.1366, -0.2712,  0.2901,  0.1018,  0.1464]\n]\n</code></pre> </li> <li> <p>fc_mu.bias (shape [2]):</p> <pre><code>[ 0.1118, -0.0062 ]\n</code></pre> </li> <li> <p>fc_logvar.weight (hidden to logvar, shape [2, 8]):</p> <pre><code>[\n    [ 0.2767, -0.2512,  0.0223, -0.2413,  0.1090, -0.1218,  0.1083, -0.0737],\n    [ 0.2932, -0.2096, -0.2109, -0.2109,  0.3180,  0.1178,  0.3402, -0.2918]\n]\n</code></pre> </li> <li> <p>fc_logvar.bias (shape [2]):</p> <pre><code>[ -0.3507, -0.2766 ]\n</code></pre> </li> </ul> <p>Decoder</p> <ul> <li> <p>fc_dec1.weight (latent to decoder hidden, shape [8, 2]):</p> <pre><code>[\n    [-0.4757,  0.2864],\n    [ 0.2532,  0.5876],\n    [-0.3652, -0.4820],\n    [ 0.3752, -0.2858],\n    [ 0.4292, -0.1678],\n    [ 0.4045, -0.5494],\n    [-0.3568,  0.2156],\n    [ 0.1495, -0.1803]\n]\n</code></pre> </li> <li> <p>fc_dec1.bias (shape [8]):</p> <pre><code>[ 0.4215,  0.4807, -0.5128, -0.3775,  0.6475, -0.2386, -0.2507, -0.6842 ]\n</code></pre> </li> <li> <p>fc_dec2.weight (decoder hidden to output, shape [4, 8]):</p> <pre><code>[\n    [-0.2025,  0.0883, -0.0467, -0.2566,  0.0083, -0.2415, -0.3000, -0.1947],\n    [-0.3094, -0.2251,  0.3534,  0.0668,  0.1090, -0.3298, -0.2322, -0.1177],\n    [ 0.0553, -0.3111, -0.1523, -0.2117,  0.0010, -0.1316, -0.0245, -0.2396],\n    [-0.2427, -0.2063, -0.1210, -0.2791,  0.2964, -0.0702,  0.3042,  0.1102]\n]\n</code></pre> </li> <li> <p>fc_dec2.bias (shape [4]):</p> <pre><code>[ -0.2994,  0.2447, -0.0973, -0.1355 ]\n</code></pre> </li> </ul>"}, {"location": "classes/variational-autoencoders/vae-numerical-simulation/#forward-pass", "title": "Forward Pass", "text": "<ol> <li> <p>Input:</p> <pre><code>x = [ 1.0, 2.0, 3.0, 4.0 ]\n</code></pre> <p>(batch size 1, dim 4).</p> </li> <li> <p>Encoding to Hidden Layer:</p> <ul> <li> <p>Compute pre-ReLU: fc1(x) = fc1.weight @ x^T + fc1.bias.</p> <ul> <li>This is a matrix multiplication: Each row of fc1.weight dotted with x, plus bias.</li> <li>Result (pre-ReLU):     <pre><code>[ 3.0842,  0.6196,  1.223,   1.2547,  0.4205,  0.7739, -2.427,  -1.4421 ]\n</code></pre></li> <li>After ReLU (non-negative):     <pre><code>encoder_8 = [ 3.0842, 0.6196, 1.223,  1.2547, 0.4205, 0.7739, 0., 0. ]\n</code></pre>     (note: last two are zeroed by ReLU).</li> </ul> </li> </ul> </li> <li> <p>Compute Mean (\u03bc) in Latent Space:</p> <ul> <li>\u03bc = fc_mu.weight @ hidden^T + fc_mu.bias.<ul> <li>Result:     <pre><code>\u03bc = [ 0.88977581, -0.07508313 ]\n</code></pre></li> <li>This is the mean of the 2D latent Gaussian.</li> </ul> </li> </ul> </li> <li> <p>Compute Log-Variance (logvar) in Latent Space:</p> <ul> <li> <p>logvar = fc_logvar.weight @ hidden^T + fc_logvar.bias.</p> <ul> <li>Result:     <pre><code>logvar = [ 0.02314189, 0.20015677 ]\n</code></pre></li> <li>Variance \u03c3\u00b2 = exp(logvar):     <pre><code>variance = [ 1.02341174, 1.22159425 ]\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Latent Space: Sampling z (Reparameterization Trick):</p> <ul> <li> <p>std (\u03c3) = exp(0.5 * logvar):     </p><pre><code>std = [ 1.01163815, 1.10525755 ]\n</code></pre><p></p> </li> <li> <p>\u03b5 ~ N(0, 1) (seeded random):     </p><pre><code>\u03b5 = [ -0.2387, -0.5050 ]\n</code></pre><p></p> </li> <li> <p>\\( z = \\mu + std * \\epsilon \\)</p> </li> <li> <p>Result:     </p><pre><code>z = [ 0.88977581 + 1.01163815*(-0.2387), -0.07508313 + 1.10525755*(-0.5050) ] \n    \u2248 [ 0.64829778, -0.63323819 ]\n</code></pre><p></p> </li> </ul> </li> <li> <p>Decoding to Reconstructed Output:</p> <ul> <li>Decoder: ReLu( fc_dec1.weight @ z^T + fc_dec1.bias ).<ul> <li>pre-ReLU:     <pre><code>decoder_hidden = [ -0.06825467,  0.27275824, -0.44433754,  0.0467208,   1.03200678,  0.37153752, -0.6185388,  -0.47310664 ]\n</code></pre></li> <li>After ReLU:     <pre><code>decoder_hidden = [ 0., 0.27275824, 0., 0.0467208,  1.03200678, 0.37153752, 0., 0. ]\n</code></pre></li> </ul> </li> <li> <p>recon_x = fc_dec2.weight @ decoder_hidden^T + fc_dec2.bias.</p> <ul> <li> <p>Result:</p> <pre><code>recon_x = [ -0.36846466,  0.17637874, -0.23990821,  0.07499507 ]\n</code></pre> </li> </ul> </li> </ul> </li> </ol>"}, {"location": "classes/variational-autoencoders/vae-numerical-simulation/#loss-calculation", "title": "Loss Calculation", "text": "<ul> <li> <p>Reconstruction Loss (MSE):</p> <p>Sum over dimensions of \\( (x - \\hat{x})^2 \u2248 31.100958927489703 \\)</p> </li> <li> <p>KL Divergence:</p> \\[ \\text{KL} = -0.5 * \\sum \\left(1 + \\text{logvar} - \\mu^2 - \\exp(\\text{logvar})\\right) \\approx 0.40952290104490313 \\] </li> <li> <p>Total Loss:</p> \\[ \\text{Loss} = \\text{MSE} + \\text{KL} \\approx 31.510481828534605 \\] </li> </ul>"}, {"location": "classes/variational-autoencoders/vae-numerical-simulation/#backward-pass", "title": "Backward Pass", "text": "<p>The backward pass computes gradients via autograd (chain rule from loss back through the network). This enables training by updating weights (e.g., via SGD). Gradients are zero-initialized before .backward().</p> <p>After loss.backward(), key gradients \\( \\displaystyle \\frac{\\partial \\text{Loss}}{\\partial \\text{param}} \\) are:</p> <p>Decoder</p> <ul> <li> <p>fc_dec2.weight.grad (shape [4, 8]):</p> \\[ \\displaystyle \\frac{\\partial \\text{L}}{\\partial \\text{fc_dec2.weight}} \\] <pre><code>[\n    [-0.0000, -0.7467, -0.0000, -0.1278, -2.8242, -1.0167, -0.0000, -0.0000],\n    [-0.0000, -0.9951, -0.0000, -0.1703, -3.7638, -1.3549, -0.0000, -0.0000],\n    [-0.0000, -1.7679, -0.0000, -0.3025, -6.6867, -2.4071, -0.0000, -0.0000],\n    [-0.0000, -2.1417, -0.0000, -0.3664, -8.1006, -2.9161, -0.0000, -0.0000]\n]\n</code></pre> </li> <li> <p>fc_dec2.bias.grad (shape [4]): </p> <pre><code>[ -2.7369, -3.6474, -6.4798, -7.8500 ]\n</code></pre> </li> <li> <p>fc_dec1.weight.grad (shape [8, 2]):</p> <pre><code>[\n    [ 0.0000, -0.0000],\n    [ 2.7321, -2.6684],\n    [ 0.0000, -0.0000],\n    [ 2.6066, -2.5459],\n    [-1.7850,  1.7434],\n    [ 2.1179, -2.0685],\n    [ 0.0000, -0.0000],\n    [ 0.0000, -0.0000]\n]\n</code></pre> <ul> <li>Primarily from MSE, backpropagated through decoder.</li> </ul> </li> <li> <p>fc_dec1.bias.grad (shape [8]):</p> <pre><code>[ 0.0000,  4.2144,  0.0000,  4.0209, -2.7535,  3.2670,  0.0000,  0.0000 ]\n</code></pre> </li> </ul> <p>Encoder</p> <ul> <li> <p>fc_mu.weight.grad (shape [2, 8]):</p> <p></p><pre><code>[\n    [11.1188,  2.2342,  4.4088,  4.5235,  1.5168,  2.7899,  0.0000,  0.0000],\n    [-0.2493, -0.0501, -0.0989, -0.1014, -0.0340, -0.0626, -0.0000, -0.0000]\n]\n</code></pre> - Includes \u2202KL/\u2202\u03bc \u2248 \u03bc (pulling toward 0) + flow from MSE via z.<p></p> </li> <li> <p>fc_mu.bias.grad (shape [2]):</p> <pre><code>[ 3.6052, -0.0808 ]\n</code></pre> </li> <li> <p>fc_logvar.weight.grad (shape [2, 8]):</p> <p></p><pre><code>[\n    [-0.9752, -0.1960, -0.3867, -0.3967, -0.1330, -0.2447, -0.0000, -0.0000],\n    [ 0.3473,  0.0698,  0.1377,  0.1413,  0.0474,  0.0871,  0.0000,  0.0000]\n]\n</code></pre> - From \u2202KL/\u2202logvar \u2248 0.5*(exp(logvar) - 1) + MSE flow.<p></p> </li> <li> <p>fc_logvar.bias.grad (shape [2]):</p> <pre><code>[ -0.3162,  0.1126 ]\n</code></pre> </li> <li> <p>fc1.weight.grad (shape [8, 4]):     </p><pre><code>[\n    [ 0.2735,  0.5470,  0.8204,  1.0939],\n    [-0.2724, -0.5448, -0.8172, -1.0896],\n    [ 0.5339,  1.0679,  1.6018,  2.1358],\n    [ 1.2016,  2.4032,  3.6049,  4.8065],\n    [ 0.7601,  1.5201,  2.2802,  3.0403],\n    [-0.5289, -1.0578, -1.5868, -2.1157],\n    [ 0.0000,  0.0000,  0.0000,  0.0000],\n    [ 0.0000,  0.0000,  0.0000,  0.0000]\n]\n</code></pre><p></p> <ul> <li>These flow from both MSE (via reconstruction) and KL (via \u03bc/logvar). Zeros in last rows due to ReLU zeroing those hidden units.</li> </ul> </li> <li> <p>fc1.bias.grad (shape [8]):</p> <pre><code>[ 0.2735, -0.2724,  0.5339,  1.2016,  0.7601, -0.5289,  0.0000,  0.0000 ]\n</code></pre> </li> </ul> <p>These gradients would update parameters in training (e.g., param -= lr * grad). Note zeros where ReLU gates flow. This example uses a single pass; real training iterates over datasets. If you change the seed, input, or dimensions, values will differ, but the process remains identical.</p>"}, {"location": "definitions/latent_space_vs_embedding/", "title": "Latent Space vs. Embedding", "text": ""}, {"location": "definitions/latent_space_vs_embedding/#latent-space", "title": "Latent Space", "text": "<p>In AI, particularly in generative models like Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), a latent space refers to a compressed, lower-dimensional representation of data learned by the model during training. It captures the underlying structure or \"essence\" of the input data in a way that allows for meaningful interpolation, generation, or manipulation. For example:</p> <ul> <li>In a VAE, the latent space is often probabilistic (e.g., modeled as a Gaussian distribution), enabling the generation of new data points by sampling from it;</li> <li>Points in latent space are not human-interpretable but encode abstract features (e.g., style or pose in images);</li> <li>It's typically continuous and high-dimensional (e.g., 100\u2013512 dimensions), designed for tasks like data synthesis or anomaly detection.</li> </ul>"}, {"location": "definitions/latent_space_vs_embedding/#embedding", "title": "Embedding", "text": "<p>An embedding is a dense, fixed-size vector representation of discrete data (e.g., words, users, or items) learned via models like Word2Vec, BERT, or collaborative filtering systems. It maps high-dimensional, sparse inputs (e.g., one-hot encoded words) into a continuous vector space where semantic similarities are preserved\u2014similar items are closer together. For example:</p> <ul> <li>In natural language processing (NLP), word embeddings like those from GloVe place \"king\" and \"queen\" near each other based on context;</li> <li>Embeddings are task-specific and can be static (pre-trained) or contextual (e.g., transformer-based);</li> <li>They're often used in recommendation systems, search, or classification, with dimensions ranging from 50\u2013768.</li> </ul>"}, {"location": "definitions/latent_space_vs_embedding/#key-differences", "title": "Key Differences", "text": "<p>While both involve vector representations, they serve distinct purposes in AI workflows. Here's a comparison:</p> Aspect Latent Space Embedding Primary Context Generative models (e.g., VAEs, GANs) Representation learning (e.g., NLP, recommendations) Purpose Compression for generation/reconstruction; enables interpolation (e.g., morphing images) Capturing semantic similarity for downstream tasks like classification or retrieval Dimensionality Often higher-dimensional, continuous, and probabilistic Lower-dimensional, dense vectors; typically deterministic Training Focus Learned via encoder-decoder architectures to minimize reconstruction loss Learned via objectives like skip-gram (Word2Vec) or masked language modeling (BERT) Interpretability Abstract and non-intuitive; optimized for data distribution matching More interpretable (e.g., cosine similarity measures relatedness) Use Case Example Generating new faces in StyleGAN by navigating latent space Finding similar products in e-commerce via user/item embeddings <p>In summary, latent spaces are about creating new data from hidden patterns, while embeddings are about representing existing data for efficient similarity computations. The terms can overlap (e.g., embeddings sometimes form a latent space in autoencoders), but the distinction lies in their generative vs. representational roles.</p>"}, {"location": "definitions/stable_difussion_vs_flow-matching/", "title": "Stable Diffusion vs. Flow-Matching", "text": "<p>Stable Diffusion (SD) is a family of latent diffusion models developed by Stability AI for text-to-image generation, operating in a compressed latent space for efficiency. As of November 2025, the latest iteration is Stable Diffusion 3.5 (released in late 2024), which incorporates a Diffusion Transformer (DiT) architecture and shifts from traditional noise-prediction to flow-matching principles for improved stability and performance. Earlier versions like SD 1.x and SDXL (up to 2023) rely on classic diffusion processes.</p> <p>Flow-matching (FM) models represent a newer paradigm in generative modeling, introduced in 2022, that trains continuous normalizing flows (CNFs) by regressing vector fields along fixed conditional probability paths from noise to data. Unlike traditional diffusion, FM enables \"straight-line\" trajectories in the generation process, leading to faster inference and more stable training. FM is not a specific model but a framework; prominent examples include Flux (by Black Forest Labs, 2024) and SD 3.5 itself, which hybridizes diffusion with FM.</p> <p>Both approaches generate images by transforming Gaussian noise into structured data (e.g., via text prompts), but they differ in their mathematical foundations, training dynamics, and practical trade-offs. Below, I'll break down the key differences, with a focus on the latest SD (3.5) versus pure FM models like Flux.</p>"}, {"location": "definitions/stable_difussion_vs_flow-matching/#key-differences-traditional-diffusion-vs-flow-matching", "title": "Key Differences: Traditional Diffusion vs. Flow-Matching", "text": "<p>Traditional diffusion models (e.g., pre-SD 3.0) add noise gradually to data and learn to reverse it stochastically. FM simplifies this by directly learning deterministic flows. SD 3.5 bridges the gap by using FM objectives on diffusion-like paths. Here's a side-by-side comparison:</p> Aspect Latest Stable Diffusion (SD 3.5) Flow-Matching Models (e.g., Flux) Core Mechanism Hybrid: Predicts velocity fields via FM on diffusion paths (Gaussian noise to data). Uses v-prediction (velocity MSE loss) for stability. Pure FM: Regresses conditional velocity fields on straight-line paths (optimal transport or linear interpolation) from noise to data. No explicit noise scheduling. Sampling Path Curved/stochastic paths (inherits from diffusion), but FM weighting straightens them for fewer steps (e.g., 20-50 NFEs). Straight deterministic paths, enabling ultra-fast inference (e.g., 1-4 steps in distilled variants). Training Objective MSE on velocity (v-MSE) with cosine noise schedule; more stable than pure noise prediction but requires careful SNR handling to avoid gray tones. Simple MSE on velocity fields; simulation-free, robust to hyperparameters, and faster convergence. Inference Speed Moderate (20-100 steps); improved over SDXL but slower than pure FM due to latent diffusion overhead. Very fast (4-10 steps standard, 1-2 with distillation); excels in real-time apps. Image Quality &amp; Fidelity Excellent text adherence and detail (e.g., anatomy, prompts); FID ~2-5 on benchmarks. Strong in diverse styles but can over-smooth. Superior in coherence and sharpness (FID ~1-3); better at complex scenes, fewer artifacts. Outperforms SD on likelihood and sample quality in ImageNet tests. Scalability &amp; Efficiency Efficient in latent space (U-Net/DiT hybrid, 800M-8B params); easy fine-tuning via ecosystem (e.g., LoRAs). Compute: ~10-20% less than SDXL. Highly scalable for large models (12B+ params in Flux); lower training variance, but foundation models harder to fine-tune without diffusion priors. Strengths Vast ecosystem (Hugging Face, ComfyUI); great for customization (ControlNet, inpainting). Simpler math, fewer steps, better for high-res/video/audio extensions. More robust empirically. Weaknesses Still tied to diffusion quirks (e.g., non-zero terminal SNR causing muted colors); higher step count. Less mature ecosystem; conditional generation (e.g., text) requires adaptations like posterior sampling. Use Cases General text-to-image, editing (e.g., via Stable Flow layers for training-free edits). High-speed generation, multimodal (e.g., Flux for video/audio); emerging in bio/mol design."}, {"location": "definitions/stable_difussion_vs_flow-matching/#detailed-explanation", "title": "Detailed Explanation", "text": "<ul> <li> <p>How Diffusion Works (Pre-SD 3.5 Baseline): Starts with data \\( x_0 \\), adds noise over time \\( t \\) to reach Gaussian noise \\( x_1 \\). The model learns to denoise by predicting added noise \\( \\epsilon \\) (or score/velocity in variants). Sampling is iterative and stochastic, often requiring 50+ steps for quality. This leads to curved paths, which can be inefficient.</p> </li> <li> <p>How Flow-Matching Works: Defines a continuous flow \\( \\phi_t \\) transforming noise \\( z_1 \\sim \\mathcal{N}(0, I) \\) to data \\( z_0 \\). The model regresses the velocity \\( v_\\theta(z_t, t) \\) that pushes points along predefined paths (e.g., linear: \\( z_t = (1-t) z_0 + t z_1 \\)). Sampling solves an ODE deterministically: \\( dz/dt = v_\\theta(z, t) \\), yielding straight paths and fewer evaluations (NFEs). For math: The loss is \\( \\mathbb{E} \\| v_\\theta(z_t, t) - u(t, z_t | z_0) \\|^2 \\), where \\( u \\) is the target velocity\u2014simple and stable.</p> </li> <li> <p>Why SD 3.5 Uses FM: Stability AI adopted FM to address diffusion's instability (e.g., sensitive noise schedules). It matches v-MSE loss to FM objectives, enabling exponential weighting that decays with \\( t \\), reducing mid-process noise emphasis. Result: Better prompt following (e.g., spelling, composition) and 2x faster training than SD 2.0.</p> </li> <li> <p>Performance Edge of FM: On ImageNet-64x64, FM achieves lower NLL (better likelihood) and FID (better samples) than diffusion baselines. Flux edges SD 3.5 in benchmarks like GenEval for prompt adherence, but SD's ecosystem makes it more accessible. Community tests (e.g., Reddit) show FM preferred for quality, diffusion for familiarity.</p> </li> </ul>"}, {"location": "definitions/stable_difussion_vs_flow-matching/#current-trends-as-of-nov-2025", "title": "Current Trends (as of Nov 2025)", "text": "<ul> <li>Adoption: FM is the \"next frontier,\" powering Meta's audio tools and xAI experiments. Hybrids like Diff2Flow transfer diffusion knowledge to FM for efficient fine-tuning.</li> <li>Community Buzz: Recent X discussions highlight FM's role in editing (e.g., Flux vs. SD for inpainting). Videos like \"Flow-Matching vs Diffusion Models Explained\" (Oct 2025) emphasize FM's 2-5x speed gains.</li> <li>Future: Expect more FM-distilled SD variants for 1-step generation. For hands-on, check Flux on Hugging Face or SD 3.5 via Stability AI.</li> </ul> <ol> <li> <p>Flux: A General Framework for Diffusion Models, 2024.\u00a0\u21a9</p> </li> </ol>"}, {"location": "versions/terms-and-conditions/", "title": "Terms and Conditions", "text": "<p>Agreement to</p> <p>The following terms and conditions apply to the course.</p> <p>By participating in the course, you agree to abide by these terms.</p>"}, {"location": "versions/terms-and-conditions/#general", "title": "General", "text": "<ul> <li>Previously, the participants were required to read the material in advance to enhance your understanding and engagement during the lectures;</li> <li>All deliverables must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - Template;</li> <li>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</li> <li>The course is designed to be completed in a single semester, and all activities are structured to fit within this timeframe;</li> </ul>"}, {"location": "versions/terms-and-conditions/#individual-activities", "title": "Individual activities", "text": "<ul> <li>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</li> <li>All deliverables for individual activities should be submitted through the course platform.</li> </ul>"}, {"location": "versions/terms-and-conditions/#team-projects", "title": "Team projects", "text": "<p>Team projects are an essential part of this course, allowing you to collaborate with your peers and apply the concepts learned in a practical setting.</p> <p>Format of Deliverables</p> <p>All deliverables for team projects must be submitted in the format of GitHub Pages. This includes the project report, code, and any other relevant materials. The use of GitHub Pages allows for easy sharing and collaboration among team members.</p>"}, {"location": "versions/2025.2/", "title": "2025.2", "text": ""}, {"location": "versions/2025.2/#20252-winter", "title": "2025.2 - winter", "text": ""}, {"location": "versions/2025.2/#instructor", "title": "Instructor", "text": "Humberto Sandmann humbertors@insper.edu.br"}, {"location": "versions/2025.2/#students", "title": "Students", "text": "<p>Subscription</p> <p>If your name is not on the list yet, please fill out the subscription form.</p>"}, {"location": "versions/2025.2/#meetings", "title": "Meetings", "text": "Lecture Mon. 12h00  14h00 Lecture Tue. 12h00  14h00 Office hours Thu. 14h00  15h30"}, {"location": "versions/2025.2/#grade", "title": "Grade", "text": "\\[ \\text{Final} = \\left\\{\\begin{array}{lll}     \\text{Individual} \\geq 5 \\bigwedge \\text{Team} \\geq 5 &amp;     \\implies &amp;     \\displaystyle \\frac{ \\text{Individual} + \\text{Team} } {2}     \\\\     \\\\     \\text{Otherwise} &amp;     \\implies &amp;     \\min\\left(\\text{Individual}, \\text{Team}\\right)     \\end{array}\\right. \\] 2025-11-06T12:10:38.076901 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/"}, {"location": "versions/2025.2/#planning", "title": "Planning", "text": ""}, {"location": "versions/2025.2/exercises/data/", "title": "1. Data", "text": "<p>Deadline and Submission</p> <p> 05.sep (friday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Data Preparation and Analysis for Neural Networks</p> <p>This activity is designed to test your skills in generating synthetic datasets, handling real-world data challenges, and preparing data to be fed into neural networks.</p>"}, {"location": "versions/2025.2/exercises/data/#exercise-1", "title": "Exercise 1", "text": ""}, {"location": "versions/2025.2/exercises/data/#exploring-class-separability-in-2d", "title": "Exploring Class Separability in 2D", "text": "<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"}, {"location": "versions/2025.2/exercises/data/#instructions", "title": "Instructions", "text": "<ol> <li>Generate the Data: Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each). Use a Gaussian distribution to generate the points for each class based on the following parameters:<ul> <li>Class 0: Mean = \\([2, 3]\\), Standard Deviation = \\([0.8, 2.5]\\)</li> <li>Class 1: Mean = \\([5, 6]\\), Standard Deviation = \\([1.2, 1.9]\\)</li> <li>Class 2: Mean = \\([8, 1]\\), Standard Deviation = \\([0.9, 0.9]\\)</li> <li>Class 3: Mean = \\([15, 4]\\), Standard Deviation = \\([0.5, 2.0]\\)</li> </ul> </li> <li>Plot the Data: Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</li> <li>Analyze and Draw Boundaries:<ol> <li>Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</li> <li>Based on your visual inspection, could a simple, linear boundary separate all classes?</li> <li>On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</li> </ol> </li> </ol>"}, {"location": "versions/2025.2/exercises/data/#exercise-2", "title": "Exercise 2", "text": ""}, {"location": "versions/2025.2/exercises/data/#non-linearity-in-higher-dimensions", "title": "Non-Linearity in Higher Dimensions", "text": "<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"}, {"location": "versions/2025.2/exercises/data/#instructions_1", "title": "Instructions", "text": "<ol> <li> <p>Generate the Data: Create a dataset with 500 samples for Class A and 500 samples for Class B. Use a multivariate normal distribution with the following parameters:</p> <ul> <li> <p>Class A:</p> <p>Mean vector:</p> \\[\\mu_A = [0, 0, 0, 0, 0]\\] <p>Covariance matrix:</p> \\[ \\Sigma_A = \\begin{pmatrix} 1.0 &amp; 0.8 &amp; 0.1 &amp; 0.0 &amp; 0.0 \\\\ 0.8 &amp; 1.0 &amp; 0.3 &amp; 0.0 &amp; 0.0 \\\\ 0.1 &amp; 0.3 &amp; 1.0 &amp; 0.5 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.5 &amp; 1.0 &amp; 0.2 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.2 &amp; 1.0 \\end{pmatrix} \\] </li> <li> <p>Class B:</p> <p>Mean vector:</p> \\[\\mu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\\] <p>Covariance matrix:</p> \\[ \\Sigma_B = \\begin{pmatrix} 1.5 &amp; -0.7 &amp; 0.2 &amp; 0.0 &amp; 0.0 \\\\ -0.7 &amp; 1.5 &amp; 0.4 &amp; 0.0 &amp; 0.0 \\\\ 0.2 &amp; 0.4 &amp; 1.5 &amp; 0.6 &amp; 0.0 \\\\ 0.0 &amp; 0.0 &amp; 0.6 &amp; 1.5 &amp; 0.3 \\\\ 0.0 &amp; 0.0 &amp; 0.0 &amp; 0.3 &amp; 1.5 \\end{pmatrix} \\] </li> </ul> </li> <li> <p>Visualize the Data: Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <ul> <li>Use a technique like Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.</li> <li>Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</li> </ul> </li> <li>Analyze the Plots:<ol> <li>Based on your 2D projection, describe the relationship between the two classes.</li> <li>Discuss the linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.</li> </ol> </li> </ol>"}, {"location": "versions/2025.2/exercises/data/#exercise-3", "title": "Exercise 3", "text": ""}, {"location": "versions/2025.2/exercises/data/#preparing-real-world-data-for-a-neural-network", "title": "Preparing Real-World Data for a Neural Network", "text": "<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (<code>tanh</code>) activation function in its hidden layers.</p>"}, {"location": "versions/2025.2/exercises/data/#instructions_2", "title": "Instructions", "text": "<ol> <li>Get the Data: Download the Spaceship Titanic dataset from Kaggle.</li> <li>Describe the Data:<ul> <li>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).</li> <li>List the features and identify which are numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>).</li> <li>Investigate the dataset for missing values. Which columns have them, and how many?</li> </ul> </li> <li>Preprocess the Data: Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.<ul> <li>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</li> <li>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</li> <li>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.</li> </ul> </li> <li>Visualize the Results:<ul> <li>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) before and after scaling to show the effect of your transformation.</li> </ul> </li> </ol>"}, {"location": "versions/2025.2/exercises/data/#evaluation-criteria", "title": "Evaluation Criteria", "text": "<p>The deliverable for this activity consists of a report that includes:</p> <ol> <li>A brief description of your approach to each exercise.</li> <li>The code used to generate the datasets, preprocess the data, and create the visualizations. With comments explaining each step.</li> <li>The plots and visualizations requested in each exercise.</li> <li>Your analysis and answers to the questions posed in each exercise.</li> </ol> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> <p>Exercise 1 (3 points):</p> Criteria Description 1 pt Data is generated correctly and visualized in a clear scatter plot with proper labels and colors. 2 pts The analysis of class separability is accurate, and the proposed decision boundaries are logical and well-explained in the context of what a network would learn. <p>Exercise 2 (3 points):</p> Criteria Description 1 pt Data is generated correctly using the specified multivariate parameters. 1 pt Dimensionality reduction is applied correctly, and the resulting 2D projection is clearly plotted. 1 pt The analysis correctly identifies the non-linear relationship and explains why a neural network would be a suitable model. <p>Exercise 3 (4 points):</p> Criteria Description 1 pt The data is correctly loaded, and its characteristics are accurately described. 2 pts All preprocessing steps (handling missing data, encoding, and appropriate feature scaling for <code>tanh</code>) are implemented correctly and with clear justification for a neural network context. 1 pt Visualizations effectively demonstrate the impact of the data preprocessing."}, {"location": "versions/2025.2/exercises/mlp/", "title": "3. MLP", "text": "<p>Deadline and Submission</p> <p> 21.sep (sunday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Understanding Multi-Layer Perceptrons (MLPs)</p> <p>This activity is designed to test your skills in Multi-Layer Perceptrons (MLPs).</p>"}, {"location": "versions/2025.2/exercises/mlp/#exercise-1-manual-calculation-of-mlp-steps", "title": "Exercise 1: Manual Calculation of MLP Steps", "text": "<p>Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): \\( L = \\frac{1}{N} (y - \\hat{y})^2 \\), where \\( \\hat{y} \\) is the network's output.</p> <p>For this exercise, use the following specific values:</p> <ul> <li> <p>Input and output vectors:</p> <p>\\( \\mathbf{x} = [0.5, -0.2] \\)</p> <p>\\( y = 1.0 \\)</p> </li> <li> <p>Hidden layer weights:</p> <p>\\( \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\)</p> </li> <li> <p>Hidden layer biases:</p> <p>\\( \\mathbf{b}^{(1)} = [0.1, -0.2] \\)</p> </li> <li> <p>Output layer weights:</p> <p>\\( \\mathbf{W}^{(2)} = [0.5, -0.3] \\)</p> </li> <li> <p>Output layer bias:</p> <p>\\( b^{(2)} = 0.2 \\)</p> </li> <li> <p>Learning rate: \\( \\eta = 0.3 \\)</p> </li> <li> <p>Activation function: \\( \\tanh \\)</p> </li> </ul> <p>Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:</p> <ol> <li> <p>Forward Pass:</p> <ul> <li>Compute the hidden layer pre-activations: \\( \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\).</li> <li>Apply tanh to get hidden activations: \\( \\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)}) \\).</li> <li>Compute the output pre-activation: \\( u^{(2)} = \\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + b^{(2)} \\).</li> <li>Compute the final output: \\( \\hat{y} = \\tanh(u^{(2)}) \\).</li> </ul> </li> <li> <p>Loss Calculation:</p> <ul> <li> <p>Compute the MSE loss:</p> <p>\\( L = \\frac{1}{N} (y - \\hat{y})^2 \\).</p> </li> </ul> </li> <li> <p>Backward Pass (Backpropagation): Compute the gradients of the loss with respect to all weights and biases. Start with \\( \\displaystyle \\frac{\\partial L}{\\partial \\hat{y}} \\), then compute:</p> <ul> <li>\\( \\displaystyle \\frac{\\partial L}{\\partial u^{(2)}} \\) (using the tanh derivative: \\( \\displaystyle \\frac{d}{du} \\tanh(u) = 1 - \\tanh^2(u) \\)).</li> <li>Gradients for output layer: \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} \\), \\( \\displaystyle \\frac{\\partial L}{\\partial b^{(2)}} \\).</li> <li>Propagate to hidden layer: \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\), \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\).</li> <li>Gradients for hidden layer: \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} \\), \\( \\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} \\).</li> </ul> <p>Show all intermediate steps and calculations.</p> </li> <li> <p>Parameter Update: Using the learning rate \\( \\eta = 0.1 \\), update all weights and biases via gradient descent:</p> <ul> <li>\\( \\displaystyle \\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} \\)</li> <li>\\( \\displaystyle b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} \\)</li> <li>\\( \\displaystyle \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} \\)</li> <li>\\( \\displaystyle \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} \\)</li> </ul> <p>Provide the numerical values for all updated parameters.</p> </li> </ol> <p>Submission Requirements: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).</p>"}, {"location": "versions/2025.2/exercises/mlp/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp", "title": "Exercise 2: Binary Classification with Synthetic Data and Scratch MLP", "text": "<p>Using the <code>make_classification</code> function from scikit-learn (documentation), generate a synthetic dataset with the following specifications:</p> <ul> <li>Number of samples: 1000</li> <li>Number of classes: 2</li> <li>Number of clusters per class: Use the <code>n_clusters_per_class</code> parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).</li> <li>Other parameters: Set <code>n_features=2</code> for easy visualization, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> for reproducibility, and adjust <code>class_sep</code> or <code>flip_y</code> as needed for a challenging but separable dataset.</li> </ul> <p>Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:</p> <ul> <li>Number of hidden layers (at least 1)</li> <li>Number of neurons per layer</li> <li>Activation functions (e.g., sigmoid, ReLU, tanh)</li> <li>Loss function (e.g., binary cross-entropy)</li> <li>Optimizer (e.g., gradient descent, with a chosen learning rate)</li> </ul> <p>Steps to follow:</p> <ol> <li>Generate and split the data into training (80%) and testing (20%) sets.</li> <li>Implement the forward pass, loss computation, backward pass, and parameter updates in code.</li> <li>Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.</li> <li>Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.</li> <li>Submit your code and results, including any visualizations.</li> </ol>"}, {"location": "versions/2025.2/exercises/mlp/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp", "title": "Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP", "text": "<p>Similar to Exercise 2, but with increased complexity.</p> <p>Use <code>make_classification</code> to generate a synthetic dataset with:</p> <ul> <li>Number of samples: 1500</li> <li>Number of classes: 3</li> <li>Number of features: 4</li> <li>Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).</li> <li>Other parameters: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.</p> <p>Steps:</p> <ol> <li>Generate and split the data (80/20 train/test).</li> <li>Train the model, tracking loss.</li> <li>Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).</li> <li>Submit code and results.</li> </ol>"}, {"location": "versions/2025.2/exercises/mlp/#exercise-4-multi-class-classification-with-deeper-mlp", "title": "Exercise 4: Multi-Class Classification with Deeper MLP", "text": "<p>Repeat Exercise 3 exactly, but now ensure your MLP has at least 2 hidden layers. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.</p>"}, {"location": "versions/2025.2/exercises/mlp/#evaluation-criteria", "title": "Evaluation Criteria", "text": "<p>The deliverable for this activity consists of a report that includes:</p> <p>Important Notes:</p> <p>Usage of Toolboxes</p> <p>You may use toolboxes (e.g., NumPy) ONLY for matrix operations and calculations during this activity. All other computations, including activation functions, loss calculations, gradients, and the forward pass, MUST BE IMPLEMENTED within your MLP (Multi-Layer Perceptron) code. The use of third-party libraries for the MLP implementation IS STRICTLY PROHIBITED.</p> <p>Failure to comply with these instructions will result in your submission being rejected.</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> <ul> <li> <p>Exercise 1 (2 points):</p> <ul> <li>Forward pass fully explicit (0.5 points)</li> <li>Loss and backward pass with all gradients derived (1 point)</li> <li>Parameter updates shown correctly (0.5 point)</li> <li>Deductions for missing steps or incorrect math.</li> </ul> </li> <li> <p>Exercise 2 (3 points):</p> <ul> <li>Correct data generation and splitting (0.5 points)</li> <li>Functional MLP implementation from scratch (2 point)</li> <li>Training, evaluation, and results reported (0.5 points)</li> <li>Deductions for using forbidden libraries in the model core or poor performance due to errors.</li> </ul> </li> <li> <p>Exercise 3 (2 points + 1 extra):</p> <ul> <li>Correct data generation and splitting (0.5 points)</li> <li>Functional MLP for multi-class (1.5 points)</li> <li>Training, evaluation, and results (1 point)</li> <li>Extra point: Exact reuse of Exercise 2's MLP code structure (1 point, optional)</li> <li>Deductions similar to Exercise 2; extra point only if reuse is verbatim in core logic.</li> </ul> </li> <li> <p>Exercise 4 (2 points):</p> <ul> <li>Successful adaptation of Exercise 3 with at least 2 hidden layers (1 point)</li> <li>Training and evaluation results showing functionality (1 point)</li> <li>Deductions if architecture doesn't meet the depth requirement or if results are not provided.</li> </ul> </li> </ul> <p>Overall: Submissions must be clear, well-documented (code comments, explanations), and reproducible.</p>"}, {"location": "versions/2025.2/exercises/perceptron/", "title": "2. Perceptron", "text": "<p>Deadline and Submission</p> <p> 14.sep (sunday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: Understanding Perceptrons and Their Limitations</p> <p>This activity is designed to test your skills in Perceptrons and their limitations.</p>"}, {"location": "versions/2025.2/exercises/perceptron/#exercise-1", "title": "Exercise 1", "text": ""}, {"location": "versions/2025.2/exercises/perceptron/#data-generation-task", "title": "Data Generation Task:", "text": "<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:  </p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([1.5, 1.5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\) (i.e., variance of \\(0.5\\) along each dimension, no covariance).  </p> </li> <li> <p>Class 1:</p> <p>Mean = \\([5, 5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\).  </p> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p>"}, {"location": "versions/2025.2/exercises/perceptron/#perceptron-implementation-task", "title": "Perceptron Implementation Task:", "text": "<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.  </p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).  </li> <li>Use the perceptron learning rule: For each misclassified sample \\((x, y)\\), update \\(w = w + \u03b7 * y * x\\) and \\(b = b + \u03b7 * y\\), where \\(\u03b7\\) is the learning rate (start with \\(\u03b7=0.01\\)).  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.  </li> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by \\(w\u00b7x + b = 0\\)) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p>"}, {"location": "versions/2025.2/exercises/perceptron/#exercise-2", "title": "Exercise 2", "text": ""}, {"location": "versions/2025.2/exercises/perceptron/#data-generation-task_1", "title": "Data Generation Task:", "text": "<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([3, 3]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\) (i.e., higher variance of 1.5 along each dimension).</p> </li> <li> <p>Class 1:</p> <p>Mean = \\([4, 4]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\).  </p> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p>"}, {"location": "versions/2025.2/exercises/perceptron/#perceptron-implementation-task_1", "title": "Perceptron Implementation Task:", "text": "<p>Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.  </p> <ul> <li>Follow the same initialization, update rule, and training process.  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.  </li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p>"}, {"location": "versions/2025.2/exercises/perceptron/#evaluation-criteria", "title": "Evaluation Criteria", "text": "<p>Usage of Toolboxes</p> <p>You may use toolboxes (e.g., NumPy) ONLY for matrix operations and calculations during this activity. All other computations, including activation functions, loss calculations, gradients, and the forward pass, MUST BE IMPLEMENTED within your Perceptron code. The use of third-party libraries for the Perceptron implementation IS STRICTLY PROHIBITED.</p> <p>Failure to comply with these instructions will result in your submission being rejected.</p> <p>The deliverable for this activity consists of a report that includes:</p> <ul> <li>A brief description of your implementation approach and any challenges faced;</li> <li>The final weights and bias after training for both exercises;</li> <li>The accuracy achieved on the training and validation sets for both exercises;</li> <li>Visualizations of the decision boundary and data distribution.</li> <li>A plot showing training accuracy over epochs for both exercises;</li> <li>Discussion on the differences in training and performance between the two exercises, particularly focusing on the impact of data separability.</li> </ul> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> Criteria Description 4 pts Correctness of the perceptron implementation 2 pts Exercise 1: Data generation, training, evaluation, and analysis. 2 pts Exercise 2: Data generation, training, evaluation, and analysis. 1 pt Visualizations: Quality and clarity of plots (data distribution, decision boundary, accuracy over epochs). 1 pt Report Quality: Clarity, organization, and completeness of the report."}, {"location": "versions/2025.2/exercises/vae/", "title": "4. VAE", "text": "<p>Deadline and Submission</p> <p> 26.oct (sunday)</p> <p> Commits until 23:59</p> <p> Individual</p> <p> Submission the GitHub Pages' Link (yes, only the link for pages) via insper.blackboard.com.</p> <p>Activity: VAE Implementation</p> <p>In this exercise, you will implement and evaluate a Variational Autoencoder (VAE) on the MNIST or Fashion MNIST dataset. The goal is to understand the architecture, training process, and performance of VAEs.</p>"}, {"location": "versions/2025.2/exercises/vae/#instructions", "title": "Instructions", "text": "<ol> <li> <p>Data Preparation:</p> <ul> <li>Load the MNIST/Fashion MNIST dataset;</li> <li>Normalize the images to the range [0, 1];</li> <li>Split the dataset into training and validation sets.</li> </ul> </li> <li> <p>Model Implementation:</p> <ul> <li>Define the VAE architecture, including the encoder and decoder networks;</li> <li>Implement the reparameterization trick.</li> </ul> </li> <li> <p>Training:</p> <ul> <li>Train the VAE on the MNIST/Fashion MNIST dataset;</li> <li>Monitor the loss and generate reconstructions during training.</li> </ul> </li> <li> <p>Evaluation:</p> <ul> <li>Evaluate the VAE's performance on the validation set;</li> <li>Generate new samples from the learned latent space.</li> </ul> </li> <li> <p>Visualization:</p> <ul> <li>Visualize original and reconstructed images;</li> <li>Visualize the latent space (in case of a latent space until 3-D, otherwise use a reduced visualization, e.g., using t-SNE, UMAP or PCA).</li> </ul> </li> <li> <p>Report:</p> <ul> <li>Summarize your findings, including challenges faced and insights gained;</li> <li>Include visualizations of reconstructions and latent space.</li> </ul> </li> <li> <p>Extra Credit (Optional):</p> <ul> <li>Experiment the same dataset with a Autoencoder (AE) and compare the results with the VAE;</li> <li>Experiment with different latent space dimensions and report the effects on reconstruction quality and sample generation.</li> </ul> </li> </ol> <p>Important Guidelines</p> <p>This is an individual activity. You must complete the work on your own. Collaboration is not allowed, but you can discuss general concepts with your peers or instructors;</p> <p>You could use the scratch MLP built in the exercise before, but you can use any framework you prefer (e.g., PyTorch, TensorFlow, Keras), also AI tools can be used to help you in the implementation. BUT remember that the main goal is to understand the VAE architecture and training process, then you must be able to explain all parts of the code and analysis submitted.</p> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grade Criteria:</p> Criteria Description 3 pts Correctness of the VAE implementation 1 pts Training and Evaluation: Proper training procedure, loss monitoring, and evaluation on the validation set. 2 pts Sampling: Quality of generated samples. 2 pts Latent Space: Quality of the learned latent space representation. 1 pts Visualizations: Quality and clarity of plots (data distribution, decision boundary, accuracy over epochs). 1 pt Report Quality: Clarity, organization, and completeness of the report."}, {"location": "versions/2025.2/projects/classification/", "title": "1. Classification", "text": "<p>Deadline and Submission</p> <p> 05.oct (sunday)</p> <p> Commits until 23:59</p> <p> Team (2-3 members) form</p> <p> Submission the GitHub Pages' Link via insper.blackboard.com.</p> <p>In this project, you will tackle a real-world classification task using a Multi-Layer Perceptron (MLP) neural network. The goal is to deepen your understanding of neural networks by handling data preparation, model implementation, training strategies, and evaluation without relying on high-level deep learning libraries. You will select a public dataset suitable for classification, process it, build and train your MLP, and analyze the results.</p> <p>Competition Bonus</p> <p>This project encourages creativity in dataset selection and rewards ambition\u2014bonus points will be awarded if you submit your solution to a relevant online competition (e.g., on platforms like Kaggle, DrivenData, or Zindi). Submissions must be documented in your report, including a link to your entry and any leaderboard position (if applicable). Bonus points:</p> Points Description +0.5 Valid submission to a recognized competition (proof required, e.g., link, screenshot). +0.5 Valid submission ranking in the top 50% of the leaderboard (proof required). <p>If selecting from a competition platform, note the competition rules and ensure your work complies.</p> <p>Important Constraints</p> <ul> <li>DO NOT USE the Titanic, Iris, Wine or others classical datasets. These are overused and will result in a zero score for the dataset selection portion.</li> <li>The task must be classification (e.g., binary, multi-class, or multi-label).</li> <li>You may implement the MLP yourself or use high-level libraries like TensorFlow, PyTorch, Keras, or scikit-learn's neural network modules, these are ALLOWED. But, you HAVE TO understand and explain all parts of the code and analysis submitted. You may use NumPy (or similar for matrix operations), Matplotlib/Seaborn for plotting, and Pandas/SciPy for data cleaning/normalization.</li> <li>The dataset must have at least 1,000 samples and multiple features (at least 5) to ensure the MLP is meaningful.</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#project-steps", "title": "Project Steps", "text": "<p>Follow these steps in your work. Your report must address each one explicitly.</p>"}, {"location": "versions/2025.2/projects/classification/#1-dataset-selection", "title": "1. Dataset Selection", "text": "<ul> <li> <p>Choose a public dataset for a classification problem. Sources include:</p> <ul> <li>Kaggle (e.g., datasets for digit recognition, spam detection, or medical diagnosis).</li> <li>UCI Machine Learning Repository (e.g., Banknote Authentication, Adult Income, or Covertype).</li> <li>Other open sources like OpenML, Google Dataset Search, or government data portals (e.g., data.gov).</li> <li>Also, consider datasets from LOTS, here you have direct access to business problems.</li> </ul> </li> <li> <p>Ensure the dataset has at least 1,000 samples and multiple features (at least 5) to make the MLP meaningful.</p> </li> <li>If selecting from a competition platform, note the competition rules and ensure your work complies.</li> <li>In your report: Provide the dataset name, source URL, size (rows/columns), and why you chose it (e.g., relevance to real-world problems, complexity).</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#2-dataset-explanation", "title": "2. Dataset Explanation", "text": "<ul> <li>Describe the dataset in detail: What does it represent? What are the features (inputs) and their types (numerical, categorical)? What is the target variable (classes/labels)?</li> <li>Discuss any domain knowledge: E.g., if it's a medical dataset, explain key terms.</li> <li>Identify potential issues: Imbalanced classes, missing values, outliers, or noise.</li> <li>In your report: Include summary statistics (e.g., mean, std dev, class distribution) and visualizations (e.g., histograms, correlation matrices).</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#3-data-cleaning-and-normalization", "title": "3. Data Cleaning and Normalization", "text": "<ul> <li>Clean the data: Handle missing values (impute or remove), remove duplicates, detect and treat outliers.</li> <li>Preprocess: Encode categorical variables (e.g., one-hot encoding), normalize/scale numerical features (e.g., min-max scaling or z-score standardization).</li> <li>You may use libraries like Pandas for loading/cleaning and SciPy/NumPy for normalization.</li> <li>In your report: Explain each step, justify choices (e.g., \"I used median imputation for missing values to avoid skew from outliers\"), and show before/after examples (e.g., via tables or plots).</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#4-mlp-implementation", "title": "4. MLP Implementation", "text": "<ul> <li>Code an MLP from scratch using only NumPy (or equivalent) for operations like matrix multiplication, activation functions, and gradients.</li> <li>Architecture: At minimum, include an input layer, one hidden layer, and output layer. Experiment with more layers/nodes for better performance.</li> <li>Activation functions: Use sigmoid, ReLU, or tanh.</li> <li>Loss function: Cross-entropy for classification.</li> <li>Optimizer: Stochastic Gradient Descent (SGD) or a variant like mini-batch GD.</li> <li>Pre-built neural network libraries allowed, but you must understand and explain all parts of the code and analysis submitted.</li> <li>In your report: Provide code or key code snippets (the full code). Explain hyperparameters (e.g., learning rate, number of epochs, hidden units).</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#5-model-training", "title": "5. Model Training", "text": "<ul> <li>Train your MLP on the prepared data.</li> <li>Implement the training loop: Forward propagation, loss calculation, backpropagation, and parameter updates.</li> <li>Handle initialization (e.g., random weights) and regularization if needed (e.g., L2 penalty, but optional).</li> <li>In your report: Describe the training process, including any challenges (e.g., vanishing gradients) and how you addressed them.</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#6-training-and-testing-strategy", "title": "6. Training and Testing Strategy", "text": "<ul> <li>Split the data: Use train/validation/test sets (e.g., 70/15/15 split) or k-fold cross-validation.</li> <li>Training mode: Choose batch, mini-batch, or online (stochastic) training; explain why (e.g., \"Mini-batch for balance between speed and stability\").</li> <li>Early stopping or other techniques to prevent overfitting.</li> <li>In your report: Detail the split ratios, random seeds for reproducibility, and rationale. Discuss validation's role in hyperparameter tuning.</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#7-error-curves-and-visualization", "title": "7. Error Curves and Visualization", "text": "<ul> <li>Plot training and validation loss/accuracy curves over epochs.</li> <li>Use Matplotlib or similar for plots.</li> <li>Analyze: Discuss convergence, overfitting/underfitting, and adjustments made.</li> <li>In your report: Include at least two plots (e.g., loss vs. epochs, accuracy vs. epochs). Interpret trends (e.g., \"Loss plateaus after 50 epochs, indicating convergence\").</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#8-evaluation-metrics", "title": "8. Evaluation Metrics", "text": "<ul> <li>Apply classification metrics on the test set: Accuracy, precision, recall, F1-score, confusion matrix (for multi-class).</li> <li>If imbalanced, include ROC-AUC or precision-recall curves.</li> <li>Compare to baselines (e.g., majority class predictor).</li> <li>In your report: Present results in tables (e.g., metric values) and visualizations (e.g., confusion matrix heatmap). Discuss strengths/weaknesses (e.g., \"High recall on class A but low on B due to imbalance\").</li> </ul>"}, {"location": "versions/2025.2/projects/classification/#evaluation-criteria", "title": "Evaluation Criteria", "text": "<p>The deliverable for this activity consists of a comprehensive report that includes:</p> <ul> <li>Sections: One for each step above (1-8).</li> <li>Conclusion: Overall findings, limitations (e.g., MLP vs. more advanced models), future improvements.</li> <li>References: Cite dataset sources, any papers on MLPs, etc.</li> </ul> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grading Rubric (out of 10 points):</p> Criteria Description 2 pts Dataset Selection and Explanation: 1 pointData Cleaning/Normalization: 1 point 6 pts MLP Implementation: 2 points (correctness and originality);Training and Strategy: 1.5 points;Error Curves: 1 point;Metrics and Analysis: 1.5 points 2 pts Report Quality (clarity, structure, visuals): 1 point;Bonus: Up to +1 for competition submission (as described). <p>This project will test your end-to-end machine learning skills. If you have questions, ask during office hours.</p>"}, {"location": "versions/2025.2/projects/generative/", "title": "3. Generative", "text": "<p>Deadline and Submission</p> <p> 16.nov (sunday)</p> <p> Commits until 23:59</p> <p> Team (2-3 members) form</p> <p> Submission the GitHub Pages' Link via insper.blackboard.com.</p> <p>This is a open themed project where you will explore generative models, the only mandatory point is to use Stable Diffusion. In this project, you can choose work a framework to work with generative models, such as Text to {Image, Video, Audio}, vice-versa, or other combinations.</p> <p>A suggestion of platform to explore is Comfy UI, which is an open-source platform for building and deploying generative AI models. It provides a user-friendly interface and tools to create, train, and deploy models for various generative tasks. Also, it supports a wide range of generative models, including text-to-image, image-to-image, and more.</p> <p></p> <p>Build on pipeline using generative models. You can start from templates available in Comfy UI or create your own from scratch. You can also explore other platforms or frameworks if you prefer.</p> <p>For example, you can create a text-to-image pipeline that generates images based on textual descriptions. So, you have to explain the architecture of the models used, the connections between them, and the results obtained. You can also explore other combinations, such as image-to-text, video-to-audio, etc.</p>"}, {"location": "versions/2025.2/projects/generative/#criteria", "title": "Criteria", "text": "Criterion Description I - Delivery of none or incomplete project. D - Delivery of a basic project with errors;  - OR, explanation of the model architecture is missing. C - Delivery of one implementation (Image2Video, Video2Image, etc.) with correct model architecture explanation.  - At least, 5 examples of input-output pairs (different parameters). B - Delivery of two implementations (Image2Video, Video2Image, etc.) with correct model architecture explanation and its respective input-output pairs. A - Delivery of B.  - Additional use of LoRA, ControlNet, or other advanced techniques. <p>A half-grade will be added or subtracted based on the quality of the report, examples provided and creativity of the project.</p>"}, {"location": "versions/2025.2/projects/generative/#reports-and-examples", "title": "Reports and Examples", "text": "<p>In your submission, include a report explaining the architecture of the models used, the connections between them, and the results obtained. For example, you can include diagrams or flowcharts to illustrate the pipeline, and, explain the blocks involved with its contribution to the overall system.</p> <p>In case of not explaining the architecture and the models used and ALL this blocks, the maximum grade will be C.</p> <p>No Free Lunch</p> <p>Take care with cost of the models. Avoid using paid services or APIs that require payment. The project should be developed using free and open-source tools and platforms.</p>"}, {"location": "versions/2025.2/projects/regression/", "title": "2. Regression", "text": "<p>Deadline and Submission</p> <p> 19.oct (sunday)</p> <p> Commits until 23:59</p> <p> Team (2-3 members) form</p> <p> Submission the GitHub Pages' Link via insper.blackboard.com.</p> <p>In this project, you will tackle a real-world regression task using a Multi-Layer Perceptron (MLP) neural network. The goal is to deepen your understanding of neural networks by handling data preparation, model implementation, training strategies, and evaluation without relying on high-level deep learning libraries. You will select a public dataset suitable for regression, process it, build and train your MLP, and analyze the results.</p> <p>Competition Bonus</p> <p>This project encourages creativity in dataset selection and rewards ambition\u2014bonus points will be awarded if you submit your solution to a relevant online competition (e.g., on platforms like Kaggle, DrivenData, or Zindi). Submissions must be documented in your report, including a link to your entry and any leaderboard position (if applicable). Bonus points:</p> Points Description +0.5 Valid submission to a recognized competition (proof required, e.g., link, screenshot). +0.5 Valid submission ranking in the top 50% of the leaderboard (proof required). <p>If selecting from a competition platform, note the competition rules and ensure your work complies.</p> <p>Important Constraints</p> <ul> <li>DO NOT USE the Boston Housing, California Housing, or others classical datasets. These are overused and will result in a zero score for the dataset selection portion.</li> <li>The task must be regression (predicting continuous values).</li> <li>You may implement the MLP yourself or use high-level libraries like TensorFlow, PyTorch, Keras, or scikit-learn's neural network modules, these are ALLOWED. But, you HAVE TO understand and explain all parts of the code and analysis submitted. You may use NumPy (or similar for matrix operations), Matplotlib/Seaborn for plotting, and Pandas/SciPy for data cleaning/normalization.</li> <li>The dataset must have at least 1,000 samples and multiple features (at least 5) to ensure the MLP is meaningful.</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#project-steps", "title": "Project Steps", "text": "<p>Follow these steps in your work. Your report must address each one explicitly.</p>"}, {"location": "versions/2025.2/projects/regression/#1-dataset-selection", "title": "1. Dataset Selection", "text": "<ul> <li> <p>Choose a public dataset for a regression problem. Sources include:</p> <ul> <li>Kaggle (e.g., datasets for house price prediction, stock prices, or energy consumption).</li> <li>UCI Machine Learning Repository (e.g., Concrete Compressive Strength, Air Quality, or Wine Quality).</li> <li>Other open sources like OpenML, Google Dataset Search, or government data portals (e.g., data.gov).</li> <li>Also, consider datasets from LOTS, here you have direct access to business problems.</li> </ul> </li> <li> <p>Ensure the dataset has at least 1,000 samples and multiple features (at least 5) to make the MLP meaningful.</p> </li> <li>If selecting from a competition platform, note the competition rules and ensure your work complies.</li> <li>In your report: Provide the dataset name, source URL, size (rows/columns), and why you chose it (e.g., relevance to real-world problems, complexity).</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#2-dataset-explanation", "title": "2. Dataset Explanation", "text": "<ul> <li>Describe the dataset in detail: What does it represent? What are the features (inputs) and their types (numerical, categorical)? What is the target variable (continuous value)?</li> <li>Discuss any domain knowledge: E.g., if it's a financial dataset, explain key terms.</li> <li>Identify potential issues: Missing values, outliers, or noise.</li> <li>In your report: Include summary statistics (e.g., mean, std dev) and visualizations (e.g., histograms, correlation matrices).</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#3-data-cleaning-and-normalization", "title": "3. Data Cleaning and Normalization", "text": "<ul> <li>Clean the data: Handle missing values (impute or remove), remove duplicates, detect and treat outliers.</li> <li>Preprocess: Encode categorical variables (e.g., one-hot encoding), normalize/scale numerical features (e.g., min-max scaling or z-score standardization).</li> <li>You may use libraries like Pandas for loading/cleaning and SciPy/NumPy for normalization.</li> <li>In your report: Explain each step, justify choices (e.g., \"I used median imputation for missing values to avoid skew from outliers\"), and show before/after examples (e.g., via tables or plots).</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#4-mlp-implementation", "title": "4. MLP Implementation", "text": "<ul> <li>Code an MLP from scratch using only NumPy (or equivalent) for operations like matrix multiplication, activation functions, and gradients.</li> <li>Architecture: At minimum, include an input layer, one hidden layer, and output layer. Experiment with more layers/nodes for better performance.</li> <li>Activation functions: Use sigmoid, ReLU, or tanh.</li> <li>Loss function: Cross-entropy for classification.</li> <li>Optimizer: Stochastic Gradient Descent (SGD) or a variant like mini-batch GD.</li> <li>Pre-built neural network libraries allowed, but you must understand and explain all parts of the code and analysis submitted.</li> <li>In your report: Provide code or key code snippets (the full code). Explain hyperparameters (e.g., learning rate, number of epochs, hidden units).</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#5-model-training", "title": "5. Model Training", "text": "<ul> <li>Train your MLP on the prepared data.</li> <li>Implement the training loop: Forward propagation, loss calculation, backpropagation, and parameter updates.</li> <li>Handle initialization (e.g., random weights) and regularization if needed (e.g., L2 penalty, but optional).</li> <li>In your report: Describe the training process, including any challenges (e.g., vanishing gradients) and how you addressed them.</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#6-training-and-testing-strategy", "title": "6. Training and Testing Strategy", "text": "<ul> <li>Split the data: Use train/validation/test sets (e.g., 70/15/15 split) or k-fold cross-validation.</li> <li>Training mode: Choose batch, mini-batch, or online (stochastic) training; explain why (e.g., \"Mini-batch for balance between speed and stability\").</li> <li>Early stopping or other techniques to prevent overfitting.</li> <li>In your report: Detail the split ratios, random seeds for reproducibility, and rationale. Discuss validation's role in hyperparameter tuning.</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#7-error-curves-and-visualization", "title": "7. Error Curves and Visualization", "text": "<ul> <li>Plot training and validation loss/accuracy curves over epochs.</li> <li>Use Matplotlib or similar for plots.</li> <li>Analyze: Discuss convergence, overfitting/underfitting, and adjustments made.</li> <li>In your report: Include at least two plots (e.g., loss vs. epochs, accuracy vs. epochs). Interpret trends (e.g., \"Loss plateaus after 50 epochs, indicating convergence\").</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#8-evaluation-metrics", "title": "8. Evaluation Metrics", "text": "<ul> <li>Apply regression metrics suitable for regression tasks, such as Mean Absolute Error (MAE), MAPE, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (R\u00b2).</li> <li>Compare to baselines (e.g., mean predictor).</li> <li>In your report: Present results in tables (e.g., metric values) and visualizations (e.g., residual plots). Discuss strengths/weaknesses (e.g., \"High RMSE indicates model struggles with outliers\").</li> </ul>"}, {"location": "versions/2025.2/projects/regression/#evaluation-criteria", "title": "Evaluation Criteria", "text": "<p>The deliverable for this activity consists of a comprehensive report that includes:</p> <ul> <li>Sections: One for each step above (1-8).</li> <li>Conclusion: Overall findings, limitations (e.g., MLP vs. more advanced models), future improvements.</li> <li>References: Cite dataset sources, any papers on MLPs, etc.</li> </ul> <p>Important Notes:</p> <ul> <li> <p>The deliverable must be submitted in the format specified: GitHub Pages. No other formats will be accepted. - there exists a template for the course that you can use to create your GitHub Pages - template;</p> </li> <li> <p>There is a strict policy against plagiarism. Any form of plagiarism will result in a zero grade for the activity and may lead to further disciplinary actions as per the university's academic integrity policies;</p> </li> <li> <p>The deadline for each activity is not extended, and it is expected that you complete them within the timeframe provided in the course schedule - NO EXCEPTIONS will be made for late submissions.</p> </li> <li> <p>AI Collaboration is allowed, but each student MUST UNDERSTAND and be able to explain all parts of the code and analysis submitted. Any use of AI tools must be properly cited in your report. ORAL EXAMS may require you to explain your work in detail.</p> </li> <li> <p>All deliverables for individual activities should be submitted through the course platform insper.blackboard.com.</p> </li> </ul> <p>Grading Rubric (out of 10 points):</p> Criteria Description 2 pts Dataset Selection and Explanation: 1 pointData Cleaning/Normalization: 1 point 6 pts MLP Implementation: 2 points (correctness and originality);Training and Strategy: 1.5 points;Error Curves: 1 point;Metrics and Analysis: 1.5 points 2 pts Report Quality (clarity, structure, visuals): 1 point;Bonus: Up to +1 for competition submission (as described). <p>This project will test your end-to-end machine learning skills. If you have questions, ask during office hours.</p>"}]}
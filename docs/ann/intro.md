
Artificial Neural Networks (ANNs), or simply **neural networks**, are computational models inspired by the structure and function of biological neural networks. They consist of interconnected nodes (neurons) that process information in a manner similar to the way neurons in the human brain operate. ANNs are capable of learning from data, making them powerful tools for various tasks such as image recognition, natural language processing, and decision-making.

Neural networks are the backbone of many modern AI applications, enabling machines to learn from experience and improve their performance over time. They are particularly effective in handling complex patterns and large datasets, making them suitable for a wide range of applications, from computer vision to speech recognition.

## Milestones

[timeline left alternate(./docs/ann/timeline.json)]

[^1]: **Hodgkin–Huxley model.**
Alan Hodgkin and Andrew Huxley develop a mathematical model of the action potential in neurons, describing how neurons transmit signals through electrical impulses. This model is foundational for understanding neural dynamics and influences the development of artificial neural networks.
*Hodgkin, A. L., Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve.*
[:octicons-book-24:](https://doi.org/10.1113/jphysiol.1952.sp004764){target='_blank'}
[:material-wikipedia:](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model){target='_blank'}
[:octicons-download-24:](https://www.its.caltech.edu/~jkenny/nb250c/papers/hodgkin_52_5.pdf){target='_blank'} [:medal:](https://www.nobelprize.org/prizes/medicine/1963/summary/){target='_blank'}.

[^2]: **Visual Cortex and Monocular Deprivation.**
David H. Hubel and Torsten N. Wiesel conduct pioneering research on the visual cortex of cats, demonstrating how visual experience shapes neural development. Their work on monocular deprivation shows that depriving one eye of visual input during a critical period leads to permanent changes in the visual cortex, highlighting the importance of experience in neural plasticity.
*Hubel, D. H., & Wiesel, T. N. (1963). Effects of monocular deprivation in kittens.*
[:octicons-book-24:](https://doi.org/10.1007/bf00348878){target='_blank'}
[:octicons-download-24:](https://cw.fel.cvut.cz/b241/_media/courses/a6m33ksy/hubel-wiesel-1964-kittens.pdf){target='_blank'}
[:medal:](https://www.nobelprize.org/prizes/medicine/1981/summary/){target='_blank'}.

[^3]: **Neocognitron.** Kunihiko Fukushima develops the Neocognitron, an early convolutional neural network (CNN) model that mimics the hierarchical structure of the visual cortex. This model is a precursor to modern CNNs and demonstrates the potential of hierarchical feature extraction in image recognition tasks.
*Fukushima, K. (1980). Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position.*
[:octicons-book-24:](https://doi.org/10.1007/BF00344251){target='_blank'}
[:material-wikipedia:](https://en.wikipedia.org/wiki/Neocognitron){target='_blank'}
[:octicons-download-24:](https://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf){target='_blank'}.

[^4]: **Hopfield Networks.**
John Hopfield introduces Hopfield networks, a type of recurrent neural network that can serve as associative memory systems. These networks are capable of storing and recalling patterns, laying the groundwork for later developments in neural network architectures.
*Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities.*
[:octicons-book-24:](https://doi.org/10.1073/pnas.79.8.2554){target='_blank'}
[:material-wikipedia:](https://en.wikipedia.org/wiki/Hopfield_network){target='_blank'}
[:octicons-download-24:](https://www.dna.caltech.edu/courses/cs191/paperscs191/Hopfield82.pdf){target='_blank'} [:medal:](https://www.nobelprize.org/prizes/physics/2024/summary/){target='_blank'}.

[^5]: **Self-Organizing Maps (SOM).**
Teuvo Kohonen develops Self-Organizing Maps, a type of unsupervised learning algorithm that maps high-dimensional data onto a lower-dimensional grid. SOMs are used for clustering and visualization of complex data, providing insights into the structure of the data.
*Kohonen, T. (1982). Self-organized formation of topologically correct feature maps.*
[:octicons-book-24:](https://doi.org/10.1007/BF00337288){target='_blank'}
[:material-wikipedia:](https://en.wikipedia.org/wiki/Self-organizing_map){target='_blank'}
[:octicons-download-24:](https://tcosmo.github.io/assets/soms/doc/kohonen1982.pdf){target='_blank'}. 

[^6]: **Long Short-Term Memory (LSTM) Networks.**
Sepp Hochreiter and Jürgen Schmidhuber introduce LSTM networks, a type of recurrent neural network designed to learn long-term dependencies in sequential data. This architecture addresses the vanishing gradient problem in RNNs, enabling effective modeling of long-term dependencies in sequential data.
*Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory.*
[:octicons-book-24:](https://doi.org/10.1162/neco.1997.9.8.1735){target='_blank'}
[:material-wikipedia:](https://en.wikipedia.org/wiki/Long_short-term_memory){target='_blank'}
[:octicons-download-24:](https://www.bioinf.jku.at/publications/older/2604.pdf){target='_blank'}".

[^7]: **Residual Networks (ResNets).**
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun introduce Residual Networks (ResNets), a deep learning architecture that uses skip connections to allow gradients to flow more easily through deep networks. This architecture enables the training of very deep neural networks, significantly improving performance on image recognition tasks.
*He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep residual learning for image recognition.*
[:octicons-book-24:](https://doi.org/10.1109/CVPR.2016.90){target='_blank'}
[:material-wikipedia:](https://en.wikipedia.org/wiki/Residual_network){target='_blank'}
[:octicons-download-24:](https://arxiv.org/pdf/1512.03385){target='_blank'}

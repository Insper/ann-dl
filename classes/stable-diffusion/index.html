<!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Artificial Neural Networks and Deep Learning course at Insper"><meta name=author content="Sandmann, H."><link href=https://insper.github.io/ann-dl/classes/stable-diffusion/ rel=canonical><link href=../clip/ rel=prev><link href=../flow-matching/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.23"><title>16. Stable Diffusion - Artificial Neural Networks and Deep Learning</title><link rel=stylesheet href=../../assets/stylesheets/main.84d31ad4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M64%20480c-35.3%200-64-28.7-64-64V96c0-35.3%2028.7-64%2064-64h320c35.3%200%2064%2028.7%2064%2064v213.5c0%2017-6.7%2033.3-18.7%2045.3L322.7%20461.3c-12%2012-28.3%2018.7-45.3%2018.7zm325.5-176H296c-13.3%200-24%2010.7-24%2024v93.5z%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M384%20512H96c-53%200-96-43-96-96V96C0%2043%2043%200%2096%200h304c26.5%200%2048%2021.5%2048%2048v288c0%2020.9-13.4%2038.7-32%2045.3V448c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032zM96%20384c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h256v-64zm32-232c0%2013.3%2010.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24H152c-13.3%200-24%2010.7-24%2024m24%2072c-13.3%200-24%2010.7-24%2024s10.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24z%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m-32-352a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200m-8%2064h48c13.3%200%2024%2010.7%2024%2024v88h8c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-80c-13.3%200-24-10.7-24-24s10.7-24%2024-24h24v-64h-24c-13.3%200-24-10.7-24-24s10.7-24%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M461.2%2018.9C472.7%2024%20480%2035.4%20480%2048v416c0%2012.6-7.3%2024-18.8%2029.1s-24.8%203.2-34.3-5.1l-46.6-40.7c-43.6-38.1-98.7-60.3-156.4-63V480c0%2017.7-14.3%2032-32%2032h-32c-17.7%200-32-14.3-32-32v-96C57.3%20384%200%20326.7%200%20256s57.3-128%20128-128h84.5c61.8-.2%20121.4-22.7%20167.9-63.3L427%2024c9.4-8.3%2022.9-10.2%2034.3-5.1zM224%20320v.2c70.3%202.7%20137.8%2028.5%20192%2073.4V118.3c-54.2%2044.9-121.7%2070.7-192%2073.4z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M434.8%2070.1c14.3%2010.4%2017.5%2030.4%207.1%2044.7l-256%20352c-5.5%207.6-14%2012.3-23.4%2013.1s-18.5-2.7-25.1-9.3l-128-128c-12.5-12.5-12.5-32.8%200-45.3s32.8-12.5%2045.3%200l101.5%20101.5%20234-321.7c10.4-14.3%2030.4-17.5%2044.7-7.1z%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m0-336c-17.7%200-32%2014.3-32%2032%200%2013.3-10.7%2024-24%2024s-24-10.7-24-24c0-44.2%2035.8-80%2080-80s80%2035.8%2080%2080c0%2047.2-36%2067.2-56%2074.5v3.8c0%2013.3-10.7%2024-24%2024s-24-10.7-24-24v-8.1c0-20.5%2014.8-35.2%2030.1-40.2%206.4-2.1%2013.2-5.5%2018.2-10.3%204.3-4.2%207.7-10%207.7-19.6%200-17.7-14.3-32-32-32zm-32%20192a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%200c14.7%200%2028.2%208.1%2035.2%2021l216%20400c6.7%2012.4%206.4%2027.4-.8%2039.5S486.1%20480%20472%20480H40c-14.1%200-27.2-7.4-34.4-19.5s-7.5-27.1-.8-39.5l216-400c7-12.9%2020.5-21%2035.2-21m0%20352a32%2032%200%201%200%200%2064%2032%2032%200%201%200%200-64m0-192c-18.2%200-32.7%2015.5-31.4%2033.7l7.4%20104c.9%2012.5%2011.4%2022.3%2023.9%2022.3%2012.6%200%2023-9.7%2023.9-22.3l7.4-104c1.3-18.2-13.1-33.7-31.4-33.7z%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M480-16c6.9%200%2013%204.4%2015.2%2010.9l13.5%2040.4%2040.4%2013.5C555.6%2051%20560%2057.1%20560%2064s-4.4%2013-10.9%2015.2l-40.4%2013.5-13.5%2040.4c-2.2%206.5-8.3%2010.9-15.2%2010.9s-13-4.4-15.2-10.9l-13.5-40.4-40.4-13.5C404.4%2077%20400%2070.9%20400%2064s4.4-13%2010.9-15.2l40.4-13.5%2013.5-40.4C467-11.6%20473.1-16%20480-16M321.4%2097.4c12.5-12.5%2032.8-12.5%2045.3%200l80%2080c12.5%2012.5%2012.5%2032.8%200%2045.3l-10.9%2010.9c7.9%2022%2012.2%2045.7%2012.2%2070.5%200%20114.9-93.1%20208-208%20208S32%20418.9%2032%20304%20125.1%2096%20240%2096c24.7%200%2048.5%204.3%2070.5%2012.3zM144%20304c0-53%2043-96%2096-96%2013.3%200%2024-10.7%2024-24s-10.7-24-24-24c-79.5%200-144%2064.5-144%20144%200%2013.3%2010.7%2024%2024%2024s24-10.7%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M416%20427.4c58.5-44%2096-111.6%2096-187.4C512%20107.5%20397.4%200%20256%200S0%20107.5%200%20240c0%2075.8%2037.5%20143.4%2096%20187.4V464c0%2026.5%2021.5%2048%2048%2048h32v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h64v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h32c26.5%200%2048-21.5%2048-48zM96%20256a64%2064%200%201%201%20128%200%2064%2064%200%201%201-128%200m256-64a64%2064%200%201%201%200%20128%2064%2064%200%201%201%200-128%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M352%200c0-17.7-14.3-32-32-32s-32%2014.3-32%2032v64h-96c-53%200-96%2043-96%2096v224c0%2053%2043%2096%2096%2096h256c53%200%2096-43%2096-96V160c0-53-43-96-96-96h-96zM160%20368c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24M224%20176a48%2048%200%201%201%200%2096%2048%2048%200%201%201%200-96m144%2048a48%2048%200%201%201%2096%200%2048%2048%200%201%201-96%200m-304%200c0-17.7-14.3-32-32-32S0%20206.3%200%20224v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32zm544-32c-17.7%200-32%2014.3-32%2032v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32v-96c0-17.7-14.3-32-32-32%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M288%200H128c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032v151.5L7.5%20426.3C2.6%20435%200%20444.7%200%20454.7%200%20486.4%2025.6%20512%2057.3%20512h333.4c31.6%200%2057.3-25.6%2057.3-57.3%200-10-2.6-19.8-7.5-28.4L320%20215.5V64c17.7%200%2032-14.3%2032-32S337.7%200%20320%200zm-96%20215.5V64h64v151.5c0%2011.1%202.9%2022.1%208.4%2031.8L306%20320H142l41.6-72.7c5.5-9.7%208.4-20.6%208.4-31.8%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M0%20216C0%20149.7%2053.7%2096%20120%2096h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064H64c-35.3%200-64-28.7-64-64zm256%200c0-66.3%2053.7-120%20120-120h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064h-64c-35.3%200-64-28.7-64-64z%22/%3E%3C/svg%3E');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_markdown_exec_pyodide.css><link rel=stylesheet href=../../assets/stylesheets/neoteroi-mkdocs.min.css><link rel=stylesheet href=../../assets/stylesheets/neoteroi-timeline.css><link rel=stylesheet href=../../assets/stylesheets/extra.css><link rel=stylesheet href=../../assets/stylesheets/badge.css><link rel=stylesheet href=../../termynal.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#diffusion-models class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Artificial Neural Networks and Deep Learning" class="md-header__button md-logo" aria-label="Artificial Neural Networks and Deep Learning" data-md-component=logo> <img src=../../assets/images/ann-dl.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Artificial Neural Networks and Deep Learning </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 16. Stable Diffusion </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"></path></svg> </label> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"></path></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_3 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"></path></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_3> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/insper/ann-dl title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> insper/ann-dl </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Artificial Neural Networks and Deep Learning" class="md-nav__button md-logo" aria-label="Artificial Neural Networks and Deep Learning" data-md-component=logo> <img src=../../assets/images/ann-dl.png alt=logo> </a> Artificial Neural Networks and Deep Learning </label> <div class=md-nav__source> <a href=https://github.com/insper/ann-dl title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> insper/ann-dl </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Ementa </span> </a> </li> <li class=md-nav__item> <a href=../../versions/2025.2/ class=md-nav__link> <span class=md-ellipsis> 2025.2 </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Classes </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Classes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../concepts/ class=md-nav__link> <span class=md-ellipsis> 1. Concepts </span> </a> </li> <li class=md-nav__item> <a href=../data/ class=md-nav__link> <span class=md-ellipsis> 2. Data </span> </a> </li> <li class=md-nav__item> <a href=../preprocessing/ class=md-nav__link> <span class=md-ellipsis> 3. Preprocessing </span> </a> </li> <li class=md-nav__item> <a href=../ann/ class=md-nav__link> <span class=md-ellipsis> 4. Neural Networks </span> </a> </li> <li class=md-nav__item> <a href=../perceptron/ class=md-nav__link> <span class=md-ellipsis> 5. Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../mlp/ class=md-nav__link> <span class=md-ellipsis> 6. Multi-Layer Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../optimization/ class=md-nav__link> <span class=md-ellipsis> 7. Optimization </span> </a> </li> <li class=md-nav__item> <a href=../regularization/ class=md-nav__link> <span class=md-ellipsis> 8. Regularization </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <label class=md-nav__link for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class=md-ellipsis> 9. Metrics and Evaluation </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> 9. Metrics and Evaluation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../metrics/classification/ class=md-nav__link> <span class=md-ellipsis> 9.1. Classification </span> </a> </li> <li class=md-nav__item> <a href=../metrics/regression/ class=md-nav__link> <span class=md-ellipsis> 9.2. Regression </span> </a> </li> <li class=md-nav__item> <a href=../metrics/generative/ class=md-nav__link> <span class=md-ellipsis> 9.3. Generative </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../deep-learning/ class=md-nav__link> <span class=md-ellipsis> 10. Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../convolutional-neural-networks/ class=md-nav__link> <span class=md-ellipsis> 11. Convolutional </span> </a> </li> <li class=md-nav__item> <a href=../generative-models/ class=md-nav__link> <span class=md-ellipsis> 12. Generative Models </span> </a> </li> <li class=md-nav__item> <a href=../generative-adversarial-networks/ class=md-nav__link> <span class=md-ellipsis> 13. GAN </span> </a> </li> <li class=md-nav__item> <a href=../variational-autoencoders/ class=md-nav__link> <span class=md-ellipsis> 14. VAE </span> </a> </li> <li class=md-nav__item> <a href=../clip/ class=md-nav__link> <span class=md-ellipsis> 15. CLIP </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 16. Stable Diffusion </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 16. Stable Diffusion </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#diffusion-models class=md-nav__link> <span class=md-ellipsis> Diffusion Models </span> </a> <nav class=md-nav aria-label="Diffusion Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-diffusion-process class=md-nav__link> <span class=md-ellipsis> Forward Diffusion Process </span> </a> </li> <li class=md-nav__item> <a href=#reverse-diffusion-process class=md-nav__link> <span class=md-ellipsis> Reverse Diffusion Process </span> </a> </li> <li class=md-nav__item> <a href=#u-net-training class=md-nav__link> <span class=md-ellipsis> U-Net Training </span> </a> <nav class=md-nav aria-label="U-Net Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dataset class=md-nav__link> <span class=md-ellipsis> Dataset </span> </a> </li> <li class=md-nav__item> <a href=#training class=md-nav__link> <span class=md-ellipsis> Training </span> </a> </li> <li class=md-nav__item> <a href=#reverse-diffusion-denoising-sampling class=md-nav__link> <span class=md-ellipsis> Reverse Diffusion / Denoising / Sampling </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#milestones class=md-nav__link> <span class=md-ellipsis> Milestones </span> </a> </li> <li class=md-nav__item> <a href=#stable-diffusion class=md-nav__link> <span class=md-ellipsis> Stable Diffusion </span> </a> <nav class=md-nav aria-label="Stable Diffusion"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#inference class=md-nav__link> <span class=md-ellipsis> Inference </span> </a> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#additional class=md-nav__link> <span class=md-ellipsis> Additional </span> </a> <nav class=md-nav aria-label=Additional> <ul class=md-nav__list> <li class=md-nav__item> <a href=#ddpm-vs-ddim class=md-nav__link> <span class=md-ellipsis> DDPM vs DDIM </span> </a> </li> <li class=md-nav__item> <a href=#sde class=md-nav__link> <span class=md-ellipsis> SDE </span> </a> </li> <li class=md-nav__item> <a href=#u-net-architecture class=md-nav__link> <span class=md-ellipsis> U-Net Architecture </span> </a> </li> <li class=md-nav__item> <a href=#videos class=md-nav__link> <span class=md-ellipsis> Videos </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../flow-matching/ class=md-nav__link> <span class=md-ellipsis> 17. Flow-Matching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Definitions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Definitions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../definitions/latent_space_vs_embedding/ class=md-nav__link> <span class=md-ellipsis> Latent Space vs. Embedding </span> </a> </li> <li class=md-nav__item> <a href=../../definitions/stable_difussion_vs_flow-matching/ class=md-nav__link> <span class=md-ellipsis> Stable Diffusion vs. Flow-Matching </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../references/ class=md-nav__link> <span class=md-ellipsis> References </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Versions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Versions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../versions/terms-and-conditions/ class=md-nav__link> <span class=md-ellipsis> Terms and Conditions </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2> <label class=md-nav__link for=__nav_6_2 id=__nav_6_2_label tabindex=0> <span class=md-ellipsis> 2025.2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2> <span class="md-nav__icon md-icon"></span> 2025.2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../versions/2025.2/ class=md-nav__link> <span class=md-ellipsis> 2025.2 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2_2> <label class=md-nav__link for=__nav_6_2_2 id=__nav_6_2_2_label tabindex=0> <span class=md-ellipsis> Exercises </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_6_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2_2> <span class="md-nav__icon md-icon"></span> Exercises </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../versions/2025.2/exercises/data/ class=md-nav__link> <span class=md-ellipsis> 1. Data </span> </a> </li> <li class=md-nav__item> <a href=../../versions/2025.2/exercises/perceptron/ class=md-nav__link> <span class=md-ellipsis> 2. Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../../versions/2025.2/exercises/mlp/ class=md-nav__link> <span class=md-ellipsis> 3. MLP </span> </a> </li> <li class=md-nav__item> <a href=../../versions/2025.2/exercises/vae/ class=md-nav__link> <span class=md-ellipsis> 4. VAE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2_3> <label class=md-nav__link for=__nav_6_2_3 id=__nav_6_2_3_label tabindex=0> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_6_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2_3> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../versions/2025.2/projects/classification/ class=md-nav__link> <span class=md-ellipsis> 1. Classification </span> </a> </li> <li class=md-nav__item> <a href=../../versions/2025.2/projects/regression/ class=md-nav__link> <span class=md-ellipsis> 2. Regression </span> </a> </li> <li class=md-nav__item> <a href=../../versions/2025.2/projects/generative/ class=md-nav__link> <span class=md-ellipsis> 3. Generative </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#diffusion-models class=md-nav__link> <span class=md-ellipsis> Diffusion Models </span> </a> <nav class=md-nav aria-label="Diffusion Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#forward-diffusion-process class=md-nav__link> <span class=md-ellipsis> Forward Diffusion Process </span> </a> </li> <li class=md-nav__item> <a href=#reverse-diffusion-process class=md-nav__link> <span class=md-ellipsis> Reverse Diffusion Process </span> </a> </li> <li class=md-nav__item> <a href=#u-net-training class=md-nav__link> <span class=md-ellipsis> U-Net Training </span> </a> <nav class=md-nav aria-label="U-Net Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#dataset class=md-nav__link> <span class=md-ellipsis> Dataset </span> </a> </li> <li class=md-nav__item> <a href=#training class=md-nav__link> <span class=md-ellipsis> Training </span> </a> </li> <li class=md-nav__item> <a href=#reverse-diffusion-denoising-sampling class=md-nav__link> <span class=md-ellipsis> Reverse Diffusion / Denoising / Sampling </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#milestones class=md-nav__link> <span class=md-ellipsis> Milestones </span> </a> </li> <li class=md-nav__item> <a href=#stable-diffusion class=md-nav__link> <span class=md-ellipsis> Stable Diffusion </span> </a> <nav class=md-nav aria-label="Stable Diffusion"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#inference class=md-nav__link> <span class=md-ellipsis> Inference </span> </a> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#additional class=md-nav__link> <span class=md-ellipsis> Additional </span> </a> <nav class=md-nav aria-label=Additional> <ul class=md-nav__list> <li class=md-nav__item> <a href=#ddpm-vs-ddim class=md-nav__link> <span class=md-ellipsis> DDPM vs DDIM </span> </a> </li> <li class=md-nav__item> <a href=#sde class=md-nav__link> <span class=md-ellipsis> SDE </span> </a> </li> <li class=md-nav__item> <a href=#u-net-architecture class=md-nav__link> <span class=md-ellipsis> U-Net Architecture </span> </a> </li> <li class=md-nav__item> <a href=#videos class=md-nav__link> <span class=md-ellipsis> Videos </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>16. Stable Diffusion</h1> <p>Stable Diffusion is a state-of-the-art text-to-image generative model developed by Stability AI in collaboration with researchers at <a href=https://www.eleuther.ai/ target=_blank>EleutherAI</a> and <a href=https://laion.ai/ target=_blank>LAION</a>. It leverages Latent Diffusion Models (LDMs) to generate high-quality images from textual descriptions efficiently. Stable Diffusion has gained significant attention for its ability to produce detailed and diverse images, making it a popular choice for various applications in art, design, and content creation.</p> <p>It is based on the principles of diffusion models, which involve a two-step process: first, adding noise to an image to create a noisy version, and then training a neural network to reverse this process by denoising the image step-by-step. Stable Diffusion operates in a latent space, which allows it to generate images more efficiently than traditional pixel-space diffusion models.</p> <!-- [timeline left alternate(./docs/classes/stable-diffusion/timeline.json)] --> <hr> <h2 id=diffusion-models>Diffusion Models</h2> <p>Diffusion models are trained to predict a way to slightly denoise a sample in each step, and after a few iterations, a result is obtained. Diffusion models have already been applied to a variety of generation tasks, such as image, speech, 3D shape, and graph synthesis.</p> <p>Diffusion models consist of two steps:</p> <div class="grid cards"> <ul> <li> <p><strong>Forward Diffusion</strong></p> <hr> <p>Maps data to noise by gradually perturbing the input data. This is formally achieved by a simple stochastic process that starts from a data sample and iteratively generates noisier samples using a simple Gaussian diffusion kernel.</p> <mark class="critic block"> <p>This process is used only during training and not on inference.</p> </mark> </li> <li> <p><strong>Reverse Diffusion</strong></p> <hr> <p>Undoes the forward diffusion and performs iterative denoising. This process represents data synthesis and is trained to generate data by converting random noise into realistic data.</p> </li> </ul> </div> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=ddpm.png><img alt src=ddpm.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>Overview of DDPM. Source: <sup id=fnref:12><a class=footnote-ref href=#fn:12>12</a></sup>.</p> </figcaption> </figure> <h3 id=forward-diffusion-process>Forward Diffusion Process</h3> <p>In the forward diffusion process, a data sample <span class=arithmatex>\( x_0 \)</span> (e.g., an image) is gradually corrupted by adding Gaussian noise over a series of time steps <span class=arithmatex>\( t = 1, 2, \ldots, T \)</span>. At each time step, a small amount of noise is added to the sample, resulting in a sequence of increasingly noisy samples <span class=arithmatex>\( x_1, x_2, \ldots, x_T \)</span>. The process can be mathematically described as:</p> <div class=arithmatex>\[ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I) \]</div> <p>where <span class=arithmatex>\( \beta_t \)</span> is a variance schedule that controls the amount of noise added at each step.</p> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=ddpm-diffusion.png><img alt src=ddpm-diffusion.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>Forward Diffusion Process. Source: <sup id=fnref2:12><a class=footnote-ref href=#fn:12>12</a></sup>.</p> </figcaption> </figure> <h3 id=reverse-diffusion-process>Reverse Diffusion Process</h3> <p>The reverse diffusion process aims to reconstruct the original data sample from the noisy version by iteratively denoising it. A neural network, typically a U-Net architecture, is trained to predict the noise added at each time step. The reverse process can be expressed as:</p> <div class=arithmatex>\[ p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) \]</div> <p>where <span class=arithmatex>\( \mu_\theta \)</span> and <span class=arithmatex>\( \Sigma_\theta \)</span> are the mean and covariance predicted by the neural network parameterized by <span class=arithmatex>\( \theta \)</span>.</p> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=ddpm-reverse.png><img alt src=ddpm-reverse.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>Reverse Diffusion Process. Source: <sup id=fnref3:12><a class=footnote-ref href=#fn:12>12</a></sup>.</p> </figcaption> </figure> <p>Unlike the forward process, we cannot use <span class=arithmatex>\( q(x_{t-1} | x_t) \)</span> directly because it requires knowledge of the original data distribution - <mark>it is intractable (uncomputable)</mark>. Instead, we train the neural network to approximate this distribution by minimizing a loss function that measures the difference between the predicted noise and the actual noise added during the forward process.</p> <p>Usually, a <a href=https://arxiv.org/abs/1505.04597 target=_blank>U-Net architecture</a> is used as the neural network for the reverse diffusion process due to its ability to capture multi-scale features effectively.</p> <h3 id=u-net-training>U-Net Training</h3> <p>The U-Net is trained using a dataset of images <sup id=fnref:8><a class=footnote-ref href=#fn:8>8</a></sup>.</p> <h4 id=dataset>Dataset</h4> <p>In each epoch:</p> <ol> <li>A random time step <span class=arithmatex>\( t \)</span> will be selected for each training sample (image).</li> <li>Apply the Gaussian noise (corresponding to <span class=arithmatex>\( t \)</span>) to each image.</li> <li>Convert the time steps to embeddings (vectors).</li> </ol> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=u-net_dataset.png><img alt src=u-net_dataset.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>U-Net Training Dataset Preparation. Source: <sup id=fnref2:8><a class=footnote-ref href=#fn:8>8</a></sup>.</p> </figcaption> </figure> <h4 id=training>Training</h4> <p>The official training algorithm is as above, and the following diagram is an illustration of how a training step works:</p> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=u-net_training_step.png><img alt src=u-net_training_step.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>U-Net Training Step. Source: <sup id=fnref3:8><a class=footnote-ref href=#fn:8>8</a></sup>.</p> </figcaption> </figure> <h4 id=reverse-diffusion-denoising-sampling>Reverse Diffusion / Denoising / Sampling</h4> <p>Once the U-Net is trained, the reverse diffusion process can be used to generate new images from random noise. The generation process involves the following steps:</p> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=u-net_sampling.png><img alt src=u-net_sampling.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>U-Net Sampling. Source: <sup id=fnref4:8><a class=footnote-ref href=#fn:8>8</a></sup>.</p> </figcaption> </figure> <hr> <h2 id=milestones>Milestones</h2> <pre class=mermaid><code>graph TD
    A[2015: Diffusion Concept] --&gt; B[2020: Denoising Diffusion Probabilistic Models - DDPM]
    B --&gt; C[2021: Denoising Diffusion Implicit Models - DDIM]
    C --&gt; D[2021: Latent Diffusion - LDM]
    D --&gt; E[2022: Stable Diffusion v1&lt;br&gt;LDM + CLIP]
    E --&gt; F[2022: SD 2.0]
    F --&gt; G[2023: SDXL]
    G --&gt; H[2024: SD3 / SD3.5]
    E --&gt; I[&lt;a href="https://github.com/lllyasviel/ControlNet" target="_blank"&gt;ControlNet&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2308.06721" target="_blank"&gt;IP-Adapter&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2106.09685" target="_blank"&gt;LoRA&lt;/a&gt;]
    E --&gt; J[Text-to-3D: DreamFusion → Magic3D → ...]
    H --&gt; K[2024: FLUX.1 - Post-Stability AI]
    click A "https://arxiv.org/abs/1503.03585" "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
    click B "https://arxiv.org/abs/2006.11239" "Denoising Diffusion Probabilistic Models"
    click C "https://arxiv.org/abs/2010.02502" "Denoising Diffusion Implicit Models"
    click D "https://github.com/CompVis/latent-diffusion" "Latent Diffusion Models"
    click E "https://huggingface.co/blog/stable_diffusion" "Stable Diffusion v1 Release"
    click F "https://stability.ai/news/stable-diffusion-v2-release" "Stable Diffusion 2.0 Release"
    click G "https://arxiv.org/abs/2307.01952" "SDXL: High-Resolution Image Synthesis with Latent Diffusion Models"
    click K "https://flux1.ai/" "FLUX.1 by Stability AI"</code></pre> <hr> <h2 id=stable-diffusion>Stable Diffusion</h2> <p>Stable Diffusion is a text-to-image generative model that utilizes Latent Diffusion Models (LDMs) to create high-quality images from textual descriptions. The model operates in a latent space, which allows for efficient image synthesis while maintaining high fidelity.</p> <p>The Stable Diffusion architecture consists of three main components:</p> <ul> <li> <p><strong>The Text Encoder</strong>: A pre-trained text encoder (like CLIP) is used to convert the input text prompt into a semantic embedding that guides the denoising process.</p> </li> <li> <p><strong>The Diffusion Model</strong>: The core of the LDM is a U-Net architecture that learns to denoise the latent representations. It takes as input the noisy latent tensor and the text embedding (from the text encoder) and iteratively refines the latent representation over a series of time steps.</p> </li> <li> <p><strong>The Autoencoder</strong>: The input of the model is a random noise of the size of the desired output. It will first reduce the sample to a lower dimensional latent space. For that, the authors used the <a href=../variational-autoencoders/ >VAE Architecture</a>, which consists of two parts - encoder and decoder. The encoder is used during training to convert the sample into a lower latent representation and passes it as input to the next block. On inference, the denoised, generated samples undergo reverse diffusion and are transformed back to their original dimensional latent space.</p> </li> </ul> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=latent-stable-diffusion-architecture.png><img alt src=latent-stable-diffusion-architecture.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>Latent Diffusion Model Architecture. Source: <sup id=fnref:6><a class=footnote-ref href=#fn:6>6</a></sup>.</p> </figcaption> </figure> <h3 id=inference>Inference</h3> <div class="tabbed-set tabbed-alternate" data-tabs=1:2><input checked=checked id=__tabbed_1_1 name=__tabbed_1 type=radio><input id=__tabbed_1_2 name=__tabbed_1 type=radio><div class=tabbed-labels><label for=__tabbed_1_1>Simplified Pipeline</label><label for=__tabbed_1_2>Detailed Pipeline</label></div> <div class=tabbed-content> <div class=tabbed-block> <pre class=mermaid><code>graph TD
    A[Text Prompt] --&gt; B(CLIP Text Encoder)
    B --&gt; C[Text Embedding]

    D[Random Noise&lt;br&gt;&lt;small&gt;Latent&lt;/small&gt;] --&gt; E[Diffusion Model&lt;br&gt;&lt;small&gt;UNet + Scheduler&lt;/small&gt;]
    C --&gt; E

    E --&gt; F[Latent Image&lt;br&gt;&lt;small&gt;after denoising&lt;/small&gt;]

    F --&gt; G(VAE Decoder)
    G --&gt; H[Final Image&lt;br&gt;&lt;small&gt;in pixels&lt;/small&gt;]

    subgraph "Latent Space"
        D
        E
        F
    end

    style A fill:#a8e6cf,stroke:#333
    style B fill:#ffccbc,stroke:#333
    style C fill:#ffccbc,stroke:#333
    style D fill:#ffd3b6,stroke:#333
    style E fill:#dcedc1,stroke:#333
    style F fill:#dcedc1,stroke:#333
    style G fill:#c7ceea,stroke:#333
    style H fill:#c7ceea,stroke:#333</code></pre> </div> <div class=tabbed-block> <pre class=mermaid><code>graph TD
    A["Text Prompt&lt;br&gt;'A cat in space'"] --&gt; B["CLIP Text Encoder&lt;br&gt;&lt;small&gt;(Transformer)&lt;/small&gt;"]
    B --&gt; C["Text Embedding&lt;br&gt;&lt;small&gt;(77 tokens × 768 dim)&lt;/small&gt;"]

    D["Random Gaussian Noise&lt;br&gt;z₀ ~ N(0,1)&lt;br&gt;&lt;small&gt;(4 × 64 × 64)&lt;/small&gt;"] 

    subgraph Diffusion_Model ["Diffusion Model&lt;br&gt;(Latent Space)"]
        direction TB
        E["UNet with Cross-Attention&lt;br&gt;Predicting noise ε(θ)"]
        F["Scheduler&lt;br&gt;&lt;small&gt;DDIM, PLMS, etc.&lt;/small&gt;"]
        G["Cross-Attention Layers&lt;br&gt;Query: latent image&lt;br&gt;Key/Value: CLIP embedding"]

        D --&gt; E
        C --&gt; G
        G --&gt; E
        E --&gt; F
        F --&gt; H{Denoising Loop&lt;br&gt;T steps}
        H --&gt;|Step t| E
    end

    H --&gt; I["Final Latent Image&lt;br&gt;ẑ_T&lt;br&gt;&lt;small&gt;(4 × 64 × 64)&lt;/small&gt;"]

    I --&gt; J["VAE Decoder"]
    J --&gt; K["Final Image in Pixels&lt;br&gt;&lt;small&gt;(3 × 512 × 512)&lt;/small&gt;"]

    subgraph Training ["Training&lt;br&gt;&lt;small&gt;optional&lt;/small&gt;"]
        L["Real Image&lt;br&gt;(3 × 512 × 512)"] --&gt; M["VAE Encoder&lt;br&gt;(Downsampling)"]
        M --&gt; N["Latent Image&lt;br&gt;z = μ + σ⊙ε"]
        N --&gt; O["Add Noise&lt;br&gt;q(z_t | z_0)"]
        O --&gt; E
    end

    classDef text fill:#fadadd,stroke:#e74c3c,stroke-width:2px
    classDef latent fill:#fff2cc,stroke:#f39c12,stroke-width:2px
    classDef pixel fill:#d5f5e3,stroke:#27ae60,stroke-width:2px
    classDef model fill:#ebebeb,stroke:#7f8c8d,stroke-width:2px

    class A,C text
    class D,I latent
    class K,L pixel
    class B,E,F,G,J model</code></pre> </div> </div> </div> <!-- === "Original Pipeline"

    ![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png){:style="max-width: 100%; height: auto;"}
 --> <ol> <li> <p><strong>Text Encoding</strong>: The input text prompt is processed using a text encoder (like CLIP) to generate a text embedding that captures the semantic meaning of the prompt.</p> </li> <li> <p><strong>Latent Space Initialization</strong>: A random noise tensor is generated in the latent space, which serves as the starting point for the image generation process.</p> </li> <li> <p><strong>Diffusion Process</strong>: The diffusion model, typically a U-Net architecture, takes the noisy latent tensor and the text embedding as inputs. It iteratively denoises the latent tensor over a series of time steps, guided by the text embedding to ensure that the generated image aligns with the input prompt.</p> </li> <li> <p><strong>Image Decoding</strong>: Once the denoising process is complete, the final latent representation is passed through a Variational Autoencoder (VAE) decoder to convert it back into pixel space, resulting in the final generated image.</p> </li> </ol> <hr> <h3 id=examples>Examples</h3> <ul> <li> <p><a href=https://www.geeksforgeeks.org/deep-learning/generate-images-from-text-in-python-stable-diffusion/ target=_blank>GeeksForGeeks - Generate Images from Text in Python - Stable Diffusion</a><sup id=fnref:9><a class=footnote-ref href=#fn:9>9</a></sup>. Coded at: <a href=https://colab.research.google.com/drive/1LzkO8GySnbTLMNQj_xVJCVRAjWD3JkpX target=_blank>https://colab.research.google.com/drive/1LzkO8GySnbTLMNQj_xVJCVRAjWD3JkpX</a></p> </li> <li> <p><a href=https://www.datacamp.com/tutorial/how-to-use-stable-diffusion-3-api target=_blank>Data Camp - How to Use the Stable Diffusion 3 API</a><sup id=fnref:17><a class=footnote-ref href=#fn:17>17</a></sup></p> </li> </ul> <hr> <h2 id=additional>Additional</h2> <h3 id=ddpm-vs-ddim>DDPM vs DDIM</h3> <table> <thead> <tr> <th>Aspect</th> <th>DDPM</th> <th>DDIM</th> </tr> </thead> <tbody> <tr> <td>Background</td> <td>Probabilistic</td> <td>Deterministic</td> </tr> <tr> <td>Speed</td> <td>More steps (slower)</td> <td>Fewer steps (faster)</td> </tr> <tr> <td>Quality</td> <td>High variability</td> <td>More consistent</td> </tr> </tbody> </table> <h3 id=sde>SDE</h3> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=ddpm-sde.png><img alt src=ddpm-sde.png style="max-width: 100%; height: auto;"></a></p> <figcaption> <p>Denoising Diffusion Probabilistic Models (DDPM) and Score-based Generative Modeling through Stochastic Differential Equations (SDE). Source: <sup id=fnref:7><a class=footnote-ref href=#fn:7>7</a></sup>.</p> </figcaption> </figure> <h3 id=u-net-architecture>U-Net Architecture</h3> <p>U-Net is a convolutional neural network architecture originally designed for biomedical image segmentation<sup id=fnref:13><a class=footnote-ref href=#fn:13>13</a></sup>. It has since been widely adopted in various image generation tasks, including diffusion models like Stable Diffusion. The U-Net architecture is characterized by its U-shaped structure, which consists of an encoder (contracting path) and a decoder (expanding path) with skip connections between corresponding layers.</p> <figure> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=u-net_architecture.png><img 100_=100%; alt auto_=auto; height:=height: max-width:=max-width: src=u-net_architecture.png></a></p> <figcaption> <p>U-Net Architecture. Source: <sup id=fnref:14><a class=footnote-ref href=#fn:14>14</a></sup>.</p> </figcaption> </figure> <details class=example> <summary>Stable Diffusion U-Net Architecture</summary> <pre class=mermaid><code>graph TD
    subgraph Input
        Z["Noisy Latent z_t&lt;br&gt;(B,4,64,64)"] 
        T[Timestep t]
        C["CLIP Text Emb&lt;br&gt;(B,77,768)"]
    end

    Z --&gt; ConvIn[Initial Conv&lt;br&gt;→ 320 ch]
    T --&gt; TEmb[Sinusoidal → MLP → 320]
    C --&gt; CProj[Linear 768→320]

    ConvIn --&gt; D1[Down Block 1&lt;br&gt;320 → 320]
    D1 --&gt; P1[Downsample]
    P1 --&gt; D2[Down Block 2&lt;br&gt;320 → 640]
    D2 --&gt; P2[Downsample]
    P2 --&gt; D3[Down Block 3&lt;br&gt;640 → 1280]
    D3 --&gt; P3[Downsample]
    P3 --&gt; Bottleneck[Bottleneck&lt;br&gt;1280 ch + Self-Attn]

    %% Skip connections
    D1 --&gt; S1[Skip 1&lt;br&gt;320,32x32]
    D2 --&gt; S2[Skip 2&lt;br&gt;640,16x16]
    D3 --&gt; S3[Skip 3&lt;br&gt;1280,8x8]

    Bottleneck --&gt; U1[Up Block 1&lt;br&gt;+ Skip 3]
    S3 --&gt; U1
    U1 --&gt; Up1[Upsample]
    Up1 --&gt; U2[Up Block 2&lt;br&gt;+ Skip 2 + Cross-Attn]
    S2 --&gt; U2
    CProj --&gt; U2
    U2 --&gt; Up2[Upsample]
    Up2 --&gt; U3[Up Block 3&lt;br&gt;+ Skip 1 + Cross-Attn]
    S1 --&gt; U3
    CProj --&gt; U3

    U3 --&gt; Out[Final Conv&lt;br&gt;→ 4 ch]
    Out --&gt; Eps["ε_pred(z_t, t, c)"]

    style Z fill:#ffd3b6
    style Eps fill:#a8e6cf
    style Bottleneck fill:#ff9999
    style U1,U2,U3 fill:#dcedc1</code></pre> </details> <h3 id=videos>Videos</h3> <div class="tabbed-set tabbed-alternate" data-tabs=2:4><input checked=checked id=__tabbed_2_1 name=__tabbed_2 type=radio><input id=__tabbed_2_2 name=__tabbed_2 type=radio><input id=__tabbed_2_3 name=__tabbed_2 type=radio><input id=__tabbed_2_4 name=__tabbed_2 type=radio><div class=tabbed-labels><label for=__tabbed_2_1>Deepia: DDPM</label><label for=__tabbed_2_2>Deepia: Score-based</label><label for=__tabbed_2_3>Welch Labs: AI images/videos</label><label for=__tabbed_2_4>Latent Diffusion Models</label></div> <div class=tabbed-content> <div class=tabbed-block> <p><strong>Deepia: Diffusion Models: DDPM | Generative AI Animated</strong></p> <p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen frameborder=0 height=480 referrerpolicy=strict-origin-when-cross-origin src=https://www.youtube.com/embed/EhndHhIvWWw title="Diffusion Models: DDPM | Generative AI Animated" width=100%></iframe></p> </div> <div class=tabbed-block> <p><strong>Deepia: Score-based Diffusion Models | Generative AI Animated</strong></p> <p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen frameborder=0 height=480 referrerpolicy=strict-origin-when-cross-origin src=https://www.youtube.com/embed/lUljxdkolK8 title="Score-based Diffusion Models | Generative AI Animated" width=100%></iframe></p> </div> <div class=tabbed-block> <p><strong>But how do AI images and videos actually work? | Guest video by Welch Labs</strong></p> <p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen frameborder=0 height=480 referrerpolicy=strict-origin-when-cross-origin src=https://www.youtube.com/embed/iv-5mZ_9CPY title="But how do AI images and videos actually work? | Guest video by Welch Labs" width=100%></iframe></p> </div> <div class=tabbed-block> <p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen frameborder=0 height=480 referrerpolicy=strict-origin-when-cross-origin src=https://www.youtube.com/embed/wuwByIh5kDU title="Latent Diffusion Models" width=100%></iframe></p> </div> </div> </div> <div class=footnote> <hr> <ol> <li id=fn:1> <p><a href=https://arxiv.org/abs/1503.03585 target=_blank>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a>, 2015.&nbsp;<a class=footnote-backref href=#fnref:1 title="Jump back to footnote 1 in the text">↩</a></p> </li> <li id=fn:2> <p><a href=https://arxiv.org/abs/2006.11239 target=_blank>Denoising Diffusion Probabilistic Models</a>, 2020.&nbsp;<a class=footnote-backref href=#fnref:2 title="Jump back to footnote 2 in the text">↩</a></p> </li> <li id=fn:3> <p><a href=https://arxiv.org/abs/2010.02502 target=_blank>Denoising Diffusion Implicit Models</a>, 2020.&nbsp;<a class=footnote-backref href=#fnref:3 title="Jump back to footnote 3 in the text">↩</a></p> </li> <li id=fn:4> <p><a href=https://github.com/CompVis/latent-diffusion target=_blank>Latent Diffusion Models</a>, 2021.&nbsp;<a class=footnote-backref href=#fnref:4 title="Jump back to footnote 4 in the text">↩</a></p> </li> <li id=fn:5> <p><a href=https://huggingface.co/blog/stable_diffusion target=_blank>Hugging Face - Stable Diffusion v1 - Release</a>&nbsp;<a class=footnote-backref href=#fnref:5 title="Jump back to footnote 5 in the text">↩</a></p> </li> <li id=fn:6> <p><a href=https://dagshub.com/blog/stable-diffusion-best-open-source-version-of-dall-e-2/ target=_blank>Dagshub - Stable Diffusion: Best Open Source Version of DALL-E 2</a>&nbsp;<a class=footnote-backref href=#fnref:6 title="Jump back to footnote 6 in the text">↩</a></p> </li> <li id=fn:7> <p><a href=https://arxiv.org/abs/2011.13456 target=_blank>Score-Based Generative Modeling through Stochastic Differential Equations</a>&nbsp;<a class=footnote-backref href=#fnref:7 title="Jump back to footnote 7 in the text">↩</a></p> </li> <li id=fn:8> <p><a href=https://codoraven.com/blog/ai/diffusion-model-clearly-explained/ target=_blank>Diffusion Models Clearly Explained</a>&nbsp;<a class=footnote-backref href=#fnref:8 title="Jump back to footnote 8 in the text">↩</a><a class=footnote-backref href=#fnref2:8 title="Jump back to footnote 8 in the text">↩</a><a class=footnote-backref href=#fnref3:8 title="Jump back to footnote 8 in the text">↩</a><a class=footnote-backref href=#fnref4:8 title="Jump back to footnote 8 in the text">↩</a></p> </li> <li id=fn:9> <p><a href=https://www.geeksforgeeks.org/deep-learning/generate-images-from-text-in-python-stable-diffusion/ target=_blank>Generate Images from Text in Python - Stable Diffusion</a>&nbsp;<a class=footnote-backref href=#fnref:9 title="Jump back to footnote 9 in the text">↩</a></p> </li> <li id=fn:10> <p><a href=https://github.com/huggingface/diffusers target=_blank>Hugging Face - Diffusers</a>&nbsp;<a class=footnote-backref href=#fnref:10 title="Jump back to footnote 10 in the text">↩</a></p> </li> <li id=fn:11> <p><a href=https://huggingface.co/learn/diffusion-course/ target=_blank>Hugging Face - Diffusion Course</a>&nbsp;<a class=footnote-backref href=#fnref:11 title="Jump back to footnote 11 in the text">↩</a></p> </li> <li id=fn:12> <p><a href=https://www.datacamp.com/tutorial/how-to-run-stable-diffusion target=_blank>How to Run Stable Diffusion: A Step-by-Step Guide</a>&nbsp;<a class=footnote-backref href=#fnref:12 title="Jump back to footnote 12 in the text">↩</a><a class=footnote-backref href=#fnref2:12 title="Jump back to footnote 12 in the text">↩</a><a class=footnote-backref href=#fnref3:12 title="Jump back to footnote 12 in the text">↩</a></p> </li> <li id=fn:13> <p><a href=https://arxiv.org/abs/1505.04597 target=_blank>U-Net: Convolutional Networks for Biomedical Image Segmentation</a>&nbsp;<a class=footnote-backref href=#fnref:13 title="Jump back to footnote 13 in the text">↩</a></p> </li> <li id=fn:14> <p>GeeksForGeeks - <a href=https://www.geeksforgeeks.org/machine-learning/u-net-architecture-explained/ target=_blank>U-Net Architecture Explained</a>&nbsp;<a class=footnote-backref href=#fnref:14 title="Jump back to footnote 14 in the text">↩</a></p> </li> <li id=fn:15> <p><a href=https://poloclub.github.io/diffusion-explainer/ target=_blank>Polo Club - Diffusion Explainer</a>&nbsp;<a class=footnote-backref href=#fnref:15 title="Jump back to footnote 15 in the text">↩</a></p> </li> <li id=fn:16> <p><a href=https://huggingface.co/stabilityai/stable-diffusion-3.5-large target=_blank>Hugging Face - Stable Diffusion 3.5 Large</a>&nbsp;<a class=footnote-backref href=#fnref:16 title="Jump back to footnote 16 in the text">↩</a></p> </li> <li id=fn:17> <p><a href=https://www.datacamp.com/tutorial/how-to-use-stable-diffusion-3-api target=_blank>How to Use Stable Diffusion 3 API</a>&nbsp;<a class=footnote-backref href=#fnref:17 title="Jump back to footnote 17 in the text">↩</a></p> </li> <li id=fn:18> <p><a href=https://ankittaxak5713.medium.com/stable-diffusion-models-24a953276240 target=_blank>Stable Diffusion Models</a>, by Ankit Kumar.&nbsp;<a class=footnote-backref href=#fnref:18 title="Jump back to footnote 18 in the text">↩</a></p> </li> <li id=fn:19> <p><a href=https://github.com/facebookresearch/DiT target=_blank>Scalable Diffusion Models with Transformers (DiT)</a>, 2023.&nbsp;<a class=footnote-backref href=#fnref:19 title="Jump back to footnote 19 in the text">↩</a></p> </li> <li id=fn:20> <p><a href=https://arxiv.org/abs/2210.02747 target=_blank>Flow-Matching: A New Paradigm for Generative Modeling</a>, 2022.&nbsp;<a class=footnote-backref href=#fnref:20 title="Jump back to footnote 20 in the text">↩</a></p> </li> <li id=fn:21> <p><a href=https://github.com/black-forest-labs/flux target=_blank>Flux: A General Framework for Diffusion Models</a>, 2024.&nbsp;<a class=footnote-backref href=#fnref:21 title="Jump back to footnote 21 in the text">↩</a></p> </li> <li id=fn:22> <p><a href=https://github.com/cumulo-autumn/StreamDiffusion target=_blank>StreamDiffusion</a>, suggested by <a href=https://github.com/pedrofracassi target=_blank>Pedro Fracassi</a>.&nbsp;<a class=footnote-backref href=#fnref:22 title="Jump back to footnote 22 in the text">↩</a></p> </li> </ol> </div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 6, 2025 12:06:33 UTC">November 6, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 6, 2025 12:06:33 UTC">November 6, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Contributors> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"></path></svg> </span> <nav> <a href=mailto:hsandmann@ieee.org>Humberto Sandmann</a> </nav> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "content.code.select", "content.code.annotate", "content.tooltips", "navigation.instant", "navigation.instant.progress", "navigation.top", "navigation.path", "navigation.tracking"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../assets/_markdown_exec_pyodide.js></script> <script src=../../assets/javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../assets/javascripts/badge.js async></script> <script src=../../termynal.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>
<!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Artificial Neural Networks and Deep Learning course at Insper"><meta name=author content="Sandmann, H."><link href=https://insper.github.io/ann-dl/classes/generative-adversarial-networks/gan_example_cifar_10/ rel=canonical><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.23"><title>Gan example cifar 10 - Artificial Neural Networks and Deep Learning</title><link rel=stylesheet href=../../../assets/stylesheets/main.84d31ad4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M64%20480c-35.3%200-64-28.7-64-64V96c0-35.3%2028.7-64%2064-64h320c35.3%200%2064%2028.7%2064%2064v213.5c0%2017-6.7%2033.3-18.7%2045.3L322.7%20461.3c-12%2012-28.3%2018.7-45.3%2018.7zm325.5-176H296c-13.3%200-24%2010.7-24%2024v93.5z%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M384%20512H96c-53%200-96-43-96-96V96C0%2043%2043%200%2096%200h304c26.5%200%2048%2021.5%2048%2048v288c0%2020.9-13.4%2038.7-32%2045.3V448c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032zM96%20384c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h256v-64zm32-232c0%2013.3%2010.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24H152c-13.3%200-24%2010.7-24%2024m24%2072c-13.3%200-24%2010.7-24%2024s10.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24z%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m-32-352a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200m-8%2064h48c13.3%200%2024%2010.7%2024%2024v88h8c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-80c-13.3%200-24-10.7-24-24s10.7-24%2024-24h24v-64h-24c-13.3%200-24-10.7-24-24s10.7-24%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M461.2%2018.9C472.7%2024%20480%2035.4%20480%2048v416c0%2012.6-7.3%2024-18.8%2029.1s-24.8%203.2-34.3-5.1l-46.6-40.7c-43.6-38.1-98.7-60.3-156.4-63V480c0%2017.7-14.3%2032-32%2032h-32c-17.7%200-32-14.3-32-32v-96C57.3%20384%200%20326.7%200%20256s57.3-128%20128-128h84.5c61.8-.2%20121.4-22.7%20167.9-63.3L427%2024c9.4-8.3%2022.9-10.2%2034.3-5.1zM224%20320v.2c70.3%202.7%20137.8%2028.5%20192%2073.4V118.3c-54.2%2044.9-121.7%2070.7-192%2073.4z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M434.8%2070.1c14.3%2010.4%2017.5%2030.4%207.1%2044.7l-256%20352c-5.5%207.6-14%2012.3-23.4%2013.1s-18.5-2.7-25.1-9.3l-128-128c-12.5-12.5-12.5-32.8%200-45.3s32.8-12.5%2045.3%200l101.5%20101.5%20234-321.7c10.4-14.3%2030.4-17.5%2044.7-7.1z%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m0-336c-17.7%200-32%2014.3-32%2032%200%2013.3-10.7%2024-24%2024s-24-10.7-24-24c0-44.2%2035.8-80%2080-80s80%2035.8%2080%2080c0%2047.2-36%2067.2-56%2074.5v3.8c0%2013.3-10.7%2024-24%2024s-24-10.7-24-24v-8.1c0-20.5%2014.8-35.2%2030.1-40.2%206.4-2.1%2013.2-5.5%2018.2-10.3%204.3-4.2%207.7-10%207.7-19.6%200-17.7-14.3-32-32-32zm-32%20192a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%200c14.7%200%2028.2%208.1%2035.2%2021l216%20400c6.7%2012.4%206.4%2027.4-.8%2039.5S486.1%20480%20472%20480H40c-14.1%200-27.2-7.4-34.4-19.5s-7.5-27.1-.8-39.5l216-400c7-12.9%2020.5-21%2035.2-21m0%20352a32%2032%200%201%200%200%2064%2032%2032%200%201%200%200-64m0-192c-18.2%200-32.7%2015.5-31.4%2033.7l7.4%20104c.9%2012.5%2011.4%2022.3%2023.9%2022.3%2012.6%200%2023-9.7%2023.9-22.3l7.4-104c1.3-18.2-13.1-33.7-31.4-33.7z%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M480-16c6.9%200%2013%204.4%2015.2%2010.9l13.5%2040.4%2040.4%2013.5C555.6%2051%20560%2057.1%20560%2064s-4.4%2013-10.9%2015.2l-40.4%2013.5-13.5%2040.4c-2.2%206.5-8.3%2010.9-15.2%2010.9s-13-4.4-15.2-10.9l-13.5-40.4-40.4-13.5C404.4%2077%20400%2070.9%20400%2064s4.4-13%2010.9-15.2l40.4-13.5%2013.5-40.4C467-11.6%20473.1-16%20480-16M321.4%2097.4c12.5-12.5%2032.8-12.5%2045.3%200l80%2080c12.5%2012.5%2012.5%2032.8%200%2045.3l-10.9%2010.9c7.9%2022%2012.2%2045.7%2012.2%2070.5%200%20114.9-93.1%20208-208%20208S32%20418.9%2032%20304%20125.1%2096%20240%2096c24.7%200%2048.5%204.3%2070.5%2012.3zM144%20304c0-53%2043-96%2096-96%2013.3%200%2024-10.7%2024-24s-10.7-24-24-24c-79.5%200-144%2064.5-144%20144%200%2013.3%2010.7%2024%2024%2024s24-10.7%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M416%20427.4c58.5-44%2096-111.6%2096-187.4C512%20107.5%20397.4%200%20256%200S0%20107.5%200%20240c0%2075.8%2037.5%20143.4%2096%20187.4V464c0%2026.5%2021.5%2048%2048%2048h32v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h64v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h32c26.5%200%2048-21.5%2048-48zM96%20256a64%2064%200%201%201%20128%200%2064%2064%200%201%201-128%200m256-64a64%2064%200%201%201%200%20128%2064%2064%200%201%201%200-128%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M352%200c0-17.7-14.3-32-32-32s-32%2014.3-32%2032v64h-96c-53%200-96%2043-96%2096v224c0%2053%2043%2096%2096%2096h256c53%200%2096-43%2096-96V160c0-53-43-96-96-96h-96zM160%20368c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24M224%20176a48%2048%200%201%201%200%2096%2048%2048%200%201%201%200-96m144%2048a48%2048%200%201%201%2096%200%2048%2048%200%201%201-96%200m-304%200c0-17.7-14.3-32-32-32S0%20206.3%200%20224v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32zm544-32c-17.7%200-32%2014.3-32%2032v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32v-96c0-17.7-14.3-32-32-32%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M288%200H128c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032v151.5L7.5%20426.3C2.6%20435%200%20444.7%200%20454.7%200%20486.4%2025.6%20512%2057.3%20512h333.4c31.6%200%2057.3-25.6%2057.3-57.3%200-10-2.6-19.8-7.5-28.4L320%20215.5V64c17.7%200%2032-14.3%2032-32S337.7%200%20320%200zm-96%20215.5V64h64v151.5c0%2011.1%202.9%2022.1%208.4%2031.8L306%20320H142l41.6-72.7c5.5-9.7%208.4-20.6%208.4-31.8%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.1.0%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M0%20216C0%20149.7%2053.7%2096%20120%2096h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064H64c-35.3%200-64-28.7-64-64zm256%200c0-66.3%2053.7-120%20120-120h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064h-64c-35.3%200-64-28.7-64-64z%22/%3E%3C/svg%3E');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/_markdown_exec_pyodide.css><link rel=stylesheet href=../../../assets/stylesheets/neoteroi-mkdocs.min.css><link rel=stylesheet href=../../../assets/stylesheets/neoteroi-timeline.css><link rel=stylesheet href=../../../assets/stylesheets/extra.css><link rel=stylesheet href=../../../assets/stylesheets/badge.css><link rel=stylesheet href=../../../termynal.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#1-importing-libraries class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="Artificial Neural Networks and Deep Learning" class="md-header__button md-logo" aria-label="Artificial Neural Networks and Deep Learning" data-md-component=logo> <img src=../../../assets/images/ann-dl.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Artificial Neural Networks and Deep Learning </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Gan example cifar 10 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"></path></svg> </label> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"></path></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_3 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"></path></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_3> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/insper/ann-dl title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> insper/ann-dl </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Artificial Neural Networks and Deep Learning" class="md-nav__button md-logo" aria-label="Artificial Neural Networks and Deep Learning" data-md-component=logo> <img src=../../../assets/images/ann-dl.png alt=logo> </a> Artificial Neural Networks and Deep Learning </label> <div class=md-nav__source> <a href=https://github.com/insper/ann-dl title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> insper/ann-dl </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Ementa </span> </a> </li> <li class=md-nav__item> <a href=../../../versions/2025.2/ class=md-nav__link> <span class=md-ellipsis> 2025.2 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Classes </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Classes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../concepts/ class=md-nav__link> <span class=md-ellipsis> 1. Concepts </span> </a> </li> <li class=md-nav__item> <a href=../../data/ class=md-nav__link> <span class=md-ellipsis> 2. Data </span> </a> </li> <li class=md-nav__item> <a href=../../preprocessing/ class=md-nav__link> <span class=md-ellipsis> 3. Preprocessing </span> </a> </li> <li class=md-nav__item> <a href=../../ann/ class=md-nav__link> <span class=md-ellipsis> 4. Neural Networks </span> </a> </li> <li class=md-nav__item> <a href=../../perceptron/ class=md-nav__link> <span class=md-ellipsis> 5. Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../../mlp/ class=md-nav__link> <span class=md-ellipsis> 6. Multi-Layer Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../../optimization/ class=md-nav__link> <span class=md-ellipsis> 7. Optimization </span> </a> </li> <li class=md-nav__item> <a href=../../regularization/ class=md-nav__link> <span class=md-ellipsis> 8. Regularization </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <label class=md-nav__link for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class=md-ellipsis> 9. Metrics and Evaluation </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> 9. Metrics and Evaluation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../metrics/classification/ class=md-nav__link> <span class=md-ellipsis> 9.1. Classification </span> </a> </li> <li class=md-nav__item> <a href=../../metrics/regression/ class=md-nav__link> <span class=md-ellipsis> 9.2. Regression </span> </a> </li> <li class=md-nav__item> <a href=../../metrics/generative/ class=md-nav__link> <span class=md-ellipsis> 9.3. Generative </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../deep-learning/ class=md-nav__link> <span class=md-ellipsis> 10. Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../convolutional-neural-networks/ class=md-nav__link> <span class=md-ellipsis> 11. Convolutional </span> </a> </li> <li class=md-nav__item> <a href=../../generative-models/ class=md-nav__link> <span class=md-ellipsis> 12. Generative Models </span> </a> </li> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> 13. GAN </span> </a> </li> <li class=md-nav__item> <a href=../../variational-autoencoders/ class=md-nav__link> <span class=md-ellipsis> 14. VAE </span> </a> </li> <li class=md-nav__item> <a href=../../clip/ class=md-nav__link> <span class=md-ellipsis> 15. CLIP </span> </a> </li> <li class=md-nav__item> <a href=../../stable-diffusion/ class=md-nav__link> <span class=md-ellipsis> 16. Stable Diffusion </span> </a> </li> <li class=md-nav__item> <a href=../../flow-matching/ class=md-nav__link> <span class=md-ellipsis> 17. Flow-Matching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Definitions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Definitions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../definitions/latent_space_vs_embedding/ class=md-nav__link> <span class=md-ellipsis> Latent Space vs. Embedding </span> </a> </li> <li class=md-nav__item> <a href=../../../definitions/stable_difussion_vs_flow-matching/ class=md-nav__link> <span class=md-ellipsis> Stable Diffusion vs. Flow-Matching </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../references/ class=md-nav__link> <span class=md-ellipsis> References </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Versions </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Versions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../versions/terms-and-conditions/ class=md-nav__link> <span class=md-ellipsis> Terms and Conditions </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2> <label class=md-nav__link for=__nav_6_2 id=__nav_6_2_label tabindex=0> <span class=md-ellipsis> 2025.2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2> <span class="md-nav__icon md-icon"></span> 2025.2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../versions/2025.2/ class=md-nav__link> <span class=md-ellipsis> 2025.2 </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2_2> <label class=md-nav__link for=__nav_6_2_2 id=__nav_6_2_2_label tabindex=0> <span class=md-ellipsis> Exercises </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_6_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2_2> <span class="md-nav__icon md-icon"></span> Exercises </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../versions/2025.2/exercises/data/ class=md-nav__link> <span class=md-ellipsis> 1. Data </span> </a> </li> <li class=md-nav__item> <a href=../../../versions/2025.2/exercises/perceptron/ class=md-nav__link> <span class=md-ellipsis> 2. Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../../../versions/2025.2/exercises/mlp/ class=md-nav__link> <span class=md-ellipsis> 3. MLP </span> </a> </li> <li class=md-nav__item> <a href=../../../versions/2025.2/exercises/vae/ class=md-nav__link> <span class=md-ellipsis> 4. VAE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6_2_3> <label class=md-nav__link for=__nav_6_2_3 id=__nav_6_2_3_label tabindex=0> <span class=md-ellipsis> Projects </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_6_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_6_2_3> <span class="md-nav__icon md-icon"></span> Projects </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../versions/2025.2/projects/classification/ class=md-nav__link> <span class=md-ellipsis> 1. Classification </span> </a> </li> <li class=md-nav__item> <a href=../../../versions/2025.2/projects/regression/ class=md-nav__link> <span class=md-ellipsis> 2. Regression </span> </a> </li> <li class=md-nav__item> <a href=../../../versions/2025.2/projects/generative/ class=md-nav__link> <span class=md-ellipsis> 3. Generative </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-importing-libraries class=md-nav__link> <span class=md-ellipsis> 1. Importing Libraries </span> </a> </li> <li class=md-nav__item> <a href=#2-defining-image-transformations class=md-nav__link> <span class=md-ellipsis> 2. Defining Image Transformations </span> </a> </li> <li class=md-nav__item> <a href=#3-loaging-the-cifar-10-dataset class=md-nav__link> <span class=md-ellipsis> 3. Loaging the CIFAR-10 Dataset </span> </a> </li> <li class=md-nav__item> <a href=#4-defining-gan-hyperparameters class=md-nav__link> <span class=md-ellipsis> 4. Defining GAN Hyperparameters </span> </a> </li> <li class=md-nav__item> <a href=#5-building-the-generator class=md-nav__link> <span class=md-ellipsis> 5. Building the Generator </span> </a> </li> <li class=md-nav__item> <a href=#6-building-the-discriminator class=md-nav__link> <span class=md-ellipsis> 6. Building the Discriminator </span> </a> </li> <li class=md-nav__item> <a href=#7-initializing class=md-nav__link> <span class=md-ellipsis> 7. Initializing </span> </a> </li> <li class=md-nav__item> <a href=#8-training class=md-nav__link> <span class=md-ellipsis> 8. Training </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>Gan example cifar 10</h1> <p>Example extracted from <a href=https://www.geeksforgeeks.org/deep-learning/generative-adversarial-network-gan/ >https://www.geeksforgeeks.org/deep-learning/generative-adversarial-network-gan/</a></p> <h4 id=1-importing-libraries>1. Importing Libraries</h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a href=#__codelineno-0-1 id=__codelineno-0-1 name=__codelineno-0-1></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-0-2><a href=#__codelineno-0-2 id=__codelineno-0-2 name=__codelineno-0-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.nn</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>nn</span>
</span><span id=__span-0-3><a href=#__codelineno-0-3 id=__codelineno-0-3 name=__codelineno-0-3></a><span class=kn>import</span><span class=w> </span><span class=nn>torch.optim</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>optim</span>
</span><span id=__span-0-4><a href=#__codelineno-0-4 id=__codelineno-0-4 name=__codelineno-0-4></a><span class=kn>import</span><span class=w> </span><span class=nn>torchvision</span>
</span><span id=__span-0-5><a href=#__codelineno-0-5 id=__codelineno-0-5 name=__codelineno-0-5></a><span class=kn>from</span><span class=w> </span><span class=nn>torchvision</span><span class=w> </span><span class=kn>import</span> <span class=n>datasets</span><span class=p>,</span> <span class=n>transforms</span>
</span><span id=__span-0-6><a href=#__codelineno-0-6 id=__codelineno-0-6 name=__codelineno-0-6></a><span class=kn>import</span><span class=w> </span><span class=nn>matplotlib.pyplot</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>plt</span>
</span><span id=__span-0-7><a href=#__codelineno-0-7 id=__codelineno-0-7 name=__codelineno-0-7></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-0-8><a href=#__codelineno-0-8 id=__codelineno-0-8 name=__codelineno-0-8></a>
</span><span id=__span-0-9><a href=#__codelineno-0-9 id=__codelineno-0-9 name=__codelineno-0-9></a><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s1>'cuda'</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>'cpu'</span><span class=p>)</span>
</span></code></pre></div> <h4 id=2-defining-image-transformations>2. Defining Image Transformations</h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a href=#__codelineno-1-1 id=__codelineno-1-1 name=__codelineno-1-1></a><span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span><span id=__span-1-2><a href=#__codelineno-1-2 id=__codelineno-1-2 name=__codelineno-1-2></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
</span><span id=__span-1-3><a href=#__codelineno-1-3 id=__codelineno-1-3 name=__codelineno-1-3></a>    <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>),</span> <span class=p>(</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>))</span>
</span><span id=__span-1-4><a href=#__codelineno-1-4 id=__codelineno-1-4 name=__codelineno-1-4></a><span class=p>])</span>
</span></code></pre></div> <h4 id=3-loaging-the-cifar-10-dataset>3. Loaging the CIFAR-10 Dataset</h4> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a href=#__codelineno-2-1 id=__codelineno-2-1 name=__codelineno-2-1></a><span class=n>train_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>CIFAR10</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>'./data'</span><span class=p>,</span>\
</span><span id=__span-2-2><a href=#__codelineno-2-2 id=__codelineno-2-2 name=__codelineno-2-2></a>              <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span><span id=__span-2-3><a href=#__codelineno-2-3 id=__codelineno-2-3 name=__codelineno-2-3></a><span class=n>dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> \
</span><span id=__span-2-4><a href=#__codelineno-2-4 id=__codelineno-2-4 name=__codelineno-2-4></a>                                <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code>100%|██████████| 170M/170M [00:16&lt;00:00, 10.5MB/s]
</code></pre></div> <h4 id=4-defining-gan-hyperparameters>4. Defining GAN Hyperparameters</h4> <p>Set important training parameters:</p> <ul> <li><strong>latent_dim</strong>: Dimensionality of the noise vector.</li> <li><strong>lr</strong>: Learning rate of the optimizer.</li> <li><strong>beta1</strong>, <strong>beta2</strong>: Beta parameters for Adam optimizer (e.g 0.5, 0.999)</li> <li><strong>num_epochs</strong>: Number of times the entire dataset will be processed (e.g 10)</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a href=#__codelineno-3-1 id=__codelineno-3-1 name=__codelineno-3-1></a><span class=n>latent_dim</span> <span class=o>=</span> <span class=mi>100</span>
</span><span id=__span-3-2><a href=#__codelineno-3-2 id=__codelineno-3-2 name=__codelineno-3-2></a><span class=n>lr</span> <span class=o>=</span> <span class=mf>0.0002</span>
</span><span id=__span-3-3><a href=#__codelineno-3-3 id=__codelineno-3-3 name=__codelineno-3-3></a><span class=n>beta1</span> <span class=o>=</span> <span class=mf>0.5</span>
</span><span id=__span-3-4><a href=#__codelineno-3-4 id=__codelineno-3-4 name=__codelineno-3-4></a><span class=n>beta2</span> <span class=o>=</span> <span class=mf>0.999</span>
</span><span id=__span-3-5><a href=#__codelineno-3-5 id=__codelineno-3-5 name=__codelineno-3-5></a><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>30</span>
</span></code></pre></div> <h4 id=5-building-the-generator>5. Building the Generator</h4> <p>Create a neural network that converts random noise into images. Use transpose convolutional layers, batch normalization and ReLU activations. The final layer uses Tanh activation to scale outputs to the range [-1, 1].</p> <ul> <li><strong>nn.Linear(latent_dim, 128 * 8 * 8)</strong>: Defines a fully connected layer that projects the noise vector into a higher dimensional feature space.</li> <li><strong>nn.Upsample(scale_factor=2)</strong>: Doubles the spatial resolution of the feature maps by upsampling.</li> <li><strong>nn.Conv2d(128, 128, kernel_size=3, padding=1)</strong>: Applies a convolutional layer keeping the number of channels the same to refine features.</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a href=#__codelineno-4-1 id=__codelineno-4-1 name=__codelineno-4-1></a><span class=k>class</span><span class=w> </span><span class=nc>Generator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-4-2><a href=#__codelineno-4-2 id=__codelineno-4-2 name=__codelineno-4-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>):</span>
</span><span id=__span-4-3><a href=#__codelineno-4-3 id=__codelineno-4-3 name=__codelineno-4-3></a>        <span class=nb>super</span><span class=p>(</span><span class=n>Generator</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-4-4><a href=#__codelineno-4-4 id=__codelineno-4-4 name=__codelineno-4-4></a>
</span><span id=__span-4-5><a href=#__codelineno-4-5 id=__codelineno-4-5 name=__codelineno-4-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-4-6><a href=#__codelineno-4-6 id=__codelineno-4-6 name=__codelineno-4-6></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=mi>128</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>8</span><span class=p>),</span>
</span><span id=__span-4-7><a href=#__codelineno-4-7 id=__codelineno-4-7 name=__codelineno-4-7></a>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span><span id=__span-4-8><a href=#__codelineno-4-8 id=__codelineno-4-8 name=__codelineno-4-8></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Unflatten</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>8</span><span class=p>)),</span>
</span><span id=__span-4-9><a href=#__codelineno-4-9 id=__codelineno-4-9 name=__codelineno-4-9></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Upsample</span><span class=p>(</span><span class=n>scale_factor</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span><span id=__span-4-10><a href=#__codelineno-4-10 id=__codelineno-4-10 name=__codelineno-4-10></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-4-11><a href=#__codelineno-4-11 id=__codelineno-4-11 name=__codelineno-4-11></a>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.78</span><span class=p>),</span>
</span><span id=__span-4-12><a href=#__codelineno-4-12 id=__codelineno-4-12 name=__codelineno-4-12></a>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span><span id=__span-4-13><a href=#__codelineno-4-13 id=__codelineno-4-13 name=__codelineno-4-13></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Upsample</span><span class=p>(</span><span class=n>scale_factor</span><span class=o>=</span><span class=mi>2</span><span class=p>),</span>
</span><span id=__span-4-14><a href=#__codelineno-4-14 id=__codelineno-4-14 name=__codelineno-4-14></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-4-15><a href=#__codelineno-4-15 id=__codelineno-4-15 name=__codelineno-4-15></a>            <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.78</span><span class=p>),</span>
</span><span id=__span-4-16><a href=#__codelineno-4-16 id=__codelineno-4-16 name=__codelineno-4-16></a>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span><span id=__span-4-17><a href=#__codelineno-4-17 id=__codelineno-4-17 name=__codelineno-4-17></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-4-18><a href=#__codelineno-4-18 id=__codelineno-4-18 name=__codelineno-4-18></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>
</span><span id=__span-4-19><a href=#__codelineno-4-19 id=__codelineno-4-19 name=__codelineno-4-19></a>        <span class=p>)</span>
</span><span id=__span-4-20><a href=#__codelineno-4-20 id=__codelineno-4-20 name=__codelineno-4-20></a>
</span><span id=__span-4-21><a href=#__codelineno-4-21 id=__codelineno-4-21 name=__codelineno-4-21></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>z</span><span class=p>):</span>
</span><span id=__span-4-22><a href=#__codelineno-4-22 id=__codelineno-4-22 name=__codelineno-4-22></a>        <span class=n>img</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-4-23><a href=#__codelineno-4-23 id=__codelineno-4-23 name=__codelineno-4-23></a>        <span class=k>return</span> <span class=n>img</span>
</span></code></pre></div> <h4 id=6-building-the-discriminator>6. Building the Discriminator</h4> <p>Create a binary classifier network that distinguishes real from fake images. Use convolutional layers, batch normalization, dropout, LeakyReLU activation and a Sigmoid output layer to give a probability between 0 and 1.</p> <ul> <li><strong>nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)</strong>: Second convolutional layer increasing channels to 64, downsampling further.</li> <li><strong>nn.BatchNorm2d(256, momentum=0.8)</strong>: Batch normalization for 256 feature maps with momentum 0.8.</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a href=#__codelineno-5-1 id=__codelineno-5-1 name=__codelineno-5-1></a><span class=k>class</span><span class=w> </span><span class=nc>Discriminator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-5-2><a href=#__codelineno-5-2 id=__codelineno-5-2 name=__codelineno-5-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-5-3><a href=#__codelineno-5-3 id=__codelineno-5-3 name=__codelineno-5-3></a>        <span class=nb>super</span><span class=p>(</span><span class=n>Discriminator</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-5-4><a href=#__codelineno-5-4 id=__codelineno-5-4 name=__codelineno-5-4></a>
</span><span id=__span-5-5><a href=#__codelineno-5-5 id=__codelineno-5-5 name=__codelineno-5-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-5-6><a href=#__codelineno-5-6 id=__codelineno-5-6 name=__codelineno-5-6></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-5-7><a href=#__codelineno-5-7 id=__codelineno-5-7 name=__codelineno-5-7></a>        <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>),</span>
</span><span id=__span-5-8><a href=#__codelineno-5-8 id=__codelineno-5-8 name=__codelineno-5-8></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.25</span><span class=p>),</span>
</span><span id=__span-5-9><a href=#__codelineno-5-9 id=__codelineno-5-9 name=__codelineno-5-9></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-5-10><a href=#__codelineno-5-10 id=__codelineno-5-10 name=__codelineno-5-10></a>        <span class=n>nn</span><span class=o>.</span><span class=n>ZeroPad2d</span><span class=p>((</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)),</span>
</span><span id=__span-5-11><a href=#__codelineno-5-11 id=__codelineno-5-11 name=__codelineno-5-11></a>        <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.82</span><span class=p>),</span>
</span><span id=__span-5-12><a href=#__codelineno-5-12 id=__codelineno-5-12 name=__codelineno-5-12></a>        <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.25</span><span class=p>),</span>
</span><span id=__span-5-13><a href=#__codelineno-5-13 id=__codelineno-5-13 name=__codelineno-5-13></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.25</span><span class=p>),</span>
</span><span id=__span-5-14><a href=#__codelineno-5-14 id=__codelineno-5-14 name=__codelineno-5-14></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-5-15><a href=#__codelineno-5-15 id=__codelineno-5-15 name=__codelineno-5-15></a>        <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.82</span><span class=p>),</span>
</span><span id=__span-5-16><a href=#__codelineno-5-16 id=__codelineno-5-16 name=__codelineno-5-16></a>        <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.2</span><span class=p>),</span>
</span><span id=__span-5-17><a href=#__codelineno-5-17 id=__codelineno-5-17 name=__codelineno-5-17></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.25</span><span class=p>),</span>
</span><span id=__span-5-18><a href=#__codelineno-5-18 id=__codelineno-5-18 name=__codelineno-5-18></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span><span id=__span-5-19><a href=#__codelineno-5-19 id=__codelineno-5-19 name=__codelineno-5-19></a>        <span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>momentum</span><span class=o>=</span><span class=mf>0.8</span><span class=p>),</span>
</span><span id=__span-5-20><a href=#__codelineno-5-20 id=__codelineno-5-20 name=__codelineno-5-20></a>        <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=mf>0.25</span><span class=p>),</span>
</span><span id=__span-5-21><a href=#__codelineno-5-21 id=__codelineno-5-21 name=__codelineno-5-21></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.25</span><span class=p>),</span>
</span><span id=__span-5-22><a href=#__codelineno-5-22 id=__codelineno-5-22 name=__codelineno-5-22></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(),</span>
</span><span id=__span-5-23><a href=#__codelineno-5-23 id=__codelineno-5-23 name=__codelineno-5-23></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span> <span class=o>*</span> <span class=mi>5</span> <span class=o>*</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span><span id=__span-5-24><a href=#__codelineno-5-24 id=__codelineno-5-24 name=__codelineno-5-24></a>        <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span><span id=__span-5-25><a href=#__codelineno-5-25 id=__codelineno-5-25 name=__codelineno-5-25></a>    <span class=p>)</span>
</span><span id=__span-5-26><a href=#__codelineno-5-26 id=__codelineno-5-26 name=__codelineno-5-26></a>
</span><span id=__span-5-27><a href=#__codelineno-5-27 id=__codelineno-5-27 name=__codelineno-5-27></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img</span><span class=p>):</span>
</span><span id=__span-5-28><a href=#__codelineno-5-28 id=__codelineno-5-28 name=__codelineno-5-28></a>        <span class=n>validity</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>
</span><span id=__span-5-29><a href=#__codelineno-5-29 id=__codelineno-5-29 name=__codelineno-5-29></a>        <span class=k>return</span> <span class=n>validity</span>
</span></code></pre></div> <h4 id=7-initializing>7. Initializing</h4> <ul> <li>Generator and Discriminator are initialized on the available device (GPU or CPU).</li> <li>Binary Cross-Entropy (BCE) Loss is chosen as the loss function.</li> <li>Adam optimizers are defined separately for the generator and discriminator with specified learning rates and betas.</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a href=#__codelineno-6-1 id=__codelineno-6-1 name=__codelineno-6-1></a><span class=n>generator</span> <span class=o>=</span> <span class=n>Generator</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-6-2><a href=#__codelineno-6-2 id=__codelineno-6-2 name=__codelineno-6-2></a><span class=n>discriminator</span> <span class=o>=</span> <span class=n>Discriminator</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-6-3><a href=#__codelineno-6-3 id=__codelineno-6-3 name=__codelineno-6-3></a>
</span><span id=__span-6-4><a href=#__codelineno-6-4 id=__codelineno-6-4 name=__codelineno-6-4></a><span class=n>adversarial_loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>BCELoss</span><span class=p>()</span>
</span><span id=__span-6-5><a href=#__codelineno-6-5 id=__codelineno-6-5 name=__codelineno-6-5></a>
</span><span id=__span-6-6><a href=#__codelineno-6-6 id=__codelineno-6-6 name=__codelineno-6-6></a><span class=n>optimizer_G</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>generator</span><span class=o>.</span><span class=n>parameters</span><span class=p>()</span>\
</span><span id=__span-6-7><a href=#__codelineno-6-7 id=__codelineno-6-7 name=__codelineno-6-7></a>                         <span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=n>beta1</span><span class=p>,</span> <span class=n>beta2</span><span class=p>))</span>
</span><span id=__span-6-8><a href=#__codelineno-6-8 id=__codelineno-6-8 name=__codelineno-6-8></a><span class=n>optimizer_D</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>discriminator</span><span class=o>.</span><span class=n>parameters</span><span class=p>()</span>\
</span><span id=__span-6-9><a href=#__codelineno-6-9 id=__codelineno-6-9 name=__codelineno-6-9></a>                         <span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=n>beta1</span><span class=p>,</span> <span class=n>beta2</span><span class=p>))</span>
</span></code></pre></div> <h4 id=8-training>8. Training</h4> <p>Train the discriminator on real and fake images, then update the generator to improve its fake image quality. Track losses and visualize generated images after each epoch.</p> <ul> <li><strong>valid = torch.ones(real_images.size(0), 1, device=device)</strong>: Create a tensor of ones representing real labels for the discriminator.</li> <li><strong>fake = torch.zeros(real_images.size(0), 1, device=device)</strong>: Create a tensor of zeros representing fake labels for the discriminator.</li> <li><strong>z = torch.randn(real_images.size(0), latent_dim, device=device)</strong>: Generate random noise vectors as input for the generator.</li> <li><strong>g_loss = adversarial_loss(discriminator(gen_images), valid)</strong>: Calculate generator loss based on the discriminator classifying fake images as real.</li> <li><strong>grid = torchvision.utils.make_grid(generated, nrow=4, normalize=True)</strong>: Arrange generated images into a grid for display, normalizing pixel values.</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a href=#__codelineno-7-1 id=__codelineno-7-1 name=__codelineno-7-1></a><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span><span id=__span-7-2><a href=#__codelineno-7-2 id=__codelineno-7-2 name=__codelineno-7-2></a>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>batch</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>dataloader</span><span class=p>):</span>
</span><span id=__span-7-3><a href=#__codelineno-7-3 id=__codelineno-7-3 name=__codelineno-7-3></a>
</span><span id=__span-7-4><a href=#__codelineno-7-4 id=__codelineno-7-4 name=__codelineno-7-4></a>        <span class=n>real_images</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-7-5><a href=#__codelineno-7-5 id=__codelineno-7-5 name=__codelineno-7-5></a>
</span><span id=__span-7-6><a href=#__codelineno-7-6 id=__codelineno-7-6 name=__codelineno-7-6></a>        <span class=c1># Create a tensor of ones representing real labels for the discriminator.</span>
</span><span id=__span-7-7><a href=#__codelineno-7-7 id=__codelineno-7-7 name=__codelineno-7-7></a>        <span class=n>valid</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>real_images</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-7-8><a href=#__codelineno-7-8 id=__codelineno-7-8 name=__codelineno-7-8></a>        <span class=c1># Create a tensor of zeros representing fake labels for the discriminator.</span>
</span><span id=__span-7-9><a href=#__codelineno-7-9 id=__codelineno-7-9 name=__codelineno-7-9></a>        <span class=n>fake</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>real_images</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=mi>1</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-7-10><a href=#__codelineno-7-10 id=__codelineno-7-10 name=__codelineno-7-10></a>
</span><span id=__span-7-11><a href=#__codelineno-7-11 id=__codelineno-7-11 name=__codelineno-7-11></a>        <span class=n>real_images</span> <span class=o>=</span> <span class=n>real_images</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-7-12><a href=#__codelineno-7-12 id=__codelineno-7-12 name=__codelineno-7-12></a>
</span><span id=__span-7-13><a href=#__codelineno-7-13 id=__codelineno-7-13 name=__codelineno-7-13></a>        <span class=n>optimizer_D</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-7-14><a href=#__codelineno-7-14 id=__codelineno-7-14 name=__codelineno-7-14></a>
</span><span id=__span-7-15><a href=#__codelineno-7-15 id=__codelineno-7-15 name=__codelineno-7-15></a>        <span class=c1># Generate random noise vectors as input for the generator.</span>
</span><span id=__span-7-16><a href=#__codelineno-7-16 id=__codelineno-7-16 name=__codelineno-7-16></a>        <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>real_images</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=n>latent_dim</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-7-17><a href=#__codelineno-7-17 id=__codelineno-7-17 name=__codelineno-7-17></a>
</span><span id=__span-7-18><a href=#__codelineno-7-18 id=__codelineno-7-18 name=__codelineno-7-18></a>        <span class=n>fake_images</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-7-19><a href=#__codelineno-7-19 id=__codelineno-7-19 name=__codelineno-7-19></a>
</span><span id=__span-7-20><a href=#__codelineno-7-20 id=__codelineno-7-20 name=__codelineno-7-20></a>        <span class=n>real_loss</span> <span class=o>=</span> <span class=n>adversarial_loss</span><span class=p>(</span><span class=n>discriminator</span>\
</span><span id=__span-7-21><a href=#__codelineno-7-21 id=__codelineno-7-21 name=__codelineno-7-21></a>                                     <span class=p>(</span><span class=n>real_images</span><span class=p>),</span> <span class=n>valid</span><span class=p>)</span>
</span><span id=__span-7-22><a href=#__codelineno-7-22 id=__codelineno-7-22 name=__codelineno-7-22></a>        <span class=n>fake_loss</span> <span class=o>=</span> <span class=n>adversarial_loss</span><span class=p>(</span><span class=n>discriminator</span>\
</span><span id=__span-7-23><a href=#__codelineno-7-23 id=__codelineno-7-23 name=__codelineno-7-23></a>                                     <span class=p>(</span><span class=n>fake_images</span><span class=o>.</span><span class=n>detach</span><span class=p>()),</span> <span class=n>fake</span><span class=p>)</span>
</span><span id=__span-7-24><a href=#__codelineno-7-24 id=__codelineno-7-24 name=__codelineno-7-24></a>        <span class=n>d_loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>real_loss</span> <span class=o>+</span> <span class=n>fake_loss</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>
</span><span id=__span-7-25><a href=#__codelineno-7-25 id=__codelineno-7-25 name=__codelineno-7-25></a>
</span><span id=__span-7-26><a href=#__codelineno-7-26 id=__codelineno-7-26 name=__codelineno-7-26></a>        <span class=n>d_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-7-27><a href=#__codelineno-7-27 id=__codelineno-7-27 name=__codelineno-7-27></a>        <span class=n>optimizer_D</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-7-28><a href=#__codelineno-7-28 id=__codelineno-7-28 name=__codelineno-7-28></a>
</span><span id=__span-7-29><a href=#__codelineno-7-29 id=__codelineno-7-29 name=__codelineno-7-29></a>        <span class=n>optimizer_G</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span><span id=__span-7-30><a href=#__codelineno-7-30 id=__codelineno-7-30 name=__codelineno-7-30></a>
</span><span id=__span-7-31><a href=#__codelineno-7-31 id=__codelineno-7-31 name=__codelineno-7-31></a>        <span class=n>gen_images</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-7-32><a href=#__codelineno-7-32 id=__codelineno-7-32 name=__codelineno-7-32></a>
</span><span id=__span-7-33><a href=#__codelineno-7-33 id=__codelineno-7-33 name=__codelineno-7-33></a>        <span class=c1># Calculate generator loss based on the discriminator classifying fake images as real.</span>
</span><span id=__span-7-34><a href=#__codelineno-7-34 id=__codelineno-7-34 name=__codelineno-7-34></a>        <span class=n>g_loss</span> <span class=o>=</span> <span class=n>adversarial_loss</span><span class=p>(</span><span class=n>discriminator</span><span class=p>(</span><span class=n>gen_images</span><span class=p>),</span> <span class=n>valid</span><span class=p>)</span>
</span><span id=__span-7-35><a href=#__codelineno-7-35 id=__codelineno-7-35 name=__codelineno-7-35></a>        <span class=n>g_loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span><span id=__span-7-36><a href=#__codelineno-7-36 id=__codelineno-7-36 name=__codelineno-7-36></a>        <span class=n>optimizer_G</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span><span id=__span-7-37><a href=#__codelineno-7-37 id=__codelineno-7-37 name=__codelineno-7-37></a>
</span><span id=__span-7-38><a href=#__codelineno-7-38 id=__codelineno-7-38 name=__codelineno-7-38></a>        <span class=k>if</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-7-39><a href=#__codelineno-7-39 id=__codelineno-7-39 name=__codelineno-7-39></a>            <span class=nb>print</span><span class=p>(</span>
</span><span id=__span-7-40><a href=#__codelineno-7-40 id=__codelineno-7-40 name=__codelineno-7-40></a>                <span class=sa>f</span><span class=s2>"Epoch [</span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>num_epochs</span><span class=si>}</span><span class=s2>]</span><span class=se>\</span>
</span><span id=__span-7-41><a href=#__codelineno-7-41 id=__codelineno-7-41 name=__codelineno-7-41></a><span class=s2>                        Batch </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>dataloader</span><span class=p>)</span><span class=si>}</span><span class=s2> "</span>
</span><span id=__span-7-42><a href=#__codelineno-7-42 id=__codelineno-7-42 name=__codelineno-7-42></a>                <span class=sa>f</span><span class=s2>"Discriminator Loss: </span><span class=si>{</span><span class=n>d_loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2> "</span>
</span><span id=__span-7-43><a href=#__codelineno-7-43 id=__codelineno-7-43 name=__codelineno-7-43></a>                <span class=sa>f</span><span class=s2>"Generator Loss: </span><span class=si>{</span><span class=n>g_loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>"</span>
</span><span id=__span-7-44><a href=#__codelineno-7-44 id=__codelineno-7-44 name=__codelineno-7-44></a>            <span class=p>)</span>
</span><span id=__span-7-45><a href=#__codelineno-7-45 id=__codelineno-7-45 name=__codelineno-7-45></a>    <span class=k>if</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>10</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-7-46><a href=#__codelineno-7-46 id=__codelineno-7-46 name=__codelineno-7-46></a>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span><span id=__span-7-47><a href=#__codelineno-7-47 id=__codelineno-7-47 name=__codelineno-7-47></a>            <span class=n>z</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-7-48><a href=#__codelineno-7-48 id=__codelineno-7-48 name=__codelineno-7-48></a>            <span class=n>generated</span> <span class=o>=</span> <span class=n>generator</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>cpu</span><span class=p>()</span>
</span><span id=__span-7-49><a href=#__codelineno-7-49 id=__codelineno-7-49 name=__codelineno-7-49></a>            <span class=c1># Arrange generated images into a grid for display, normalizing pixel values.</span>
</span><span id=__span-7-50><a href=#__codelineno-7-50 id=__codelineno-7-50 name=__codelineno-7-50></a>            <span class=n>grid</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>make_grid</span><span class=p>(</span><span class=n>generated</span><span class=p>,</span>\
</span><span id=__span-7-51><a href=#__codelineno-7-51 id=__codelineno-7-51 name=__codelineno-7-51></a>                                        <span class=n>nrow</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>normalize</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-7-52><a href=#__codelineno-7-52 id=__codelineno-7-52 name=__codelineno-7-52></a>            <span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>grid</span><span class=p>,</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>)))</span>
</span><span id=__span-7-53><a href=#__codelineno-7-53 id=__codelineno-7-53 name=__codelineno-7-53></a>            <span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s2>"off"</span><span class=p>)</span>
</span><span id=__span-7-54><a href=#__codelineno-7-54 id=__codelineno-7-54 name=__codelineno-7-54></a>            <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span><span id=__span-7-55><a href=#__codelineno-7-55 id=__codelineno-7-55 name=__codelineno-7-55></a>
</span><span id=__span-7-56><a href=#__codelineno-7-56 id=__codelineno-7-56 name=__codelineno-7-56></a><span class=n>plt</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></code></pre></div> <div class="language-text highlight"><pre><span></span><code>Epoch [1/30]                        Batch 100/1563 Discriminator Loss: 0.6310 Generator Loss: 1.0856
Epoch [1/30]                        Batch 200/1563 Discriminator Loss: 0.7243 Generator Loss: 1.0722
Epoch [1/30]                        Batch 300/1563 Discriminator Loss: 0.7337 Generator Loss: 1.1005
Epoch [1/30]                        Batch 400/1563 Discriminator Loss: 0.6917 Generator Loss: 1.1717
Epoch [1/30]                        Batch 500/1563 Discriminator Loss: 0.5310 Generator Loss: 1.1474
Epoch [1/30]                        Batch 600/1563 Discriminator Loss: 0.7355 Generator Loss: 0.8708
Epoch [1/30]                        Batch 700/1563 Discriminator Loss: 0.6745 Generator Loss: 1.2315
Epoch [1/30]                        Batch 800/1563 Discriminator Loss: 0.7858 Generator Loss: 0.8245
Epoch [1/30]                        Batch 900/1563 Discriminator Loss: 0.6330 Generator Loss: 0.9612
Epoch [1/30]                        Batch 1000/1563 Discriminator Loss: 0.6853 Generator Loss: 1.1186
Epoch [1/30]                        Batch 1100/1563 Discriminator Loss: 0.5872 Generator Loss: 1.3537
Epoch [1/30]                        Batch 1200/1563 Discriminator Loss: 0.6135 Generator Loss: 1.0075
Epoch [1/30]                        Batch 1300/1563 Discriminator Loss: 0.5315 Generator Loss: 1.4053
Epoch [1/30]                        Batch 1400/1563 Discriminator Loss: 0.7756 Generator Loss: 0.8813
Epoch [1/30]                        Batch 1500/1563 Discriminator Loss: 0.6498 Generator Loss: 0.8439
Epoch [2/30]                        Batch 100/1563 Discriminator Loss: 0.6724 Generator Loss: 0.7996
Epoch [2/30]                        Batch 200/1563 Discriminator Loss: 0.8157 Generator Loss: 0.7308
Epoch [2/30]                        Batch 300/1563 Discriminator Loss: 0.5542 Generator Loss: 1.0256
Epoch [2/30]                        Batch 400/1563 Discriminator Loss: 0.5977 Generator Loss: 1.1029
Epoch [2/30]                        Batch 500/1563 Discriminator Loss: 0.6931 Generator Loss: 0.8717
Epoch [2/30]                        Batch 600/1563 Discriminator Loss: 0.5693 Generator Loss: 1.3365
Epoch [2/30]                        Batch 700/1563 Discriminator Loss: 0.6488 Generator Loss: 0.9503
Epoch [2/30]                        Batch 800/1563 Discriminator Loss: 0.6761 Generator Loss: 0.8909
Epoch [2/30]                        Batch 900/1563 Discriminator Loss: 0.5708 Generator Loss: 0.9972
Epoch [2/30]                        Batch 1000/1563 Discriminator Loss: 0.6001 Generator Loss: 1.1221
Epoch [2/30]                        Batch 1100/1563 Discriminator Loss: 0.6978 Generator Loss: 0.8663
Epoch [2/30]                        Batch 1200/1563 Discriminator Loss: 0.7402 Generator Loss: 1.1249
Epoch [2/30]                        Batch 1300/1563 Discriminator Loss: 0.6372 Generator Loss: 1.0888
Epoch [2/30]                        Batch 1400/1563 Discriminator Loss: 0.6317 Generator Loss: 0.9584
Epoch [2/30]                        Batch 1500/1563 Discriminator Loss: 0.5677 Generator Loss: 1.0163
Epoch [3/30]                        Batch 100/1563 Discriminator Loss: 0.6370 Generator Loss: 1.3195
Epoch [3/30]                        Batch 200/1563 Discriminator Loss: 0.5646 Generator Loss: 1.3755
Epoch [3/30]                        Batch 300/1563 Discriminator Loss: 0.7698 Generator Loss: 0.8023
Epoch [3/30]                        Batch 400/1563 Discriminator Loss: 0.5839 Generator Loss: 1.0319
Epoch [3/30]                        Batch 500/1563 Discriminator Loss: 0.5613 Generator Loss: 1.2391
Epoch [3/30]                        Batch 600/1563 Discriminator Loss: 0.5798 Generator Loss: 1.1867
Epoch [3/30]                        Batch 700/1563 Discriminator Loss: 0.6622 Generator Loss: 1.4665
Epoch [3/30]                        Batch 800/1563 Discriminator Loss: 0.7389 Generator Loss: 1.3259
Epoch [3/30]                        Batch 900/1563 Discriminator Loss: 0.6855 Generator Loss: 1.4243
Epoch [3/30]                        Batch 1000/1563 Discriminator Loss: 0.5348 Generator Loss: 1.5141
Epoch [3/30]                        Batch 1100/1563 Discriminator Loss: 0.4696 Generator Loss: 1.3948
Epoch [3/30]                        Batch 1200/1563 Discriminator Loss: 0.5467 Generator Loss: 0.6435
Epoch [3/30]                        Batch 1300/1563 Discriminator Loss: 0.6741 Generator Loss: 0.9211
Epoch [3/30]                        Batch 1400/1563 Discriminator Loss: 0.4664 Generator Loss: 1.0165
Epoch [3/30]                        Batch 1500/1563 Discriminator Loss: 0.4799 Generator Loss: 0.9388
Epoch [4/30]                        Batch 100/1563 Discriminator Loss: 0.6887 Generator Loss: 0.9108
Epoch [4/30]                        Batch 200/1563 Discriminator Loss: 0.7852 Generator Loss: 0.8538
Epoch [4/30]                        Batch 300/1563 Discriminator Loss: 0.5036 Generator Loss: 0.7702
Epoch [4/30]                        Batch 400/1563 Discriminator Loss: 0.4476 Generator Loss: 1.3409
Epoch [4/30]                        Batch 500/1563 Discriminator Loss: 0.6550 Generator Loss: 1.2383
Epoch [4/30]                        Batch 600/1563 Discriminator Loss: 0.7891 Generator Loss: 1.6518
Epoch [4/30]                        Batch 700/1563 Discriminator Loss: 0.5458 Generator Loss: 1.3395
Epoch [4/30]                        Batch 800/1563 Discriminator Loss: 0.6637 Generator Loss: 0.9561
Epoch [4/30]                        Batch 900/1563 Discriminator Loss: 0.6408 Generator Loss: 1.4673
Epoch [4/30]                        Batch 1000/1563 Discriminator Loss: 0.5879 Generator Loss: 0.8866
Epoch [4/30]                        Batch 1100/1563 Discriminator Loss: 0.5762 Generator Loss: 1.2028
Epoch [4/30]                        Batch 1200/1563 Discriminator Loss: 0.4307 Generator Loss: 1.8184
Epoch [4/30]                        Batch 1300/1563 Discriminator Loss: 0.8079 Generator Loss: 0.7371
Epoch [4/30]                        Batch 1400/1563 Discriminator Loss: 0.5739 Generator Loss: 0.9877
Epoch [4/30]                        Batch 1500/1563 Discriminator Loss: 0.5317 Generator Loss: 1.7946
Epoch [5/30]                        Batch 100/1563 Discriminator Loss: 0.4036 Generator Loss: 1.1946
Epoch [5/30]                        Batch 200/1563 Discriminator Loss: 0.4312 Generator Loss: 1.4645
Epoch [5/30]                        Batch 300/1563 Discriminator Loss: 0.4584 Generator Loss: 1.0263
Epoch [5/30]                        Batch 400/1563 Discriminator Loss: 0.5096 Generator Loss: 1.4634
Epoch [5/30]                        Batch 500/1563 Discriminator Loss: 0.6048 Generator Loss: 1.7682
Epoch [5/30]                        Batch 600/1563 Discriminator Loss: 0.7375 Generator Loss: 1.0852
Epoch [5/30]                        Batch 700/1563 Discriminator Loss: 0.6004 Generator Loss: 1.3448
Epoch [5/30]                        Batch 800/1563 Discriminator Loss: 0.5066 Generator Loss: 1.0084
Epoch [5/30]                        Batch 900/1563 Discriminator Loss: 0.6089 Generator Loss: 1.0463
Epoch [5/30]                        Batch 1000/1563 Discriminator Loss: 0.4824 Generator Loss: 1.2175
Epoch [5/30]                        Batch 1100/1563 Discriminator Loss: 0.7544 Generator Loss: 0.8785
Epoch [5/30]                        Batch 1200/1563 Discriminator Loss: 0.4876 Generator Loss: 1.2693
Epoch [5/30]                        Batch 1300/1563 Discriminator Loss: 0.4419 Generator Loss: 0.9601
Epoch [5/30]                        Batch 1400/1563 Discriminator Loss: 0.5805 Generator Loss: 1.0689
Epoch [5/30]                        Batch 1500/1563 Discriminator Loss: 0.5700 Generator Loss: 1.5043
Epoch [6/30]                        Batch 100/1563 Discriminator Loss: 0.3690 Generator Loss: 1.8759
Epoch [6/30]                        Batch 200/1563 Discriminator Loss: 0.4619 Generator Loss: 1.5484
Epoch [6/30]                        Batch 300/1563 Discriminator Loss: 0.5607 Generator Loss: 1.1555
Epoch [6/30]                        Batch 400/1563 Discriminator Loss: 0.3786 Generator Loss: 1.5910
Epoch [6/30]                        Batch 500/1563 Discriminator Loss: 0.6497 Generator Loss: 0.9485
Epoch [6/30]                        Batch 600/1563 Discriminator Loss: 0.4534 Generator Loss: 1.4235
Epoch [6/30]                        Batch 700/1563 Discriminator Loss: 0.5807 Generator Loss: 0.6237
Epoch [6/30]                        Batch 800/1563 Discriminator Loss: 0.5381 Generator Loss: 1.4090
Epoch [6/30]                        Batch 900/1563 Discriminator Loss: 0.4686 Generator Loss: 1.3971
Epoch [6/30]                        Batch 1000/1563 Discriminator Loss: 0.4787 Generator Loss: 1.4606
Epoch [6/30]                        Batch 1100/1563 Discriminator Loss: 0.6769 Generator Loss: 1.3275
Epoch [6/30]                        Batch 1200/1563 Discriminator Loss: 0.4402 Generator Loss: 1.3996
Epoch [6/30]                        Batch 1300/1563 Discriminator Loss: 0.6449 Generator Loss: 1.2970
Epoch [6/30]                        Batch 1400/1563 Discriminator Loss: 0.7675 Generator Loss: 1.1988
Epoch [6/30]                        Batch 1500/1563 Discriminator Loss: 0.6860 Generator Loss: 1.0752
Epoch [7/30]                        Batch 100/1563 Discriminator Loss: 0.5656 Generator Loss: 0.5764
Epoch [7/30]                        Batch 200/1563 Discriminator Loss: 0.4979 Generator Loss: 1.2684
Epoch [7/30]                        Batch 300/1563 Discriminator Loss: 0.6714 Generator Loss: 0.9369
Epoch [7/30]                        Batch 400/1563 Discriminator Loss: 0.6402 Generator Loss: 1.0803
Epoch [7/30]                        Batch 500/1563 Discriminator Loss: 0.5717 Generator Loss: 1.3460
Epoch [7/30]                        Batch 600/1563 Discriminator Loss: 0.4720 Generator Loss: 1.2296
Epoch [7/30]                        Batch 700/1563 Discriminator Loss: 0.5592 Generator Loss: 1.6116
Epoch [7/30]                        Batch 800/1563 Discriminator Loss: 0.7146 Generator Loss: 1.3487
Epoch [7/30]                        Batch 900/1563 Discriminator Loss: 0.4830 Generator Loss: 1.3931
Epoch [7/30]                        Batch 1000/1563 Discriminator Loss: 0.4894 Generator Loss: 1.7213
Epoch [7/30]                        Batch 1100/1563 Discriminator Loss: 0.8574 Generator Loss: 1.3492
Epoch [7/30]                        Batch 1200/1563 Discriminator Loss: 0.8195 Generator Loss: 0.9998
Epoch [7/30]                        Batch 1300/1563 Discriminator Loss: 0.7399 Generator Loss: 1.3046
Epoch [7/30]                        Batch 1400/1563 Discriminator Loss: 0.5688 Generator Loss: 0.9374
Epoch [7/30]                        Batch 1500/1563 Discriminator Loss: 0.4341 Generator Loss: 1.2548
Epoch [8/30]                        Batch 100/1563 Discriminator Loss: 0.6533 Generator Loss: 1.1707
Epoch [8/30]                        Batch 200/1563 Discriminator Loss: 0.4287 Generator Loss: 1.1481
Epoch [8/30]                        Batch 300/1563 Discriminator Loss: 0.5281 Generator Loss: 1.7373
Epoch [8/30]                        Batch 400/1563 Discriminator Loss: 0.5967 Generator Loss: 0.9080
Epoch [8/30]                        Batch 500/1563 Discriminator Loss: 0.6105 Generator Loss: 1.0256
Epoch [8/30]                        Batch 600/1563 Discriminator Loss: 0.5247 Generator Loss: 1.5951
Epoch [8/30]                        Batch 700/1563 Discriminator Loss: 0.7669 Generator Loss: 1.3100
Epoch [8/30]                        Batch 800/1563 Discriminator Loss: 0.5892 Generator Loss: 1.1584
Epoch [8/30]                        Batch 900/1563 Discriminator Loss: 0.8667 Generator Loss: 0.8409
Epoch [8/30]                        Batch 1000/1563 Discriminator Loss: 0.8007 Generator Loss: 0.6958
Epoch [8/30]                        Batch 1100/1563 Discriminator Loss: 0.6248 Generator Loss: 1.0027
Epoch [8/30]                        Batch 1200/1563 Discriminator Loss: 0.9791 Generator Loss: 1.2779
Epoch [8/30]                        Batch 1300/1563 Discriminator Loss: 0.6273 Generator Loss: 1.3308
Epoch [8/30]                        Batch 1400/1563 Discriminator Loss: 0.4397 Generator Loss: 2.0570
Epoch [8/30]                        Batch 1500/1563 Discriminator Loss: 0.5385 Generator Loss: 1.1689
Epoch [9/30]                        Batch 100/1563 Discriminator Loss: 0.8302 Generator Loss: 1.3391
Epoch [9/30]                        Batch 200/1563 Discriminator Loss: 0.4323 Generator Loss: 1.5036
Epoch [9/30]                        Batch 300/1563 Discriminator Loss: 0.6001 Generator Loss: 2.3062
Epoch [9/30]                        Batch 400/1563 Discriminator Loss: 0.5612 Generator Loss: 1.0222
Epoch [9/30]                        Batch 500/1563 Discriminator Loss: 0.5333 Generator Loss: 1.5129
Epoch [9/30]                        Batch 600/1563 Discriminator Loss: 0.4739 Generator Loss: 0.9228
Epoch [9/30]                        Batch 700/1563 Discriminator Loss: 0.6237 Generator Loss: 1.3650
Epoch [9/30]                        Batch 800/1563 Discriminator Loss: 0.6755 Generator Loss: 0.7273
Epoch [9/30]                        Batch 900/1563 Discriminator Loss: 0.5411 Generator Loss: 1.7899
Epoch [9/30]                        Batch 1000/1563 Discriminator Loss: 0.5930 Generator Loss: 1.0269
Epoch [9/30]                        Batch 1100/1563 Discriminator Loss: 0.5476 Generator Loss: 1.8822
Epoch [9/30]                        Batch 1200/1563 Discriminator Loss: 0.7660 Generator Loss: 1.4145
Epoch [9/30]                        Batch 1300/1563 Discriminator Loss: 0.5074 Generator Loss: 1.5135
Epoch [9/30]                        Batch 1400/1563 Discriminator Loss: 0.5582 Generator Loss: 1.1927
Epoch [9/30]                        Batch 1500/1563 Discriminator Loss: 0.8168 Generator Loss: 1.2279
Epoch [10/30]                        Batch 100/1563 Discriminator Loss: 0.2423 Generator Loss: 1.7337
Epoch [10/30]                        Batch 200/1563 Discriminator Loss: 0.9663 Generator Loss: 1.0028
Epoch [10/30]                        Batch 300/1563 Discriminator Loss: 0.5886 Generator Loss: 0.9407
Epoch [10/30]                        Batch 400/1563 Discriminator Loss: 0.7274 Generator Loss: 1.0736
Epoch [10/30]                        Batch 500/1563 Discriminator Loss: 0.6266 Generator Loss: 0.8596
Epoch [10/30]                        Batch 600/1563 Discriminator Loss: 0.5225 Generator Loss: 2.0611
Epoch [10/30]                        Batch 700/1563 Discriminator Loss: 0.4990 Generator Loss: 1.6639
Epoch [10/30]                        Batch 800/1563 Discriminator Loss: 0.8305 Generator Loss: 0.8100
Epoch [10/30]                        Batch 900/1563 Discriminator Loss: 0.5522 Generator Loss: 1.2785
Epoch [10/30]                        Batch 1000/1563 Discriminator Loss: 0.8009 Generator Loss: 1.3512
Epoch [10/30]                        Batch 1100/1563 Discriminator Loss: 0.6749 Generator Loss: 1.1727
Epoch [10/30]                        Batch 1200/1563 Discriminator Loss: 0.8850 Generator Loss: 1.1001
Epoch [10/30]                        Batch 1300/1563 Discriminator Loss: 0.6387 Generator Loss: 1.4558
Epoch [10/30]                        Batch 1400/1563 Discriminator Loss: 0.6613 Generator Loss: 2.5028
Epoch [10/30]                        Batch 1500/1563 Discriminator Loss: 0.5191 Generator Loss: 0.9412
</code></pre></div> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=../gan_example_cifar_10_files/gan_example_cifar_10_21_1.png><img alt=png src=../gan_example_cifar_10_files/gan_example_cifar_10_21_1.png></a></p> <div class="language-text highlight"><pre><span></span><code>Epoch [11/30]                        Batch 100/1563 Discriminator Loss: 0.6146 Generator Loss: 1.1872
Epoch [11/30]                        Batch 200/1563 Discriminator Loss: 0.7205 Generator Loss: 1.0533
Epoch [11/30]                        Batch 300/1563 Discriminator Loss: 0.6223 Generator Loss: 1.1095
Epoch [11/30]                        Batch 400/1563 Discriminator Loss: 0.5828 Generator Loss: 0.7897
Epoch [11/30]                        Batch 500/1563 Discriminator Loss: 0.5107 Generator Loss: 1.1712
Epoch [11/30]                        Batch 600/1563 Discriminator Loss: 0.5452 Generator Loss: 1.1918
Epoch [11/30]                        Batch 700/1563 Discriminator Loss: 0.4733 Generator Loss: 1.6043
Epoch [11/30]                        Batch 800/1563 Discriminator Loss: 0.6569 Generator Loss: 1.1823
Epoch [11/30]                        Batch 900/1563 Discriminator Loss: 0.7731 Generator Loss: 0.9131
Epoch [11/30]                        Batch 1000/1563 Discriminator Loss: 0.7283 Generator Loss: 0.9966
Epoch [11/30]                        Batch 1100/1563 Discriminator Loss: 0.6360 Generator Loss: 1.4022
Epoch [11/30]                        Batch 1200/1563 Discriminator Loss: 0.6779 Generator Loss: 1.6380
Epoch [11/30]                        Batch 1300/1563 Discriminator Loss: 0.5721 Generator Loss: 1.0732
Epoch [11/30]                        Batch 1400/1563 Discriminator Loss: 0.5500 Generator Loss: 1.6293
Epoch [11/30]                        Batch 1500/1563 Discriminator Loss: 0.6260 Generator Loss: 1.6629
Epoch [12/30]                        Batch 100/1563 Discriminator Loss: 0.2396 Generator Loss: 1.8887
Epoch [12/30]                        Batch 200/1563 Discriminator Loss: 0.8208 Generator Loss: 0.7347
Epoch [12/30]                        Batch 300/1563 Discriminator Loss: 0.7865 Generator Loss: 1.2288
Epoch [12/30]                        Batch 400/1563 Discriminator Loss: 0.8839 Generator Loss: 1.5515
Epoch [12/30]                        Batch 500/1563 Discriminator Loss: 0.4100 Generator Loss: 1.6313
Epoch [12/30]                        Batch 600/1563 Discriminator Loss: 0.5070 Generator Loss: 0.7579
Epoch [12/30]                        Batch 700/1563 Discriminator Loss: 0.4470 Generator Loss: 1.4843
Epoch [12/30]                        Batch 800/1563 Discriminator Loss: 0.3804 Generator Loss: 1.9155
Epoch [12/30]                        Batch 900/1563 Discriminator Loss: 0.8740 Generator Loss: 0.9627
Epoch [12/30]                        Batch 1000/1563 Discriminator Loss: 0.6253 Generator Loss: 1.2338
Epoch [12/30]                        Batch 1100/1563 Discriminator Loss: 0.5772 Generator Loss: 0.7276
Epoch [12/30]                        Batch 1200/1563 Discriminator Loss: 0.6984 Generator Loss: 1.2770
Epoch [12/30]                        Batch 1300/1563 Discriminator Loss: 0.5281 Generator Loss: 1.6734
Epoch [12/30]                        Batch 1400/1563 Discriminator Loss: 0.7011 Generator Loss: 1.5269
Epoch [12/30]                        Batch 1500/1563 Discriminator Loss: 0.6542 Generator Loss: 1.1141
Epoch [13/30]                        Batch 100/1563 Discriminator Loss: 0.3590 Generator Loss: 1.6407
Epoch [13/30]                        Batch 200/1563 Discriminator Loss: 0.6046 Generator Loss: 0.6858
Epoch [13/30]                        Batch 300/1563 Discriminator Loss: 0.5175 Generator Loss: 2.0719
Epoch [13/30]                        Batch 400/1563 Discriminator Loss: 0.6386 Generator Loss: 1.1742
Epoch [13/30]                        Batch 500/1563 Discriminator Loss: 0.5023 Generator Loss: 1.7407
Epoch [13/30]                        Batch 600/1563 Discriminator Loss: 0.4565 Generator Loss: 1.8108
Epoch [13/30]                        Batch 700/1563 Discriminator Loss: 0.3615 Generator Loss: 0.7747
Epoch [13/30]                        Batch 800/1563 Discriminator Loss: 0.5649 Generator Loss: 1.1583
Epoch [13/30]                        Batch 900/1563 Discriminator Loss: 0.5132 Generator Loss: 1.2286
Epoch [13/30]                        Batch 1000/1563 Discriminator Loss: 0.6191 Generator Loss: 1.4292
Epoch [13/30]                        Batch 1100/1563 Discriminator Loss: 0.4916 Generator Loss: 1.4518
Epoch [13/30]                        Batch 1200/1563 Discriminator Loss: 0.5353 Generator Loss: 1.0762
Epoch [13/30]                        Batch 1300/1563 Discriminator Loss: 0.6027 Generator Loss: 0.7657
Epoch [13/30]                        Batch 1400/1563 Discriminator Loss: 0.6896 Generator Loss: 1.5476
Epoch [13/30]                        Batch 1500/1563 Discriminator Loss: 0.5865 Generator Loss: 0.7674
Epoch [14/30]                        Batch 100/1563 Discriminator Loss: 0.3891 Generator Loss: 1.4870
Epoch [14/30]                        Batch 200/1563 Discriminator Loss: 0.4392 Generator Loss: 0.7682
Epoch [14/30]                        Batch 300/1563 Discriminator Loss: 0.9042 Generator Loss: 0.7021
Epoch [14/30]                        Batch 400/1563 Discriminator Loss: 0.5336 Generator Loss: 1.6118
Epoch [14/30]                        Batch 500/1563 Discriminator Loss: 0.6186 Generator Loss: 1.0359
Epoch [14/30]                        Batch 600/1563 Discriminator Loss: 0.7181 Generator Loss: 0.8545
Epoch [14/30]                        Batch 700/1563 Discriminator Loss: 0.8549 Generator Loss: 0.8246
Epoch [14/30]                        Batch 800/1563 Discriminator Loss: 0.8046 Generator Loss: 1.3709
Epoch [14/30]                        Batch 900/1563 Discriminator Loss: 0.4450 Generator Loss: 1.2583
Epoch [14/30]                        Batch 1000/1563 Discriminator Loss: 0.4912 Generator Loss: 1.8047
Epoch [14/30]                        Batch 1100/1563 Discriminator Loss: 0.4890 Generator Loss: 1.1271
Epoch [14/30]                        Batch 1200/1563 Discriminator Loss: 0.7053 Generator Loss: 1.3877
Epoch [14/30]                        Batch 1300/1563 Discriminator Loss: 0.8480 Generator Loss: 0.9890
Epoch [14/30]                        Batch 1400/1563 Discriminator Loss: 0.3800 Generator Loss: 1.6878
Epoch [14/30]                        Batch 1500/1563 Discriminator Loss: 0.4641 Generator Loss: 1.5752
Epoch [15/30]                        Batch 100/1563 Discriminator Loss: 0.6491 Generator Loss: 1.2644
Epoch [15/30]                        Batch 200/1563 Discriminator Loss: 0.5590 Generator Loss: 0.9048
Epoch [15/30]                        Batch 300/1563 Discriminator Loss: 0.7712 Generator Loss: 1.0408
Epoch [15/30]                        Batch 400/1563 Discriminator Loss: 0.6446 Generator Loss: 1.2220
Epoch [15/30]                        Batch 500/1563 Discriminator Loss: 0.5891 Generator Loss: 0.8177
Epoch [15/30]                        Batch 600/1563 Discriminator Loss: 0.4539 Generator Loss: 1.5968
Epoch [15/30]                        Batch 700/1563 Discriminator Loss: 0.9036 Generator Loss: 1.2203
Epoch [15/30]                        Batch 800/1563 Discriminator Loss: 0.6573 Generator Loss: 1.2048
Epoch [15/30]                        Batch 900/1563 Discriminator Loss: 0.5110 Generator Loss: 0.8048
Epoch [15/30]                        Batch 1000/1563 Discriminator Loss: 0.6662 Generator Loss: 1.6221
Epoch [15/30]                        Batch 1100/1563 Discriminator Loss: 0.4542 Generator Loss: 1.4416
Epoch [15/30]                        Batch 1200/1563 Discriminator Loss: 0.5644 Generator Loss: 1.5006
Epoch [15/30]                        Batch 1300/1563 Discriminator Loss: 0.5741 Generator Loss: 1.5613
Epoch [15/30]                        Batch 1400/1563 Discriminator Loss: 0.5394 Generator Loss: 1.6310
Epoch [15/30]                        Batch 1500/1563 Discriminator Loss: 0.2746 Generator Loss: 1.2002
Epoch [16/30]                        Batch 100/1563 Discriminator Loss: 0.9168 Generator Loss: 1.0404
Epoch [16/30]                        Batch 200/1563 Discriminator Loss: 0.3522 Generator Loss: 2.0820
Epoch [16/30]                        Batch 300/1563 Discriminator Loss: 1.0174 Generator Loss: 1.2016
Epoch [16/30]                        Batch 400/1563 Discriminator Loss: 0.9039 Generator Loss: 0.9897
Epoch [16/30]                        Batch 500/1563 Discriminator Loss: 0.7441 Generator Loss: 1.5617
Epoch [16/30]                        Batch 600/1563 Discriminator Loss: 0.6339 Generator Loss: 0.7029
Epoch [16/30]                        Batch 700/1563 Discriminator Loss: 0.6705 Generator Loss: 1.2205
Epoch [16/30]                        Batch 800/1563 Discriminator Loss: 0.4608 Generator Loss: 1.9291
Epoch [16/30]                        Batch 900/1563 Discriminator Loss: 0.3003 Generator Loss: 2.2707
Epoch [16/30]                        Batch 1000/1563 Discriminator Loss: 0.4935 Generator Loss: 0.9453
Epoch [16/30]                        Batch 1100/1563 Discriminator Loss: 0.6426 Generator Loss: 2.0306
Epoch [16/30]                        Batch 1200/1563 Discriminator Loss: 0.3256 Generator Loss: 1.9450
Epoch [16/30]                        Batch 1300/1563 Discriminator Loss: 0.4460 Generator Loss: 1.3895
Epoch [16/30]                        Batch 1400/1563 Discriminator Loss: 0.5697 Generator Loss: 1.8839
Epoch [16/30]                        Batch 1500/1563 Discriminator Loss: 0.3510 Generator Loss: 1.2868
Epoch [17/30]                        Batch 100/1563 Discriminator Loss: 0.4061 Generator Loss: 1.5765
Epoch [17/30]                        Batch 200/1563 Discriminator Loss: 0.6059 Generator Loss: 1.4607
Epoch [17/30]                        Batch 300/1563 Discriminator Loss: 0.3888 Generator Loss: 2.0352
Epoch [17/30]                        Batch 400/1563 Discriminator Loss: 0.9055 Generator Loss: 1.5344
Epoch [17/30]                        Batch 500/1563 Discriminator Loss: 0.4919 Generator Loss: 2.0243
Epoch [17/30]                        Batch 600/1563 Discriminator Loss: 0.5129 Generator Loss: 1.3443
Epoch [17/30]                        Batch 700/1563 Discriminator Loss: 0.5908 Generator Loss: 1.3944
Epoch [17/30]                        Batch 800/1563 Discriminator Loss: 0.6198 Generator Loss: 1.9401
Epoch [17/30]                        Batch 900/1563 Discriminator Loss: 0.4722 Generator Loss: 1.3389
Epoch [17/30]                        Batch 1000/1563 Discriminator Loss: 0.6337 Generator Loss: 1.2892
Epoch [17/30]                        Batch 1100/1563 Discriminator Loss: 0.4962 Generator Loss: 1.3870
Epoch [17/30]                        Batch 1200/1563 Discriminator Loss: 0.4610 Generator Loss: 0.8908
Epoch [17/30]                        Batch 1300/1563 Discriminator Loss: 0.7297 Generator Loss: 1.1273
Epoch [17/30]                        Batch 1400/1563 Discriminator Loss: 0.7066 Generator Loss: 0.8907
Epoch [17/30]                        Batch 1500/1563 Discriminator Loss: 0.4157 Generator Loss: 1.4886
Epoch [18/30]                        Batch 100/1563 Discriminator Loss: 0.4349 Generator Loss: 2.3884
Epoch [18/30]                        Batch 200/1563 Discriminator Loss: 0.6107 Generator Loss: 0.9189
Epoch [18/30]                        Batch 300/1563 Discriminator Loss: 0.6645 Generator Loss: 0.9639
Epoch [18/30]                        Batch 400/1563 Discriminator Loss: 0.6846 Generator Loss: 1.6454
Epoch [18/30]                        Batch 500/1563 Discriminator Loss: 0.6662 Generator Loss: 1.3799
Epoch [18/30]                        Batch 600/1563 Discriminator Loss: 0.3959 Generator Loss: 1.6886
Epoch [18/30]                        Batch 700/1563 Discriminator Loss: 0.7645 Generator Loss: 0.4036
Epoch [18/30]                        Batch 800/1563 Discriminator Loss: 0.3437 Generator Loss: 1.5305
Epoch [18/30]                        Batch 900/1563 Discriminator Loss: 0.7294 Generator Loss: 1.9114
Epoch [18/30]                        Batch 1000/1563 Discriminator Loss: 0.3701 Generator Loss: 2.1974
Epoch [18/30]                        Batch 1100/1563 Discriminator Loss: 0.3689 Generator Loss: 1.6446
Epoch [18/30]                        Batch 1200/1563 Discriminator Loss: 0.4035 Generator Loss: 1.3158
Epoch [18/30]                        Batch 1300/1563 Discriminator Loss: 0.6356 Generator Loss: 1.4208
Epoch [18/30]                        Batch 1400/1563 Discriminator Loss: 0.6068 Generator Loss: 1.0243
Epoch [18/30]                        Batch 1500/1563 Discriminator Loss: 0.7674 Generator Loss: 1.6801
Epoch [19/30]                        Batch 100/1563 Discriminator Loss: 0.4846 Generator Loss: 1.2064
Epoch [19/30]                        Batch 200/1563 Discriminator Loss: 0.6070 Generator Loss: 1.7031
Epoch [19/30]                        Batch 300/1563 Discriminator Loss: 0.7450 Generator Loss: 1.0996
Epoch [19/30]                        Batch 400/1563 Discriminator Loss: 0.6458 Generator Loss: 1.0814
Epoch [19/30]                        Batch 500/1563 Discriminator Loss: 0.4784 Generator Loss: 1.4959
Epoch [19/30]                        Batch 600/1563 Discriminator Loss: 0.3522 Generator Loss: 1.8114
Epoch [19/30]                        Batch 700/1563 Discriminator Loss: 0.6813 Generator Loss: 1.4926
Epoch [19/30]                        Batch 800/1563 Discriminator Loss: 0.3082 Generator Loss: 1.2779
Epoch [19/30]                        Batch 900/1563 Discriminator Loss: 0.4817 Generator Loss: 1.6282
Epoch [19/30]                        Batch 1000/1563 Discriminator Loss: 0.6549 Generator Loss: 1.8198
Epoch [19/30]                        Batch 1100/1563 Discriminator Loss: 0.5999 Generator Loss: 2.1225
Epoch [19/30]                        Batch 1200/1563 Discriminator Loss: 0.2709 Generator Loss: 1.7114
Epoch [19/30]                        Batch 1300/1563 Discriminator Loss: 0.5228 Generator Loss: 1.3422
Epoch [19/30]                        Batch 1400/1563 Discriminator Loss: 0.6171 Generator Loss: 1.0001
Epoch [19/30]                        Batch 1500/1563 Discriminator Loss: 0.5075 Generator Loss: 1.5396
Epoch [20/30]                        Batch 100/1563 Discriminator Loss: 0.7957 Generator Loss: 1.0529
Epoch [20/30]                        Batch 200/1563 Discriminator Loss: 0.4388 Generator Loss: 0.9466
Epoch [20/30]                        Batch 300/1563 Discriminator Loss: 0.3565 Generator Loss: 1.0153
Epoch [20/30]                        Batch 400/1563 Discriminator Loss: 0.5776 Generator Loss: 1.3406
Epoch [20/30]                        Batch 500/1563 Discriminator Loss: 0.5628 Generator Loss: 1.7318
Epoch [20/30]                        Batch 600/1563 Discriminator Loss: 0.6587 Generator Loss: 1.0502
Epoch [20/30]                        Batch 700/1563 Discriminator Loss: 0.3938 Generator Loss: 1.5252
Epoch [20/30]                        Batch 800/1563 Discriminator Loss: 0.5031 Generator Loss: 1.4253
Epoch [20/30]                        Batch 900/1563 Discriminator Loss: 0.8166 Generator Loss: 0.8745
Epoch [20/30]                        Batch 1000/1563 Discriminator Loss: 0.8261 Generator Loss: 0.7913
Epoch [20/30]                        Batch 1100/1563 Discriminator Loss: 0.3707 Generator Loss: 1.8016
Epoch [20/30]                        Batch 1200/1563 Discriminator Loss: 0.5905 Generator Loss: 1.0425
Epoch [20/30]                        Batch 1300/1563 Discriminator Loss: 0.4316 Generator Loss: 1.2454
Epoch [20/30]                        Batch 1400/1563 Discriminator Loss: 0.6296 Generator Loss: 1.1763
Epoch [20/30]                        Batch 1500/1563 Discriminator Loss: 0.3132 Generator Loss: 1.5287
</code></pre></div> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=../gan_example_cifar_10_files/gan_example_cifar_10_21_3.png><img alt=png src=../gan_example_cifar_10_files/gan_example_cifar_10_21_3.png></a></p> <div class="language-text highlight"><pre><span></span><code>Epoch [21/30]                        Batch 100/1563 Discriminator Loss: 0.6704 Generator Loss: 0.9994
Epoch [21/30]                        Batch 200/1563 Discriminator Loss: 0.4294 Generator Loss: 1.6680
Epoch [21/30]                        Batch 300/1563 Discriminator Loss: 0.3051 Generator Loss: 1.7558
Epoch [21/30]                        Batch 400/1563 Discriminator Loss: 0.5725 Generator Loss: 1.0703
Epoch [21/30]                        Batch 500/1563 Discriminator Loss: 0.5754 Generator Loss: 0.8874
Epoch [21/30]                        Batch 600/1563 Discriminator Loss: 0.5921 Generator Loss: 0.9687
Epoch [21/30]                        Batch 700/1563 Discriminator Loss: 0.3578 Generator Loss: 2.0567
Epoch [21/30]                        Batch 800/1563 Discriminator Loss: 0.3976 Generator Loss: 1.6124
Epoch [21/30]                        Batch 900/1563 Discriminator Loss: 0.5874 Generator Loss: 1.4437
Epoch [21/30]                        Batch 1000/1563 Discriminator Loss: 0.3392 Generator Loss: 1.8707
Epoch [21/30]                        Batch 1100/1563 Discriminator Loss: 0.6126 Generator Loss: 0.8583
Epoch [21/30]                        Batch 1200/1563 Discriminator Loss: 0.6787 Generator Loss: 1.2624
Epoch [21/30]                        Batch 1300/1563 Discriminator Loss: 0.4979 Generator Loss: 1.1952
Epoch [21/30]                        Batch 1400/1563 Discriminator Loss: 0.4609 Generator Loss: 1.2393
Epoch [21/30]                        Batch 1500/1563 Discriminator Loss: 0.4389 Generator Loss: 1.0301
Epoch [22/30]                        Batch 100/1563 Discriminator Loss: 0.5087 Generator Loss: 0.8303
Epoch [22/30]                        Batch 200/1563 Discriminator Loss: 0.5395 Generator Loss: 1.7216
Epoch [22/30]                        Batch 300/1563 Discriminator Loss: 0.7014 Generator Loss: 1.3354
Epoch [22/30]                        Batch 400/1563 Discriminator Loss: 0.4920 Generator Loss: 1.2890
Epoch [22/30]                        Batch 500/1563 Discriminator Loss: 0.6030 Generator Loss: 0.8941
Epoch [22/30]                        Batch 600/1563 Discriminator Loss: 0.5582 Generator Loss: 0.8958
Epoch [22/30]                        Batch 700/1563 Discriminator Loss: 0.5013 Generator Loss: 2.0577
Epoch [22/30]                        Batch 800/1563 Discriminator Loss: 0.6041 Generator Loss: 1.3337
Epoch [22/30]                        Batch 900/1563 Discriminator Loss: 0.5399 Generator Loss: 0.9913
Epoch [22/30]                        Batch 1000/1563 Discriminator Loss: 0.6801 Generator Loss: 0.9158
Epoch [22/30]                        Batch 1100/1563 Discriminator Loss: 0.4355 Generator Loss: 1.8976
Epoch [22/30]                        Batch 1200/1563 Discriminator Loss: 0.6911 Generator Loss: 1.3129
Epoch [22/30]                        Batch 1300/1563 Discriminator Loss: 0.5185 Generator Loss: 1.0487
Epoch [22/30]                        Batch 1400/1563 Discriminator Loss: 0.7541 Generator Loss: 1.2654
Epoch [22/30]                        Batch 1500/1563 Discriminator Loss: 0.4369 Generator Loss: 1.2044
Epoch [23/30]                        Batch 100/1563 Discriminator Loss: 0.5012 Generator Loss: 0.8639
Epoch [23/30]                        Batch 200/1563 Discriminator Loss: 0.7417 Generator Loss: 1.0816
Epoch [23/30]                        Batch 300/1563 Discriminator Loss: 0.5031 Generator Loss: 1.6705
Epoch [23/30]                        Batch 400/1563 Discriminator Loss: 0.7800 Generator Loss: 1.1085
Epoch [23/30]                        Batch 500/1563 Discriminator Loss: 0.6581 Generator Loss: 0.7194
Epoch [23/30]                        Batch 600/1563 Discriminator Loss: 0.6880 Generator Loss: 1.2136
Epoch [23/30]                        Batch 700/1563 Discriminator Loss: 0.8604 Generator Loss: 1.2978
Epoch [23/30]                        Batch 800/1563 Discriminator Loss: 0.4069 Generator Loss: 1.5116
Epoch [23/30]                        Batch 900/1563 Discriminator Loss: 0.6084 Generator Loss: 1.3218
Epoch [23/30]                        Batch 1000/1563 Discriminator Loss: 0.3370 Generator Loss: 2.3237
Epoch [23/30]                        Batch 1100/1563 Discriminator Loss: 0.5806 Generator Loss: 1.6243
Epoch [23/30]                        Batch 1200/1563 Discriminator Loss: 0.5700 Generator Loss: 1.3030
Epoch [23/30]                        Batch 1300/1563 Discriminator Loss: 0.4817 Generator Loss: 1.0187
Epoch [23/30]                        Batch 1400/1563 Discriminator Loss: 0.5350 Generator Loss: 0.9343
Epoch [23/30]                        Batch 1500/1563 Discriminator Loss: 0.3988 Generator Loss: 1.9423
Epoch [24/30]                        Batch 100/1563 Discriminator Loss: 0.5867 Generator Loss: 1.8228
Epoch [24/30]                        Batch 200/1563 Discriminator Loss: 0.6380 Generator Loss: 0.7886
Epoch [24/30]                        Batch 300/1563 Discriminator Loss: 1.0712 Generator Loss: 1.7067
Epoch [24/30]                        Batch 400/1563 Discriminator Loss: 0.3691 Generator Loss: 1.6998
Epoch [24/30]                        Batch 500/1563 Discriminator Loss: 0.7596 Generator Loss: 0.6284
Epoch [24/30]                        Batch 600/1563 Discriminator Loss: 0.8459 Generator Loss: 1.5625
Epoch [24/30]                        Batch 700/1563 Discriminator Loss: 0.4499 Generator Loss: 1.0025
Epoch [24/30]                        Batch 800/1563 Discriminator Loss: 0.3667 Generator Loss: 1.3655
Epoch [24/30]                        Batch 900/1563 Discriminator Loss: 0.8259 Generator Loss: 1.6259
Epoch [24/30]                        Batch 1000/1563 Discriminator Loss: 0.8847 Generator Loss: 1.0071
Epoch [24/30]                        Batch 1100/1563 Discriminator Loss: 0.2908 Generator Loss: 2.2473
Epoch [24/30]                        Batch 1200/1563 Discriminator Loss: 0.8809 Generator Loss: 0.8121
Epoch [24/30]                        Batch 1300/1563 Discriminator Loss: 0.5599 Generator Loss: 0.8316
Epoch [24/30]                        Batch 1400/1563 Discriminator Loss: 0.5931 Generator Loss: 1.6818
Epoch [24/30]                        Batch 1500/1563 Discriminator Loss: 0.4780 Generator Loss: 1.4393
Epoch [25/30]                        Batch 100/1563 Discriminator Loss: 0.4862 Generator Loss: 1.0551
Epoch [25/30]                        Batch 200/1563 Discriminator Loss: 0.7419 Generator Loss: 0.8418
Epoch [25/30]                        Batch 300/1563 Discriminator Loss: 0.3005 Generator Loss: 1.4868
Epoch [25/30]                        Batch 400/1563 Discriminator Loss: 0.4458 Generator Loss: 0.8691
Epoch [25/30]                        Batch 500/1563 Discriminator Loss: 0.4591 Generator Loss: 1.3558
Epoch [25/30]                        Batch 600/1563 Discriminator Loss: 0.2917 Generator Loss: 1.1712
Epoch [25/30]                        Batch 700/1563 Discriminator Loss: 0.6088 Generator Loss: 1.1128
Epoch [25/30]                        Batch 800/1563 Discriminator Loss: 0.8086 Generator Loss: 1.5543
Epoch [25/30]                        Batch 900/1563 Discriminator Loss: 0.5244 Generator Loss: 1.4409
Epoch [25/30]                        Batch 1000/1563 Discriminator Loss: 0.6720 Generator Loss: 1.2794
Epoch [25/30]                        Batch 1100/1563 Discriminator Loss: 0.5955 Generator Loss: 1.0705
Epoch [25/30]                        Batch 1200/1563 Discriminator Loss: 0.6215 Generator Loss: 1.2729
Epoch [25/30]                        Batch 1300/1563 Discriminator Loss: 1.2661 Generator Loss: 1.4346
Epoch [25/30]                        Batch 1400/1563 Discriminator Loss: 0.6111 Generator Loss: 1.4205
Epoch [25/30]                        Batch 1500/1563 Discriminator Loss: 0.5564 Generator Loss: 1.5329
Epoch [26/30]                        Batch 100/1563 Discriminator Loss: 0.7150 Generator Loss: 1.1040
Epoch [26/30]                        Batch 200/1563 Discriminator Loss: 0.5486 Generator Loss: 0.8081
Epoch [26/30]                        Batch 300/1563 Discriminator Loss: 0.4655 Generator Loss: 1.0543
Epoch [26/30]                        Batch 400/1563 Discriminator Loss: 0.6280 Generator Loss: 1.3089
Epoch [26/30]                        Batch 500/1563 Discriminator Loss: 0.6435 Generator Loss: 1.2938
Epoch [26/30]                        Batch 600/1563 Discriminator Loss: 0.5681 Generator Loss: 0.6704
Epoch [26/30]                        Batch 700/1563 Discriminator Loss: 0.5975 Generator Loss: 1.6594
Epoch [26/30]                        Batch 800/1563 Discriminator Loss: 0.5214 Generator Loss: 1.5375
Epoch [26/30]                        Batch 900/1563 Discriminator Loss: 0.7233 Generator Loss: 2.1745
Epoch [26/30]                        Batch 1000/1563 Discriminator Loss: 0.7600 Generator Loss: 1.9514
Epoch [26/30]                        Batch 1100/1563 Discriminator Loss: 0.4599 Generator Loss: 1.6351
Epoch [26/30]                        Batch 1200/1563 Discriminator Loss: 0.4130 Generator Loss: 0.6645
Epoch [26/30]                        Batch 1300/1563 Discriminator Loss: 0.7173 Generator Loss: 1.5409
Epoch [26/30]                        Batch 1400/1563 Discriminator Loss: 0.6444 Generator Loss: 0.4363
Epoch [26/30]                        Batch 1500/1563 Discriminator Loss: 0.8130 Generator Loss: 0.5472
Epoch [27/30]                        Batch 100/1563 Discriminator Loss: 0.3806 Generator Loss: 2.5330
Epoch [27/30]                        Batch 200/1563 Discriminator Loss: 0.6471 Generator Loss: 1.4548
Epoch [27/30]                        Batch 300/1563 Discriminator Loss: 0.4945 Generator Loss: 2.0417
Epoch [27/30]                        Batch 400/1563 Discriminator Loss: 0.8287 Generator Loss: 0.8313
Epoch [27/30]                        Batch 500/1563 Discriminator Loss: 0.2590 Generator Loss: 1.3648
Epoch [27/30]                        Batch 600/1563 Discriminator Loss: 0.7229 Generator Loss: 1.0506
Epoch [27/30]                        Batch 700/1563 Discriminator Loss: 0.5257 Generator Loss: 0.6630
Epoch [27/30]                        Batch 800/1563 Discriminator Loss: 0.7077 Generator Loss: 1.5787
Epoch [27/30]                        Batch 900/1563 Discriminator Loss: 0.2241 Generator Loss: 2.4144
Epoch [27/30]                        Batch 1000/1563 Discriminator Loss: 0.9249 Generator Loss: 1.0945
Epoch [27/30]                        Batch 1100/1563 Discriminator Loss: 0.5512 Generator Loss: 1.3305
Epoch [27/30]                        Batch 1200/1563 Discriminator Loss: 0.6963 Generator Loss: 0.6857
Epoch [27/30]                        Batch 1300/1563 Discriminator Loss: 0.6866 Generator Loss: 1.0320
Epoch [27/30]                        Batch 1400/1563 Discriminator Loss: 0.8760 Generator Loss: 0.5938
Epoch [27/30]                        Batch 1500/1563 Discriminator Loss: 0.6564 Generator Loss: 1.1705
Epoch [28/30]                        Batch 100/1563 Discriminator Loss: 0.6794 Generator Loss: 0.6070
Epoch [28/30]                        Batch 200/1563 Discriminator Loss: 0.5660 Generator Loss: 1.3422
Epoch [28/30]                        Batch 300/1563 Discriminator Loss: 0.4716 Generator Loss: 1.3143
Epoch [28/30]                        Batch 400/1563 Discriminator Loss: 0.5311 Generator Loss: 1.8503
Epoch [28/30]                        Batch 500/1563 Discriminator Loss: 0.5435 Generator Loss: 1.4696
Epoch [28/30]                        Batch 600/1563 Discriminator Loss: 0.4778 Generator Loss: 1.1126
Epoch [28/30]                        Batch 700/1563 Discriminator Loss: 0.4951 Generator Loss: 0.9386
Epoch [28/30]                        Batch 800/1563 Discriminator Loss: 0.5161 Generator Loss: 1.7677
Epoch [28/30]                        Batch 900/1563 Discriminator Loss: 0.5352 Generator Loss: 1.3093
Epoch [28/30]                        Batch 1000/1563 Discriminator Loss: 0.4307 Generator Loss: 1.0498
Epoch [28/30]                        Batch 1100/1563 Discriminator Loss: 0.8755 Generator Loss: 1.3385
Epoch [28/30]                        Batch 1200/1563 Discriminator Loss: 0.5321 Generator Loss: 1.0425
Epoch [28/30]                        Batch 1300/1563 Discriminator Loss: 0.6281 Generator Loss: 0.8529
Epoch [28/30]                        Batch 1400/1563 Discriminator Loss: 0.5690 Generator Loss: 1.3257
Epoch [28/30]                        Batch 1500/1563 Discriminator Loss: 1.0743 Generator Loss: 1.1915
Epoch [29/30]                        Batch 100/1563 Discriminator Loss: 0.6799 Generator Loss: 1.0841
Epoch [29/30]                        Batch 200/1563 Discriminator Loss: 0.6932 Generator Loss: 1.3634
Epoch [29/30]                        Batch 300/1563 Discriminator Loss: 0.4663 Generator Loss: 1.1661
Epoch [29/30]                        Batch 400/1563 Discriminator Loss: 0.7206 Generator Loss: 0.9654
Epoch [29/30]                        Batch 500/1563 Discriminator Loss: 0.7813 Generator Loss: 1.4285
Epoch [29/30]                        Batch 600/1563 Discriminator Loss: 0.8215 Generator Loss: 1.1129
Epoch [29/30]                        Batch 700/1563 Discriminator Loss: 0.5939 Generator Loss: 1.5154
Epoch [29/30]                        Batch 800/1563 Discriminator Loss: 0.6840 Generator Loss: 1.2734
Epoch [29/30]                        Batch 900/1563 Discriminator Loss: 0.4068 Generator Loss: 1.2772
Epoch [29/30]                        Batch 1000/1563 Discriminator Loss: 0.6416 Generator Loss: 0.4559
Epoch [29/30]                        Batch 1100/1563 Discriminator Loss: 0.4972 Generator Loss: 1.6791
Epoch [29/30]                        Batch 1200/1563 Discriminator Loss: 0.8609 Generator Loss: 1.1342
Epoch [29/30]                        Batch 1300/1563 Discriminator Loss: 0.6266 Generator Loss: 0.8422
Epoch [29/30]                        Batch 1400/1563 Discriminator Loss: 0.5542 Generator Loss: 1.1366
Epoch [29/30]                        Batch 1500/1563 Discriminator Loss: 0.5414 Generator Loss: 1.0412
Epoch [30/30]                        Batch 100/1563 Discriminator Loss: 1.0110 Generator Loss: 0.9997
Epoch [30/30]                        Batch 200/1563 Discriminator Loss: 0.4734 Generator Loss: 1.2250
Epoch [30/30]                        Batch 300/1563 Discriminator Loss: 0.3835 Generator Loss: 1.3170
Epoch [30/30]                        Batch 400/1563 Discriminator Loss: 0.3179 Generator Loss: 2.2588
Epoch [30/30]                        Batch 500/1563 Discriminator Loss: 0.3329 Generator Loss: 1.5280
Epoch [30/30]                        Batch 600/1563 Discriminator Loss: 0.3452 Generator Loss: 0.8436
Epoch [30/30]                        Batch 700/1563 Discriminator Loss: 0.8666 Generator Loss: 0.6735
Epoch [30/30]                        Batch 800/1563 Discriminator Loss: 0.6920 Generator Loss: 1.8045
Epoch [30/30]                        Batch 900/1563 Discriminator Loss: 0.6625 Generator Loss: 0.8388
Epoch [30/30]                        Batch 1000/1563 Discriminator Loss: 0.7458 Generator Loss: 2.1390
Epoch [30/30]                        Batch 1100/1563 Discriminator Loss: 1.4766 Generator Loss: 1.7256
Epoch [30/30]                        Batch 1200/1563 Discriminator Loss: 0.4399 Generator Loss: 1.4570
Epoch [30/30]                        Batch 1300/1563 Discriminator Loss: 0.4310 Generator Loss: 1.7740
Epoch [30/30]                        Batch 1400/1563 Discriminator Loss: 0.4642 Generator Loss: 1.1918
Epoch [30/30]                        Batch 1500/1563 Discriminator Loss: 0.5108 Generator Loss: 1.7462
</code></pre></div> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=auto href=../gan_example_cifar_10_files/gan_example_cifar_10_21_5.png><img alt=png src=../gan_example_cifar_10_files/gan_example_cifar_10_21_5.png></a></p> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 6, 2025 12:06:33 UTC">November 6, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 6, 2025 12:06:33 UTC">November 6, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Contributors> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"></path></svg> </span> <nav> <a href=mailto:hsandmann@ieee.org>Humberto Sandmann</a> </nav> </span> <span class=md-source-file__fact> <span class=md-icon title=Contributors> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path></svg> </span> <span>GitHub</span> <nav> <a href=https://github.com/hsandmann class=md-author title=@hsandmann> <img src="https://avatars.githubusercontent.com/u/20843348?v=4&size=72" alt=hsandmann> </a> </nav> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.copy", "content.code.select", "content.code.annotate", "content.tooltips", "navigation.instant", "navigation.instant.progress", "navigation.top", "navigation.path", "navigation.tracking"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../../assets/_markdown_exec_pyodide.js></script> <script src=../../../assets/javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../assets/javascripts/badge.js async></script> <script src=../../../termynal.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>